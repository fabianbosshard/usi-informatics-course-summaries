% author: Fabian Bosshard % Â© CC BY 4.0
\documentclass[9pt,headings=standardclasses,parskip=half]{scrartcl}

% ===================== basics =====================
\usepackage{ifthen,iftex,csquotes}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}

% ===================== page style =====================
\usepackage[automark]{scrlayer-scrpage}
\pagestyle{scrheadings}

% ===================== graphics / color =====================
\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{caption,subcaption}
\usepackage[left=38mm,right=38mm,top=20mm,bottom=30mm]{geometry}

% ===================== math =====================
\usepackage{amsmath,amssymb,amsthm,mathtools,mathdots,accents}
\numberwithin{equation}{section}

% ===================== lists =====================
\usepackage{enumitem}
\renewcommand{\labelitemi}{\textbullet}
\renewcommand{\labelitemii}{\raisebox{0.1ex}{\scalebox{0.8}{\textbullet}}}
\renewcommand{\labelitemiii}{\raisebox{0.2ex}{\scalebox{0.6}{\textbullet}}}
\renewcommand{\labelitemiv}{\raisebox{0.3ex}{\scalebox{0.4}{\textbullet}}}

% ===================== bibliography =====================
\usepackage[backend=biber,style=numeric]{biblatex}
\addbibresource{\jobname.bib}

% ===================== symbols =====================
\usepackage{pifont}

% ===================== TikZ / plots =====================
\usepackage{tikz}
\usetikzlibrary{arrows,arrows.meta,shapes,positioning,calc,fit,patterns,intersections,math,3d,tikzmark,decorations.pathreplacing,decorations.markings}
\usepackage{tikz-dependency,tikz-qtree,tikz-qtree-compat,tikz-3dplot,tikzpagenodes}
\usepackage{pgfplots}

% ===================== highlighting (SOUL + math-safe \hl) =====================
\usepackage{soulutf8}
\usepackage{xparse}

\ExplSyntaxOn
\tl_new:N \__l_SOUL_argument_tl
\cs_set_eq:Nc \SOUL_start:n { SOUL@start }
\cs_generate_variant:Nn \SOUL_start:n { V }
\cs_set_protected:cpn {SOUL@start} #1 {
  \tl_set:Nn \__l_SOUL_argument_tl { #1 }
  \regex_replace_all:nnN { \c{\(} (.*?) \c{\)} } { \cM\$ \1 \cM\$ } \__l_SOUL_argument_tl
  \SOUL_start:V \__l_SOUL_argument_tl
}
\ExplSyntaxOff

\let\SOULhl\hl
\renewcommand{\hl}[1]{\SOULhl{#1}}

\definecolor{HLgreen}{HTML}{77DD77}
\definecolor{HLyellow}{HTML}{FFFF66}
\definecolor{HLorange}{HTML}{FFB347}
\definecolor{HLred}{HTML}{FF6961}
\definecolor{HLpink}{HTML}{FFB6C1}
\definecolor{HLturquoise}{HTML}{40E0D0}

\RenewDocumentCommand{\hl}{O{1} m}{%
  \begingroup
  \IfEqCase{#1}{%
    {1}{\sethlcolor{HLgreen}\SOULhl{#2}}%
    {2}{\sethlcolor{HLyellow}\SOULhl{#2}}%
    {3}{\sethlcolor{HLorange}\SOULhl{#2}}%
    {4}{\sethlcolor{HLred}\SOULhl{#2}}%
    {5}{\sethlcolor{red}\SOULhl{#2}}%
    {6}{\sethlcolor{HLturquoise}\SOULhl{#2}}%
  }[\PackageError{hl}{Undefined highlight level: #1}{}]%
  \endgroup
}

% ===================== colors / emphasis =====================
\definecolor{funblue}{rgb}{0.10,0.35,0.66}
\definecolor{alizarincrimsonred}{rgb}{0.85,0.17,0.11}
\definecolor{amethyst}{rgb}{0.6,0.4,0.8}
\definecolor{mypurple}{rgb}{0.5,0,0.5}
\definecolor{highlightpurple}{rgb}{0.6,0,0.6}
\renewcommand{\emph}[1]{\textcolor{highlightpurple}{\textsl{#1}}}

% ===================== theorem environments =====================
\usepackage{thmtools}

\newlength{\thmspace}\setlength{\thmspace}{3pt plus 1pt minus 1pt}

\declaretheoremstyle[headfont=\bfseries,bodyfont=\normalfont,spaceabove=\thmspace,spacebelow=\thmspace,qed=\ensuremath{\vartriangleleft},postheadspace=1em]{assertionstyle}
\declaretheorem[style=assertionstyle,name=Theorem,numberwithin=section]{theorem}
\declaretheorem[style=assertionstyle,name=Lemma,sibling=theorem]{lemma}
\declaretheorem[style=assertionstyle,name=Corollary,sibling=theorem]{corollary}
\declaretheorem[style=assertionstyle,name=Proposition,sibling=theorem]{proposition}
\declaretheorem[style=assertionstyle,name=Conjecture,sibling=theorem]{conjecture}
\declaretheorem[style=assertionstyle,name=Claim,sibling=theorem]{claim}
\declaretheorem[style=assertionstyle,name=Fact,sibling=theorem]{fact}

\declaretheoremstyle[headfont=\bfseries,bodyfont=\normalfont,spaceabove=\thmspace,spacebelow=\thmspace,qed=\ensuremath{\blacktriangleleft},postheadspace=1em]{definitionstyle}
\declaretheorem[style=definitionstyle,name=Definition,numberwithin=section]{definition}
\declaretheorem[style=definitionstyle,name=Problem,sibling=definition]{problem}

\declaretheoremstyle[headfont=\bfseries,bodyfont=\normalfont,spaceabove=\thmspace,spacebelow=\thmspace,qed=\ding{45},postheadspace=1em]{exercisestyle}
\declaretheorem[style=exercisestyle,name=Exercise,numberwithin=section]{exercise}
\declaretheoremstyle[headfont=\bfseries\color{red},bodyfont=\normalfont,spaceabove=\thmspace,spacebelow=\thmspace,qed=\ensuremath{\color{red}\blacktriangleleft},postheadspace=1em]{solutionstyle}
\declaretheorem[style=solutionstyle,name=Solution,numbered=no]{solution}

\declaretheoremstyle[headfont=\bfseries,bodyfont=\normalfont,spaceabove=6pt,spacebelow=6pt,qed=\ensuremath{\square},postheadspace=1em]{proofstyle}
\let\proof\relax \let\endproof\relax
\declaretheorem[style=proofstyle,name=Proof,numbered=no]{proof}

\declaretheoremstyle[headfont=\bfseries,bodyfont=\normalfont\normalsize,spaceabove=\thmspace,spacebelow=\thmspace,qed=\ensuremath{\blacktriangleleft},postheadspace=1em]{remarkstyle}
\declaretheorem[style=remarkstyle,name=Remark,numberwithin=section]{remark}

\declaretheoremstyle[headfont=\bfseries\color{funblue},bodyfont=\normalfont\normalsize,spaceabove=\thmspace,spacebelow=\thmspace,qed=\ensuremath{\color{funblue}\blacktriangleleft},postheadspace=1em]{examplestyle}
\declaretheorem[style=examplestyle,name=Example,sibling=remark]{example}

\declaretheoremstyle[headfont=\color{alizarincrimsonred}\bfseries,bodyfont=\normalfont\normalsize,spaceabove=\thmspace,spacebelow=\thmspace,qed=\ensuremath{\color{alizarincrimsonred}\blacktriangleleft},postheadspace=1em]{cautionstyle}
\declaretheorem[style=cautionstyle,name=Caution,sibling=remark]{caution}

\declaretheoremstyle[headfont=\bfseries,bodyfont=\normalfont\footnotesize,spaceabove=\thmspace,spacebelow=\thmspace,postheadspace=1em]{smallremarkstyle}
\declaretheorem[style=smallremarkstyle,name=Remark,sibling=remark]{smallremark}

\declaretheoremstyle[headfont=\bfseries\color{amethyst},bodyfont=\normalfont,spaceabove=6pt,spacebelow=6pt,qed=\ensuremath{\color{amethyst}\blacktriangleleft},postheadspace=1em]{digressionstyle}
\declaretheorem[style=digressionstyle,name=Digression,sibling=remark]{digression}

% ===================== misc helpers =====================
\newenvironment{verticalhack}{\begin{array}[b]{@{}c@{}}\displaystyle}{\\\noalign{\hrule height0pt}\end{array}}

% ===================== algorithms =====================
\usepackage{algorithm,algorithmicx}
\usepackage[italicComments=false]{algpseudocodex}
\newcommand*{\algorithmautorefname}{Algorithm}

\algnewcommand{\TO}{, \ldots ,}
\algnewcommand{\DOWNTO}{, \ldots ,}
\algnewcommand{\OR}{\vee}
\algnewcommand{\AND}{\wedge}
\algnewcommand{\NOT}{\neg}
\algnewcommand{\LEN}{\operatorname{len}}
\algnewcommand{\tru}{\ensuremath{\mathrm{\texttt{true}}}}
\algnewcommand{\fals}{\ensuremath{\mathrm{\texttt{false}}}}
\algnewcommand{\append}{\circ}
\algnewcommand{\nil}{\ensuremath{\mathrm{\textsc{nil}}}}
\algnewcommand{\red}{\ensuremath{\mathrm{\textsc{red}}}}
\algnewcommand{\black}{\ensuremath{\mathrm{\textsc{black}}}}
\algnewcommand{\gray}{\ensuremath{\mathrm{\textsc{gray}}}}
\algnewcommand{\white}{\ensuremath{\mathrm{\textsc{white}}}}

% ===================== operators / notation =====================
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Kern}{kern}
\DeclareMathOperator{\Trace}{trace}
\DeclareMathOperator{\Rank}{rank}
\newcommand{\im}{\operatorname{Im}}
\newcommand{\re}{\operatorname{Re}}

\newcommand*\matrspace{0.8mu}
\newcommand{\matr}[1]{\mspace{\matrspace}\underline{\mspace{-\matrspace}\smash[b]{\boldsymbol{#1}}\mspace{-\matrspace}}\mspace{\matrspace}}
\newcommand{\vect}[1]{{\boldsymbol{#1}}}

\newcommand{\dif}{\mathrm{d}}
\newcommand{\eu}{\mathrm{e}}
\newcommand{\iu}{\mathrm{i}}
\newcommand{\sign}{\operatorname{sgn}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\F}{\mathbb{F}}

\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Exp}{\operatorname{E}}
\newcommand{\Prob}{\operatorname{P}}
\newcommand{\numof}{\ensuremath{\#\,}}
\newcommand{\blackheight}{\operatorname{bh}}

\newcommand{\attrib}[2]{\ensuremath{#1\mathtt{.}\mathtt{#2}}}
\newcommand{\attribnormal}[2]{\ensuremath{#1\mathtt{.}#2}}

% ===================== metadata / hyperlinks =====================
\title{Graph Deep Learning}
\author{Fabian Bosshard}
\date{\today}

\usepackage[
  linktoc=none,
  pdfauthor={Fabian Bosshard},
  pdftitle={USI - Graph Deep Learning - Course Notes},
  pdfkeywords={USI, Graph Deep Learning, course notes, informatics},
  colorlinks=false,
  pdfborder={0 0 0},
  linkbordercolor={0 0.6 1},
  urlbordercolor={0 0.6 1},
  citebordercolor={0 0.6 1}
]{hyperref}

% ToC entries: dotted leaders + full-width clickable line
\DeclareTOCStyleEntry[linefill=\dotfill]{tocline}{section}
\DeclareTOCStyleEntry[linefill=\dotfill]{tocline}{subsection}
\DeclareTOCStyleEntry[linefill=\dotfill]{tocline}{subsubsection}

\makeatletter
\newlength\FB@toclinkht \newlength\FB@toclinkdp
\setlength\FB@toclinkht{.80\ht\strutbox}
\setlength\FB@toclinkdp{.40\dp\strutbox}

\let\FB@orig@contentsline\contentsline
\renewcommand*\contentsline[4]{%
  \begingroup
    \Hy@safe@activestrue
    \edef\Hy@tocdestname{#4}%
    \FB@orig@contentsline{#1}{%
      \Hy@raisedlink{\hyper@anchorstart{toc:\Hy@tocdestname}\hyper@anchorend}%
      \leavevmode
      \rlap{%
        \hyper@linkstart{link}{\Hy@tocdestname}%
          \raisebox{0pt}[\FB@toclinkht][\FB@toclinkdp]{%
            \hbox to \dimexpr\hsize-\parindent\relax{\hfil}%
          }%
        \hyper@linkend
      }%
      #2%
    }{#3}{#4}%
  \endgroup
}

\let\FB@orig@sectionlinesformat\sectionlinesformat
\renewcommand{\sectionlinesformat}[4]{%
  \ifx\@currentHref\@empty
    \FB@orig@sectionlinesformat{#1}{#2}{#3}{#4}%
  \else
    \leavevmode
    \hyper@linkstart{link}{toc:\@currentHref}%
      \@hangfrom{\hskip #2\relax #3}{#4}%
    \hyper@linkend
    \par
  \fi
}
\makeatother

% hyperref anchor uniqueness for section-relative numbering
\makeatletter
\renewcommand{\theHequation}{\thesection.\arabic{equation}}
\renewcommand{\theHtheorem}{\thesection.\arabic{theorem}}
\renewcommand{\theHlemma}{\theHtheorem}
\renewcommand{\theHcorollary}{\theHtheorem}
\renewcommand{\theHproposition}{\theHtheorem}
\renewcommand{\theHconjecture}{\theHtheorem}
\renewcommand{\theHclaim}{\theHtheorem}
\renewcommand{\theHfact}{\theHtheorem}
\renewcommand{\theHdefinition}{\thesection.\arabic{definition}}
\renewcommand{\theHproblem}{\theHdefinition}
\renewcommand{\theHexercise}{\thesection.\arabic{exercise}}
\renewcommand{\theHremark}{\thesection.\arabic{remark}}
\renewcommand{\theHexample}{\theHremark}
\renewcommand{\theHcaution}{\theHremark}
\renewcommand{\theHsmallremark}{\theHremark}
\renewcommand{\theHdigression}{\theHremark}
\makeatother

% ===================== license / cleveref =====================
\usepackage[type={CC},modifier={by},version={4.0}]{doclicense}
\usepackage{cleveref}

% ===================== acronyms =====================
\usepackage[acronym,nomain,toc,nonumberlist]{glossaries-extra}
\setabbreviationstyle[acronym]{long-short}
\makeglossaries
\newacronym{rbf}{RBF}{radial basis function}
\newacronym{rkhs}{RKHS}{reproducing kernel Hilbert space}


















%% commands from the paper
\newcommand{\gph}{G}
\newcommand{\MPNN}{\text{MPNN}}
\newcommand{\cheeg}{{h}_{\text{Cheeg}
}}
\newcommand{\res}{\text{Res}}
\newcommand{\V}{{V}}
\newcommand{\E}{{E}}
\newcommand{\GAMma}{\boldsymbol{\Gamma}}
%\newcommand{\MPNN}{\mathrm{MPNN}}
\newcommand{\Hi}{\mathbf{H}}
\newcommand{\Hit}{\mathbf{H}^{(t)}}
\newcommand{\rs}{\text{r}}
\newcommand{\mpas}{\text{a}}


\newcommand{\up}{\gamma}
\newcommand{\agg}{\texttt{agg}}
\newcommand{\mess}{\phi}


\newcommand{\com}{\text{com}}
\newcommand{\colfirst}{\textcolor{red}}
\newcommand{\colsecond}{\textcolor{blue}}
\newcommand{\colthird}{\textcolor{violet}}
% \newcommand{\MPNN}{\text{MPNN}}
\newcommand{\eigen}{\boldsymbol{\psi}}
\newcommand{\cost}{{c}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\DELta}{\boldsymbol{\Delta}}
\newcommand{\KER}{\mathcal{K}}
\newcommand{\OMEga}{\boldsymbol{\Omega}}
\newcommand{\Anorm}{\boldsymbol{\text{A}}}
\newcommand{\tel}{\text{MPNN}_{\text{tel}}}
\newcommand{\poly}{\text{p}}
\newcommand{\ourname}{\text{telescopic}-\text{MPNN}}
\newcommand{\oper}{\boldsymbol{\text{S}}_{\rs,\mpas}}
\newcommand{\xb}{\vb{x}}
\newcommand{\Xb}{\mathbf{X}}
\newcommand{\Wb}{\vb{W}}
\newcommand{\Hb}{\mathbf{H}}
\newcommand{\Ab}{\mathbf{A}}
\newcommand{\Ib}{\mathbf{I}}
\newcommand{\hb}{\mathbf{h}}
\newcommand{\phib}{\boldsymbol{\phi}}
\newcommand{\pepfunc}{\texttt{Peptides-func}\xspace}
\newcommand{\pepstruct}{\texttt{Peptides-struct}\xspace}









\begin{document}

\maketitle

\tableofcontents


% table of acronyms (remember everything is in build folder)
% \printglossary[type=\acronymtype, title={List of Acronyms}]

% \clearpage

\section{Spectral theorem}
\begin{theorem}\label{thm:spectral_theorem}
Let $\matr{A}\in\R^{n\times n}$ be symmetric, i.e.\ $\matr{A}^\top=\matr{A}$.
Then,
\begin{equation}\label{eq:spectral_decomposition}
    \matr{A}
    =
    \sum_{i=1}^{n} \lambda_i\, \vect{u}_i \vect{u}_i^\top
\end{equation}
with orthonormal eigenvectors $\vect{u}_1,\dots,\vect{u}_n\in\R^n$
and real eigenvalues $\lambda_1,\dots,\lambda_n\in\R$.
Equivalently,
\begin{equation}\label{eq:diagonalization}
    \matr{A} = \matr{U}\matr{\Lambda}\matr{U}^\top
\end{equation}
with $\matr{U} := \big[\vect{u}_1\,\cdots\,\vect{u}_n\big]$, $\matr{U}^\top \matr{U} = \matr{I}_n$ and
$\matr{\Lambda}:=\operatorname{diag}(\lambda_1,\dots,\lambda_n)$.
\end{theorem}

\autoref{thm:spectral_theorem} is extensively used in Principal Component Analysis (PCA) to reduce the complexity of the input space (it is applied to the covariance matrix of the inputs).


\clearpage

\section{Graphs}

Let $G = (V, E)$ be an undirected graph with the
adjacency matrix $\matr{A} \in \R^{n\times n}$
\begin{equation}\label{eq:adjacency_matrix}
    % \matr{A}_{uv}
    % a_{uv}
    [\matr{A}]_{uv}
    =
    \begin{cases}
        1 & (u,v) \in E\\
        0 & (u,v) \notin E
    \end{cases}
\end{equation}
where \([\cdot]_{uv}\) denotes the entry in row \(u\) and column \(v\).

The \emph{diagonal degree matrix} $\matr{D}\in\R^{n\times n}$ is defined by
\begin{equation}\label{eq:degree_matrix}
    % \matr{D}_{uv}
    [\matr{D}]_{uv}
    =
    \begin{cases}
        d_u &  u = v\\
        % \deg{u} &  u = v\\
        0 & u \ne v
    \end{cases}
\end{equation}
where \(d_u\) is the degree of node \(u\),
i.e. $\matr{D}$ simply places all node degrees on the diagonal.




\subsection{normalized adjacency and multi-hop propagation}


\begin{definition}\label{def:normalized_adjacency}
The \emph{symmetrically normalized adjacency matrix} is
\begin{equation}\label{eq:normalized_adjacency}
    {\color{Red}\boxed{\color{black}
    \hat{\matr{A}}
    =
    \matr{D}^{-1/2}\matr{A}\matr{D}^{-1/2}
    }}
\end{equation}
or, entrywise,
\[
\begin{verticalhack}
    % \hat{\matr{A}}_{uv}
    [\hat{\matr{A}}]_{uv}
    =
    % \hat{a}_{uv}
    % =
    \begin{cases}
        \dfrac{1}{\sqrt{d_u d_v}} & (u,v) \in E\\
        % \frac{1}{\sqrt{\deg(u)  \deg(v)}} & (u,v) \in E\\
        0 & (u,v) \notin E
    \end{cases}
    \end{verticalhack}
    \qedhere
\]
\end{definition}

\begin{fact}[multi-hop propagation]\label{prop:multi_hop_propagation}
The entry $(\hat{\matr{A}}^{k})_{vu}$ can be computed explicitly as follows:
\begin{equation}\label{eq:multi_hop_propagation}
    [\hat{\matr{A}}^{k}]_{vu}
    =
    \sum_{\pi}
    \prod_{(x, y) \in E_\pi} \frac{1}{\sqrt{d_x d_y}}
\end{equation}
the sum is over all walks $\pi = (v, \ldots, u)$ of length $k$ from $v$ to $u$ and the product is over the edges \(E_\pi = \{(v, u_1), \ldots, (u_{k-1}, u)\}\) on the walk.
\end{fact}



\begin{corollary}\label{cor:multi_hop_propagation_single_path}
Let $v,u \in V$ with $r = d_{G}(v,u)$, where $d_{G}(\cdot,\cdot)$ denotes the shortest-path distance.
Assume there is exactly one path
\[
    (v, u_{1}, \ldots, u_{r-1}, u)
\]
of length $r$ between $v$ and $u$:
\[
\begin{tikzpicture}[
  font=\footnotesize,
  scale=0.4,
  vertex/.style={
    circle,
    draw,
    fill=green!40,
    minimum size=10pt,
    inner sep=0,
    label={
        [anchor=base, label distance=5pt]
        below:{#1} % parentheses necessary, otherwise the '=' breaks TikZ parsing
    }
  },
  measure/.style={|-|}
]

\node[vertex={$v$}] (v) at (0, 0) {};
\node[vertex={$u_{1}$}] (u1) at (3, 0) {};
\node[vertex={$u_{r-1}$}] (urm1) at (9, 0) {};
\node[vertex={$u$}] (u) at (12, 0) {};


\draw[] (v) -- (u1);
\draw[] (u1) -- ($(u1)!1/3!(urm1)$);
\draw[dotted] ($(u1)!1/3!(urm1)$) -- ($(u1)!2/3!(urm1)$);
\draw[] ($(u1)!2/3!(urm1)$) -- (urm1);
\draw[] (urm1) -- (u);


\def\offset{-0.5cm};
\draw[measure] ($(v) + (0, -1) + (0, \offset)$) -- node[below, outer sep = 2pt, inner sep = 0] {$d_{G}(v,u) = r$} ($(u) + (0, -1) + (0, \offset)$);


\end{tikzpicture}
\]
Then
\begin{equation}\label{eq:multi_hop_propagation_single_path}
    % \begin{aligned}
    % (\hat{\matr{A}}^{r})_{vu}
    % &=
    % \frac{1}{\sqrt{d_v d_{u_{1}}}} \cdot
    % \prod_{i=1}^{r-2} \frac{1}{\sqrt{d_{u_{i}} d_{u_{i+1}}}} \cdot
    % \frac{1}{\sqrt{d_{u_{r-1}} d_{u}}} \\
    % &=
    % \frac{1}{\sqrt{d_v d_u}} \prod_{i=1}^{r-1} \frac{1}{d_{u_i}}
    % \end{aligned}
    (\hat{\matr{A}}^{r})_{vu}
    =
    \frac{1}{\sqrt{d_v d_{u_{1}}}} \cdot
    \prod_{i=1}^{r-2} \frac{1}{\sqrt{d_{u_{i}} d_{u_{i+1}}}} \cdot
    \frac{1}{\sqrt{d_{u_{r-1}} d_{u}}} 
    =
    \frac{1}{\sqrt{d_v d_u}} \prod_{i=1}^{r-1} \frac{1}{d_{u_i}}
\end{equation}
\end{corollary}



% \subsubsection{distance layers and layer degrees}

% \begin{definition}\label{def:distance_layer_adjacency}
% For $\ell \in \N_0$, we define the \emph{distance-($\ell+1$) adjacency matrix} $\matr{A}_{\ell} \in \R^{n\times n}$ by
% \begin{equation}\label{eq:distance_layer_adjacency}
%     \bigl(\matr{A}_{\ell}\bigr)_{uv}
%     =
%     \begin{cases}
%         1 & d_{G}(u,v) = \ell+1\\
%         0 & \text{otherwise}
%     \end{cases}
% \end{equation}
% where $d_{G}(u,v)$ is the shortest-path distance.
% The corresponding \emph{layer degree} of a node $v$ at distance level $\ell$ is
% \begin{equation}\label{eq:layer_degree}
%     d_{v,\ell}
%     =
%     \sum_{u \in V} \bigl(\matr{A}_{\ell}\bigr)_{vu},
% \end{equation}
% i.e.\ the number of nodes at graph distance $\ell+1$ from $v$.
% Let $\matr{D}_{\ell}$ be the diagonal matrix with $(\matr{D}_{\ell})_{vv} = d_{v,\ell}$.
% The \emph{normalized distance-$(\ell+1)$ adjacency} is
% \begin{equation}\label{eq:normalized_distance_layer_adjacency}
%     {\color{Red}\boxed{\color{black}
%     \hat{\matr{A}}_{\ell}
%     =
%     \matr{D}_{\ell}^{-1/2} \matr{A}_{\ell} \matr{D}_{\ell}^{-1/2}
%     }}
% \end{equation}
% so that
% \[
% \begin{verticalhack}
% \bigl(\hat{\matr{A}}_{\ell}\bigr)_{uv}
% =
% \begin{cases}
%     \frac{1}{\sqrt{d_{u,\ell} d_{v,\ell}}} & d_{G}(u,v) = \ell+1 \\
%     0 & \text{otherwise}
% \end{cases}
% \end{verticalhack}
% \qedhere
% \]
% \end{definition}






\subsection{graph Laplacian}

\begin{definition}\label{def:graph_laplacian}
The \emph{combinatorial} graph Laplacian is
% \begin{equation}\label{eq:graph_laplacian}
%     \matr{L} = \matr{D} - \matr{A}
% \end{equation}
\begin{equation}\label{eq:graph_laplacian}
    {\color{Red}\setlength{\fboxrule}{2pt}\boxed{\color{black}
\matr{L} = \matr{D} - \matr{A}
}}
\end{equation}
and the \emph{normalized} graph Laplacian is
\begin{equation}\label{eq:normalized_graph_laplacian}
    {\color{Red}\boxed{\color{black}
    \hat{\matr{L}}
    =
    \matr{D}^{-1/2}\matr{L}\matr{D}^{-1/2}
    % \overset{\text{\eqref{eq:graph_laplacian}}}{=}
    =
    \matr{D}^{-1/2}(\matr{D} - \matr{A})\matr{D}^{-1/2}
    \overset{\text{\eqref{eq:normalized_adjacency}}}{=}
    \matr{I}_{n} - \hat{\matr{A}}
    }}
\end{equation}
Both are symmetric and positive semidefinite, and their eigenvalues satisfy
\[
    0 = \lambda_{0} \le \lambda_{1} \le \dots \le \lambda_{n-1}
\]
$\lambda_1$ is called the \emph{spectral gap}.
\hl[6]{The number of zero eigenvalues (i.e., the multiplicity of the \(0\) eigenvalue) equals the number of connected components of the graph}.
\end{definition}

To understand \autoref{def:graph_laplacian}, consider a function $f\colon V \to \R$.
Denote by $\vect{f} \in \R^{n}$ the vector whose $v$-th entry is $f(v)$.
Then
% \begin{equation}\label{eq:laplacian_action_on_function}
% (\matr{L}\vect{f})_v
% =
% d_v f(v) - \sum_{(u,v) \in E} f(u)
% \end{equation}
% i.e., $(\matr{L}\vect{f})_v$ is (up to the factor $d_v$) the difference between the value at $v$ and the sum of the values at its neighbors.
% For the normalized Laplacian, we have
\begin{equation}\label{eq:normalized_laplacian_action_on_function}
(\hat{\matr{L}}\vect{f})_v
=
f(v)
-
\frac{1}{\sqrt{d_v}}
\sum_{(u,v) \in E}
\frac{f(u)}{\sqrt{d_u}}
\end{equation}
i.e., $(\hat{\matr{L}}\vect{f})_v$ is the value at $v$ minus a degree-normalized average of the neighbors.
This is why the Laplacian is often viewed as a \emph{discrete second derivative} on the graph:
\hl[2]{it measures how much $f$ at $v$ deviates from its neighborhood}.

If we plot the eigenvectors of the Laplacian, it resembles that of a signal.

Another important identity is the quadratic form
\begin{equation}\label{eq:laplacian_quadratic_form}
    \vect{f}^{\top}\matr{L}\vect{f}
    =
    \frac{1}{2}
    \sum_{(u,v) \in E}
    \bigl(f(u) - f(v)\bigr)^{2}
\end{equation}
which shows that \(\matr{L}\) (and hence also \(\hat{\matr{L}}\)) is positive semidefinite, since the right-hand side is always nonnegative.
Moreover, \eqref{eq:laplacian_quadratic_form} is small exactly when $f$ varies slowly across edges, so the Laplacian encodes the \emph{smoothness} of functions on the graph.




\subsection{Cheeger inequality}

The \emph{Cheeger inequality} relates the spectral gap $\lambda_1$ to the \emph{Cheeger constant} $h(G)$, which measures how difficult it is to separate the graph into two large pieces.  It states, in particular, that
\[
    \tfrac{1}{2}h(G)^2
    \le
    \lambda_1
    \le
    2 h(G),
\]
so a larger spectral gap implies that the graph is more ``well-connected''.


\subsection{effective resistance}
\begin{definition}[effective resistance]\label{def:effective_resistance}
View each edge $(u,v)\in E$ as an electrical resistor of resistance $1\,$\(\Omega\).
The resulting network has a well-defined resistance between any two nodes.

For two nodes $s,t \in V$, the \emph{effective resistance} $R(s,t)$ is defined as the voltage difference needed to send one unit of electrical current from $s$ to $t$.
It can be computed as
\begin{equation}\label{eq:effective_resistance}
{\color{Red}\boxed{\color{black}
    R(s,t)
    =
    (\vect{e}_s - \vect{e}_t)^{\top}
    \matr{L}^{\dagger}
    (\vect{e}_s - \vect{e}_t)
}}
\end{equation}
where $\matr{L}^{\dagger}$ is the Moore--Penrose pseudoinverse of the graph Laplacian \eqref{eq:graph_laplacian} and $\vect{e}_v$ is the standard basis vector of vertex $v$.
\end{definition}


\subsubsection{Interpretation}
If the graph offers many short, parallel paths between $s$ and $t$, then current can flow easily, so $R(s,t)$ is small.
If there are few or long paths, the current is ``bottlenecked'' and $R(s,t)$ is large.
Thus, effective resistance measures how ``well-connected'' two nodes are inside the global geometry of the graph.

\subsubsection{Connection to random walks}
A \emph{random walk} on $G$ is the Markov chain that, from a node $v$, moves to a uniformly random neighbor of $v$.  Its transition matrix is
\begin{equation}\label{eq:random_walk_transition_matrix}
{\color{Red}\setlength{\fboxrule}{2pt}\boxed{\color{black}
\matr{P} = \matr{D}^{-1}\matr{A}
}}
\end{equation}
so $\matr{P}_{vu} = 1/d_v$ if $(v,u)\in E$. 
The matrix \eqref{eq:random_walk_transition_matrix} is often called \hl{random-walk matrix}.

\medskip
For two nodes $u,v$, the \emph{commute time} $\texttt{CT}(u,v)$ is the expected number of steps for the random walk to start at $u$, reach $v$, and return to $u$ again.  
It can be related to the \nameref{def:effective_resistance} via
\begin{equation}\label{eq:commute_time_effective_resistance}
    \texttt{CT}(u,v)
    =
    2|E|
    R(u,v)
    % \text{,}
\end{equation}
giving a geometric interpretation of how ``far apart'' two nodes are in terms of random-walk behavior,
i.e. two nodes have small commute time exactly when they have small effective resistance.





\section{Graph neural networks}

\subsection{Graph shift operators}

\begin{definition}\label{def:graph_shift_operator}
% Let $\matr{A}\in\R^{n\times n}$ be the adjacency matrix of $G$.
A matrix $\tilde{\matr{A}}\in\R^{n\times n}$ is called a \emph{graph shift operator} ({GSO}) if it satisfies
\[
{\color{Red}\setlength{\fboxrule}{2pt}\boxed{\color{black}
    \tilde{a}_{ij} = 0 \quad \text{whenever } (i,j)\notin E \text{ and } i\neq j
}}
\]
where $\tilde{a}_{ij} = [\tilde{\matr{A}}]_{ij}$.
This means that applying $\tilde{\matr{A}}$ to node attributes only mixes information from direct neighbors.
Typical choices include the Laplacian \eqref{eq:graph_laplacian} and the random-walk matrix \eqref{eq:random_walk_transition_matrix}.
\end{definition}

% \begin{remark}[local action]
Applying \(\tilde{\matr{A}}\) to node attributes \(\matr{X}\in\R^{n\times d_x}\) is local in the sense that the $i$-th row of \(\tilde{\matr{A}}\matr{X}\) depends only the neighbors $N(i)$ together with possibly $i$ itself:
\[
\vect{x}_i' 
= 
[\tilde{\matr{A}}\matr{X}]_{i,:} 
= 
% \sum_{j=1}^n [\tilde{\matr{A}}]_{ij} \vect{x}_j 
\sum_{j=1}^n \tilde{a}_{ij} \vect{x}_j 
= 
\tilde{a}_{ii} \vect{x}_i + \sum_{j\in N(i)} \tilde{a}_{ij} \vect{x}_j
% \sum_{j\in N(i)\cup\{i\}} [\tilde{\matr{A}}]_{ij} \vect{x}_j
\]


Using a parameter matrix \(\matr{\Theta}\in\R^{d_x\times d_h}\), we can apply the filter on a different space:
\[
\vect{h}_i 
=
[\tilde{\matr{A}}\matr{X}\matr{\Theta}]_{i,:}
=
\sum_{j=1}^n \tilde{a}_{ij} \vect{x}_j \matr{\Theta}
=
\tilde{a}_{ii} \vect{x}_i \matr{\Theta} + \sum_{j\in N(i)} \tilde{a}_{ij} \vect{x}_j \matr{\Theta}
\]
% \end{remark}

\subsection{Graph convolutions}

Let $\matr{X}\in\R^{n\times d_x}$ be node features, let $\tilde{\matr{A}}\in\R^{n\times n}$ be a GSO and let $\matr{\Theta}\in\R^{d_x\times d_h}$ be trainable weights.
A \emph{linear graph convolution} is
\begin{equation}\label{eq:linear_graph_convolution}
    \matr{H}=\tilde{\matr{A}}\matr{X}\matr{\Theta}
\end{equation}
where $\matr{H}\in\R^{n\times d_h}$ are the transformed node features.

Adding a nonlinearity $\sigma$ yields a \emph{graph convolutional layer}
\begin{equation}\label{eq:nonlinear_graph_convolution}
{\color{Red}\boxed{\color{black}
    \matr{H}=\sigma\bigl(\tilde{\matr{A}}\matr{X}\matr{\Theta}\bigr)
}}
\end{equation}
so the parameters can be learned by gradient-based optimization.

\begin{remark}[multi-hop aggregation]
Stacking $K$ layers increases the receptive field.
Ignoring nonlinearities for intuition, applying two layers gives
\(
    \tilde{\matr{A}}\bigl(\tilde{\matr{A}}\matr{X}\bigr)=\tilde{\matr{A}}^{2}\matr{X}
\)
which aggregates information from $2$-hop neighborhoods.
\end{remark}

Two common ways to aggregate up to $K$ hops are \hl[2]{polynomial filters}
\begin{equation}\label{eq:polynomial_filter}
    \matr{H}^{(K)}=\sum_{k=0}^{K}\tilde{\matr{A}}^{k}\matr{X}\matr{\Theta}^{(k)}
\end{equation}
or a sequence of first-order steps
\(
    \matr{H}^{(0)}=\matr{X}
\),
\(
    \matr{H}^{(k)}=\tilde{\matr{A}}\matr{H}^{(k-1)}\matr{\Theta}^{(k)}
\)
where nonlinearities can be inserted between layers.

\subsection{Examples of choices for \texorpdfstring{$\tilde{\matr{A}}$}{Atilde}}

Several popular layers differ essentially by the choice of $\tilde{\matr{A}}$.
A standard example is the \emph{GCN normalization}
\begin{equation}\label{eq:gcn_normalization}
{\color{Red}\boxed{\color{black}
  \tilde{\matr{A}}
  =
  \matr{D}^{-1/2}\bigl(\matr{I}_n+\matr{A}\bigr)\matr{D}^{-1/2}
}}
\end{equation}
which \hl[1]{includes self-loops} through $\matr{I}_n+\matr{A}$.
Another example is the random-walk normalization \eqref{eq:random_walk_transition_matrix}
which corresponds to averaging over neighbors with probabilities.
A third example is the \emph{GIN} choice
\begin{equation}\label{eq:gin_normalization}
    \tilde{\matr{A}}=\matr{A}+(1+\varepsilon)\matr{I}_n
\end{equation}
which strengthens the contribution of the root node via $\varepsilon$.

\subsection{Message passing}

\begin{definition}\label{def:message_passing}
Let $\vect{x}_i\in\R^{d_x}$ be the feature of node $i$ and let $\vect{e}_{ji}\in\R^{d_e}$ be the feature of edge $(j,i)$.
A \emph{message-passing} ({MP}) layer has the form
\begin{equation}\label{eq:message_passing}
{\color{Red}\boxed{\color{black}
\vect{h}_i
=
\up\Bigl(
    \vect{x}_i,\,
    \agg_{j\in N(i)}
    \bigl\{
        \mess(\vect{x}_i,\vect{x}_j,\vect{e}_{ji})
    \bigr\}
\Bigr)
}}
\end{equation}
where 
\begin{itemize}
\item $\mess$ is a \hl[2]{message function}, depending on \(\vect{x}_i\), \(\vect{x}_j\) and possibly edge features \(\vect{e}_{ji}\)
\item $\agg$ is a permutation-invariant \hl[2]{aggregation function} (e.g.\ sum, mean, max)
\item $\up$ is an \hl[2]{update function} to obtain new features from aggregated messages and previous features
\end{itemize}
Note that \(\mess\) and \(\up\) are often parametric (e.g. MLPs).
\end{definition}

\begin{remark}%[convolutions as MP]
% The layer $\matr{H}=\sigma(\tilde{\matr{A}}\matr{X}\matr{\Theta})$ 
% \eqref{eq:nonlinear_graph_convolution}
% is a special case of \autoref{def:message_passing}.
\autoref{def:message_passing} is the most general (and expressive) form of GNN, encompassing also graph convolutions \eqref{eq:nonlinear_graph_convolution} as a special case.
\eqref{eq:nonlinear_graph_convolution} can be rewritten as
\[
\vect{h}_i 
=
\sigma
\Bigl(
\sum\nolimits_{j\in N(i)}
a_{ij} \vect{x}_j \matr{\Theta}
\Bigr)
\]
with $a_{ij}=\tilde{\matr{A}}_{ij}$.
So it fits into \autoref{def:message_passing} with 
\(
\mess(\vect{x}_i,\vect{x}_j,\vect{e}_{ji}) = a_{ij} \vect{x}_j \matr{\Theta}
\) (independent of \(\vect{x}_i\) and \(\vect{e}_{ji}\)),
\(\agg = \text{sum}\), 
and
\(\up(\vect{x}_i, \cdot) = \sigma(\cdot)\) (independent of \(\vect{x}_i\)).
\end{remark}

Message passing operations whose message function \(\mess\) depends only on the sender node's features are called \emph{isotropic}.
They are called \emph{anisotropic} when also edge's or receiver node's features are exploited, i.e. \(\mess\) also depends on \(\vect{e}_{ji}\) or \(\vect{x}_i\).



\subsubsection{Graph attention networks}

are a typical example of anisotropic message passing.

\begin{enumerate}
\item 
Transform node features:
\begin{equation}\label{eq:gat_linear}
    \vect{x}_i' = \vect{x}_i \matr{\Theta}_1
\end{equation}
with \(\matr{\Theta}_1\in\R^{d_x\times d_h}\).
\item
Compute attention scores between neighbors:
\begin{enumerate}[label=\arabic{enumi}.\arabic{enumii}.]
\item 
Score (\(\vect{\theta}_2\in\R^{2d_h}\)):
\begin{equation}\label{eq:gat_scores}
    \alpha_{ij}=\sigma\bigl([\vect{x}_i' \mathbin{\|} \vect{x}_j']\vect{\theta}_2\bigr)
\end{equation}
\item 
Normalize with softmax over $N(i)$:
\begin{equation}\label{eq:gat_softmax}
    \tilde{\alpha}_{ij}
    =
    \frac{\exp(\alpha_{ij})}{\sum_{k\in N(i)}\exp(\alpha_{ik})}
\end{equation}
\end{enumerate}
\item
Aggregate (weighted sum) using attention coefficients as weights:
\begin{equation}\label{eq:gat_aggregation}
    \vect{h}_i=\sum_{j\in N(i)}\tilde{\alpha}_{ij}\,\vect{x}_j'
\end{equation}
\end{enumerate}

\subsubsection{Edge-conditioned convolution}

To incorporate edge attributes into the messages, one may use an MLP $\rho \colon \R^{d_e}\to\R^{d_x\times d_h}$ to generate edge-dependent weights.
For each edge $(j,i)$ we compute
\begin{equation}\label{eq:ecc_weights}
    \matr{\Theta}_{ji}=\rho(\vect{e}_{ji}) \in \R^{d_x\times d_h}
\end{equation}
and update nodes by
\begin{equation}\label{eq:ecc_update}
    \vect{h}_i=\vect{x}_i\matr{\Theta}_i+\sum_{j\in N(i)}\vect{x}_j\matr{\Theta}_{ji}
\end{equation}
so edges directly control how neighbor information is transformed.


% good recipe here, including tikz image
\subsection{A good recipe}
is to pre- and post-process node features with a $2$-layer MLP and to use $4$ to $6$ message-passing steps in between:
\[
\begin{tikzpicture}[
  font=\footnotesize,
  scale=0.6,
  box/.style={draw, rounded corners=2pt, minimum width=30mm, minimum height=4mm, align=center},
  mp/.style={box, fill=orange!25},
  mlp/.style={box, fill=gray!20},
  arr/.style={-{Latex[length=1mm,width=1mm]}, line width=0.5pt},
  res/.style={densely dotted, -{Latex[length=1mm,width=1mm]}, line width=0.5pt}
]
\node[mlp] (mlp1) {2-layer MLP};
\node[mp, below=6mm of mlp1] (mp1) {Message Passing};
\node[mp, below=4mm of mp1] (mp2) {Message Passing};
\node[mp, below=4mm of mp2] (mp3) {Message Passing};
\node[mp, below=4mm of mp3] (mp4) {Message Passing};
\node[mlp, below=6mm of mp4] (mlp2) {2-layer MLP};

\draw[arr] (mlp1) -- coordinate[pos=0.5] (res1out) (mp1);
\draw[arr] (mp1) -- coordinate[pos=0.3] (res1in) coordinate[pos=0.55] (res2out) (mp2);
\draw[arr] (mp2) -- coordinate[pos=0.3] (res2in) coordinate[pos=0.55] (res3out) (mp3);
\draw[arr] (mp3) -- coordinate[pos=0.3] (res3in) coordinate[pos=0.55] (res4out) (mp4);
\draw[arr] (mp4) -- coordinate[pos=0.5] (res4in) (mlp2);

% residual (side) connections: left -> down -> right, bypassing each MP block
\foreach \a/\b in {res1out/res1in, res2out/res2in, res3out/res3in, res4out/res4in} {
  \draw[res]
    ($(\a)+(0mm,0)$) --
    ($(\a)+(-32mm,0)$) --
    ($(\b)+(-32mm,0)$) --
    ($(\b)+(0mm,0)$);
}
\end{tikzpicture}
\]

Message passing at the $\ell$-th layer:
\begin{enumerate}
\item
Message:
\begin{equation}\label{eq:good_recipe_message}
    \vect{m}^{\ell}_{ji}
    =
    \operatorname{PReLU}\Bigl(
        \operatorname{BatchNorm}\bigl(
            \vect{h}^{\ell}_{j}\matr{\Theta}^{\ell} + \vect{b}^{\ell}
        \bigr)
    \Bigr)
\end{equation}
% where $\matr{\Theta}^{\ell}$ and $\vect{b}^{\ell}$ are learnable parameters.
\item
Aggregate by summation:
\begin{equation}\label{eq:good_recipe_aggregation}
    \vect{m}^{\ell}_{i}
    =
    \sum_{j\in N(i)} \vect{m}^{\ell}_{ji}
\end{equation}
\item
Update by concatenation:
\begin{equation}\label{eq:good_recipe_update}
    \vect{h}^{\ell+1}_{i}
    =
    \vect{h}^{\ell}_{i} \mathbin{\|} \vect{m}^{\ell}_{i}
\end{equation}
\end{enumerate}

\begin{remark}
This message-passing instance is \emph{isotropic}, since the message \eqref{eq:good_recipe_message} depends only on the sender embedding $\vect{h}^{\ell}_{j}$ and not on edge attributes or the receiver features.
\end{remark}



\subsection{Over-smoothing}

Repeated graph convolutions tend to reduce feature differences across neighbors, behaving like low-pass filtering on the graph.
After many layers, node representations can become almost indistinguishable within connected components, which can harm node-level prediction tasks.




\clearpage

\section{Pooling on graphs}

Pooling builds coarser graphs to reduce size or to obtain hierarchical representations.

\subsection{SRC decomposition}
\begin{enumerate}
\item \textcolor{red}{\textbf{S}election}: \label{def:selection}
A selection operator computes $K$ supernodes
\[
    \operatorname{SEL}\colon G \mapsto S=\{S_1,\dots,S_K\}
\]
where each $S_k$ is a set of nodes equipped with nonnegative scores.
Equivalently, selection can be encoded by a matrix $\matr{S}\in\R^{K\times n}$.

\item \textcolor{red}{\textbf{R}eduction}: \label{def:reduction}
Given a selection matrix $\matr{S}\in\R^{K\times n}$ and node features $\matr{X}\in\R^{n\times d_x}$, a typical reduction is the weighted aggregation
\[
    \matr{X}'=\matr{S}\matr{X}
\]
which produces pooled features $\matr{X}'\in\R^{K\times d_x}$.

\item \textcolor{red}{\textbf{C}onnection}: \label{def:connection}
A common connection rule builds the pooled adjacency by aggregating edges between supernodes.
Using the same $\matr{S}$, a standard choice is
\[
    \matr{A}'=\matr{S}\matr{A}\matr{S}^{\top}
\]
which yields $\matr{A}'\in\R^{K\times K}$.
\end{enumerate}

\begin{remark}[spectral intuition]
Low-frequency eigenvectors of the Laplacian reveal coarse clusters.
A classical pipeline performs clustering (e.g.\ $k$-means) in the space spanned by the first few Laplacian eigenvectors, but this can be expensive and ignores attributes.
\end{remark}

\subsection{Global pooling}

For graph-level tasks, one often needs a graph-to-vector map.
A \emph{global pooling} (or \emph{readout}) aggregates node embeddings $\{\vect{h}_i\}_{i\in V}$ into a single vector and must be permutation-invariant.
Typical choices include sum, mean or max pooling, as well as attention-weighted sums.




\end{document}
