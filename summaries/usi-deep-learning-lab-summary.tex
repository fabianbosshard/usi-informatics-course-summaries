% author: Fabian Bosshard % © CC BY 4.0

\documentclass[9pt, headings=standardclasses, parskip=half]{scrartcl}
\usepackage{ifthen}
\usepackage{iftex}
\usepackage{csquotes}

\usepackage[T1]{fontenc}     % handle accented glyphs properly
\usepackage[english]{babel}  % load the hyphenation patterns you need



\usepackage[automark]{scrlayer-scrpage}
% \clearpairofpagestyles
% \ofoot{\pagemark} % für ein einseitiges Dokument: \ofoot platziert den Inhalt in der äußeren Fußzeile (unten rechts)
\pagestyle{scrheadings}

% \renewcommand{\familydefault}{\sfdefault} % sans serif font for text (math font is still serif)

\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{soulutf8}
\usepackage{xparse}

\ExplSyntaxOn
\tl_new:N \__l_SOUL_argument_tl
\cs_set_eq:Nc \SOUL_start:n { SOUL@start }
\cs_generate_variant:Nn \SOUL_start:n { V }
\cs_set_protected:cpn {SOUL@start} #1
 {
  \tl_set:Nn \__l_SOUL_argument_tl { #1 }
  \regex_replace_all:nnN
   { \c{\(} (.*?) \c{\)} } % look for \(...\) (lazily)
   { \cM\$ \1 \cM\$ }      % replace with $...$
   \__l_SOUL_argument_tl
  \SOUL_start:V \__l_SOUL_argument_tl % do the usual
 }
\ExplSyntaxOff

% save soul's \hl under a private name
\let\SOULhl\hl
\renewcommand{\hl}[1]{\SOULhl{#1}} % keep plain \hl working if you want

% punchy highlighter colors
\definecolor{HLgreen}{HTML}{77DD77}
\definecolor{HLyellow}{HTML}{FFFF66}
\definecolor{HLorange}{HTML}{FFB347}
\definecolor{HLred}{HTML}{FF6961}
\definecolor{HLpink}{HTML}{FFB6C1}
\definecolor{HLturquoise}{HTML}{40E0D0}



% master highlight macro: \hl[level]{text}, default = green
\RenewDocumentCommand{\hl}{O{1} m}{%
  \begingroup
  \IfEqCase{#1}{%
    {1}{\sethlcolor{HLgreen}\SOULhl{#2}}%
    {2}{\sethlcolor{HLyellow}\SOULhl{#2}}%
    {3}{\sethlcolor{HLorange}\SOULhl{#2}}%
    {4}{\sethlcolor{HLred}\SOULhl{#2}}%
    {5}{\sethlcolor{red}\SOULhl{#2}}%
    {6}{\sethlcolor{HLturquoise}\SOULhl{#2}}%
  }[\PackageError{hl}{Undefined highlight level: #1}{}]%
  \endgroup
}








% \usepackage[left=20mm, right=20mm, top=20mm, bottom=30mm]{geometry}
% \usepackage[left=25mm, right=25mm, top=20mm, bottom=30mm]{geometry}
% \usepackage[left=30mm, right=30mm, top=20mm, bottom=30mm]{geometry}
% \usepackage[left=20mm, right=60mm, top=20mm, bottom=30mm]{geometry}
\usepackage[left=35mm, right=35mm, top=20mm, bottom=30mm]{geometry}


\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{mathdots} % without this package, only \vdots and \ddots are taken from the text font (not the math font), which looks bad if the text font is different from the math font
\usepackage{accents}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Kern}{kern}
\DeclareMathOperator{\Trace}{trace}
\DeclareMathOperator{\Rank}{rank}

\usepackage{enumitem}
\renewcommand{\labelitemi}{\textbullet}
\renewcommand{\labelitemii}{\raisebox{0.1ex}{\scalebox{0.8}{\textbullet}}}
\renewcommand{\labelitemiii}{\raisebox{0.2ex}{\scalebox{0.6}{\textbullet}}}
\renewcommand{\labelitemiv}{\raisebox{0.3ex}{\scalebox{0.4}{\textbullet}}}

\usepackage{caption, subcaption}

\usepackage[backend=biber,style=numeric]{biblatex}
\addbibresource{\jobname.bib}

\usepackage{pifont}


\usepackage{tikz}
\usetikzlibrary{arrows, arrows.meta, shapes, positioning, calc, fit, patterns, intersections, math, 3d, tikzmark, decorations.pathreplacing, decorations.markings}
\usepackage{tikz-dependency, tikz-qtree, tikz-qtree-compat}
\usepackage{tikz-3dplot}
\usepackage{tikzpagenodes}
% \usetikzlibrary{external}
% \tikzexternalize[prefix=tikz/]
\usepackage{pgfplots}



\usetikzlibrary{shapes,arrows,positioning,calc,chains,scopes}

% colors
\definecolor{snowymint}{HTML}{E3F8D1}
\definecolor{wepeep}{HTML}{FAD2D2}
\definecolor{portafino}{HTML}{F5EE9D}
\definecolor{plum}{HTML}{DCACEF}
\definecolor{sail}{HTML}{A3CEEE}
\definecolor{highland}{HTML}{6D885A}

\tikzstyle{signal}=[arrows={-latex},draw=black,line width=1.5pt,rounded corners=4pt]

% RNN
\tikzstyle{block}=[draw=black,line width=1.0pt]
\tikzstyle{cell}=[style=block,draw=highland,fill=snowymint,
    rounded corners]
\tikzstyle{celllayer}=[style=block,draw,fill=portafino,
    inner sep=1pt,outer sep=0,
    minimum width=28pt, minimum height=14pt]
\tikzstyle{pointwise}=[style=block,circle,fill=wepeep,
    inner sep=0pt,outer sep=0, minimum size=11pt]

\def\iolen{24pt}
\def\intergape{2pt}

% MLP and CNN
\tikzstyle{netnode}=[circle, inner sep=0pt, text width=22pt, align=center, line width=1.0pt]
\tikzstyle{inputnode}=[netnode, fill=sail,draw=black]
\tikzstyle{hiddennode}=[netnode, fill=snowymint,draw=black]
\tikzstyle{outputnode}=[netnode, fill=plum,draw=black]

% Architecture
\def\layerwidth{90pt}
\def\layerheight{14pt}

\tikzstyle{layer}=[style=block, draw, fill=black!20!white,
    inner sep=1pt,outer sep=0, font=\footnotesize,
    text centered, 
    minimum width=\layerwidth, minimum height=\layerheight]

\tikzstyle{fc}=[style=layer, fill=blue!30!white]
\tikzstyle{conv}=[style=layer, fill=green!30!white]
\tikzstyle{activation}=[style=layer, fill=orange!30!white]
\tikzstyle{pool}=[style=layer, fill=red!30!white]
\tikzstyle{bn}=[style=layer, fill=cyan!30!white]
\tikzstyle{recurrent}=[style=layer, fill=purple!30!white]
\tikzstyle{softmax}=[style=layer, fill=yellow!30!white]
\tikzstyle{point}=[]
\tikzstyle{branch}=[coordinate]

\def\vlayerwidth{30pt}
\def\vlayerheight{3pt}
\def\vblockheight{28pt}

\tikzstyle{vlayer}=[minimum width=\vlayerwidth, minimum height=\vlayerheight]
\tikzstyle{vblock}=[minimum width=\vlayerwidth, minimum height=\vblockheight, text width=1cm, align=center]


% Precision, Recall
\colorlet{fn}{gray!90!green!30!white}
\colorlet{tp}{green!40!white}
\colorlet{fp}{red!40!white}
\colorlet{tn}{gray!90!red!20!white}


% define colors
\definecolor{funblue}{rgb}{0.10, 0.35, 0.66}
\definecolor{alizarincrimsonred}{rgb}{0.85, 0.17, 0.11}
\definecolor{amethyst}{rgb}{0.6, 0.4, 0.8}
\definecolor{mypurple}{rgb}{128, 0, 128} 
\definecolor{highlightpurple}{rgb}{0.6, 0.0, 0.6}
% \renewcommand{\emph}[1]{\textcolor{highlightpurple}{#1}}
% \renewcommand{\emph}[1]{\textsl{#1}}
\renewcommand{\emph}[1]{\textcolor{highlightpurple}{\textsl{#1}}}

\usepackage{thmtools}

\newlength{\thmspace}
\setlength{\thmspace}{3pt plus 1pt minus 1pt}

% ===== main assertion-style family (Theorem, Lemma, etc.) =====

\declaretheoremstyle[
  headfont=\bfseries,
  bodyfont=\normalfont,
  spaceabove=\thmspace,
  spacebelow=\thmspace,
  qed=\ensuremath{\vartriangleleft},
  postheadspace=1em
]{assertionstyle}

\declaretheorem[
  style=assertionstyle,
  name=Theorem,
  numberwithin=section   % <-- resets in each section
]{theorem}

\declaretheorem[style=assertionstyle, name=Lemma,       sibling=theorem]{lemma}
\declaretheorem[style=assertionstyle, name=Corollary,   sibling=theorem]{corollary}
\declaretheorem[style=assertionstyle, name=Proposition, sibling=theorem]{proposition}
\declaretheorem[style=assertionstyle, name=Conjecture,  sibling=theorem]{conjecture}
\declaretheorem[style=assertionstyle, name=Claim,       sibling=theorem]{claim}
\declaretheorem[style=assertionstyle, name=Fact,        sibling=theorem]{fact}


% ===== definitions =====

\declaretheoremstyle[
  headfont=\bfseries,
  bodyfont=\normalfont,
  spaceabove=\thmspace,
  spacebelow=\thmspace,
  qed=\ensuremath{\blacktriangleleft},
  postheadspace=1em
]{definitionstyle}

\declaretheorem[style=definitionstyle, name=Definition, numberwithin=section]{definition}
\declaretheorem[style=definitionstyle, name=Problem,    sibling=definition]{problem}

% ===== exercises, solutions =====
\declaretheoremstyle[
  headfont=\bfseries,
  bodyfont=\normalfont,
  spaceabove=\thmspace,
  spacebelow=\thmspace,
  qed=\ding{45},
  postheadspace=1em
]{exercisestyle}
\declaretheorem[style=exercisestyle, name=Exercise, numberwithin=section]{exercise}
\declaretheoremstyle[
  headfont=\bfseries\color{red},
  bodyfont=\normalfont,
  spaceabove=\thmspace,
  spacebelow=\thmspace,
  qed=\ensuremath{\color{red}\blacktriangleleft},
  postheadspace=1em
]{solutionstyle}
\declaretheorem[style=solutionstyle, name=Solution, numbered=no]{solution}

% ===== proofs =====

\declaretheoremstyle[
  headfont=\bfseries,
  bodyfont=\normalfont,
  spaceabove=6pt,
  spacebelow=6pt,
  qed=\ensuremath{\square},
  postheadspace=1em
]{proofstyle}

\let\proof\relax
\let\endproof\relax
\declaretheorem[style=proofstyle, name=Proof,    numbered=no]{proof}


% ===== remarks, cautions, examples =====

\declaretheoremstyle[
  headfont=\bfseries,
  bodyfont=\normalfont\normalsize,
  spaceabove=\thmspace,
  spacebelow=\thmspace,
  qed=\ensuremath{\blacktriangleleft},
  postheadspace=1em
]{remarkstyle}

\declaretheorem[style=remarkstyle, name=Remark,   numberwithin=section]{remark}

\declaretheoremstyle[
  headfont=\bfseries\color{funblue},
  bodyfont=\normalfont\normalsize,
  spaceabove=\thmspace,
  spacebelow=\thmspace,
  qed=\ensuremath{\color{funblue}\blacktriangleleft},
  postheadspace=1em
]{examplestyle}

\declaretheorem[style=examplestyle, name=Example, sibling=remark]{example}


\declaretheoremstyle[
  headfont=\color{alizarincrimsonred}\bfseries,
  bodyfont=\normalfont\normalsize,
  spaceabove=\thmspace,
  spacebelow=\thmspace,
  qed=\ensuremath{\color{alizarincrimsonred}\blacktriangleleft},
  postheadspace=1em
]{cautionstyle}

\declaretheorem[style=cautionstyle, name=Caution, sibling=remark]{caution}

\declaretheoremstyle[
  headfont=\bfseries,
  bodyfont=\normalfont\footnotesize,
  spaceabove=\thmspace,
  spacebelow=\thmspace,
  postheadspace=1em
]{smallremarkstyle}

\declaretheorem[style=smallremarkstyle, name=Remark, sibling=remark]{smallremark}


\declaretheoremstyle[
  headfont=\bfseries\color{amethyst},
  bodyfont=\normalfont,
  spaceabove=6pt,
  spacebelow=6pt,
  qed=\ensuremath{\color{amethyst}\blacktriangleleft},
  postheadspace=1em
]{digressionstyle}

\declaretheorem[style=digressionstyle, name=Digression, sibling=remark]{digression}


\numberwithin{equation}{section} % equations numbered within sections






\newenvironment{verticalhack}
  {\begin{array}[b]{@{}c@{}}\displaystyle}
  {\\\noalign{\hrule height0pt}\end{array}} % to make qed symbol aligned


\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[italicComments=false]{algpseudocodex} % macht probleme mit TikZ externalization

\newcommand*{\algorithmautorefname}{Algorithm}

% commands for functions etc.
\newcommand{\im}{\operatorname{Im}}

% commands for vectors and matrices
\newcommand*\matrspace{0.8mu}
\newcommand{\matr}[1]{%
  \mspace{\matrspace}%
  \underline{%
    \mspace{-\matrspace}%
    \smash[b]{\boldsymbol{#1}}%
    \mspace{-\matrspace}%
  }%
  \mspace{\matrspace}%
}

\newcommand{\vect}[1]{{\boldsymbol{#1}}}

% commands for derivatives
\newcommand{\dif}{\mathrm{d}}

\DeclareMathOperator{\sigm}{\sigma}
\newcommand{\diff}{\mathop{}\!\mathrm{d}}

% commands for number sets
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\F}{\mathbb{F}}

% commands for probability
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Exp}{\operatorname{E}}
\newcommand{\Prob}{\operatorname{P}}
\newcommand{\numof}{\ensuremath{\# \,}} % number of elements in a set

% algorithm helpers
\algnewcommand{\TO}{, \ldots ,}
\algnewcommand{\DOWNTO}{, \ldots ,}
\algnewcommand{\OR}{\vee}
\algnewcommand{\AND}{\wedge}
\algnewcommand{\NOT}{\neg}
\algnewcommand{\LEN}{\operatorname{len}}
\algnewcommand{\tru}{\ensuremath{\mathrm{\texttt{true}}}}
\algnewcommand{\fals}{\ensuremath{\mathrm{\texttt{false}}}}
\algnewcommand{\append}{\circ}

\algnewcommand{\nil}{\ensuremath{\mathrm{\textsc{nil}}}}

\newcommand{\attrib}[2]{\ensuremath{#1\mathtt{.}\mathtt{#2}}} % object.field with field in math typewriter (like code)
\newcommand{\attribnormal}[2]{\ensuremath{#1\mathtt{.}#2}} 

\title{Deep Learning}
\author{Fabian Bosshard}
\date{\today}

\usepackage[
  linktoc=none,
  pdfauthor={Fabian Bosshard},
  pdftitle={USI - Deep Learning - Course Notes},
  pdfkeywords={USI, deep learning, course notes, informatics},
  colorlinks=false,  
  pdfborder={0.0 0.0 0.0},      
  linkbordercolor={0 0.6 1},      % internal links
  urlbordercolor={0 0.6 1},       % URLs
  citebordercolor={0 0.6 1}       % citations
]{hyperref}

\DeclareTOCStyleEntry[linefill=\dotfill]{tocline}{section}
\DeclareTOCStyleEntry[linefill=\dotfill]{tocline}{subsection}
\DeclareTOCStyleEntry[linefill=\dotfill]{tocline}{subsubsection}
\makeatletter
\newlength\FB@toclinkht
\newlength\FB@toclinkdp
\setlength\FB@toclinkht{.80\ht\strutbox}
\setlength\FB@toclinkdp{.40\dp\strutbox}

\let\FB@orig@contentsline\contentsline
\renewcommand*\contentsline[4]{%
  \begingroup
    \Hy@safe@activestrue
    \edef\Hy@tocdestname{#4}%
    \FB@orig@contentsline{#1}{%
      \Hy@raisedlink{\hyper@anchorstart{toc:\Hy@tocdestname}\hyper@anchorend}%
      \leavevmode
      \rlap{%
        \hyper@linkstart{link}{\Hy@tocdestname}%
          \raisebox{0pt}[\FB@toclinkht][\FB@toclinkdp]{%
            \hbox to \dimexpr\hsize-\parindent\relax{\hfil}%
          }%
        \hyper@linkend
      }%
      #2%
    }{#3}{#4}%
  \endgroup
}

\let\FB@orig@sectionlinesformat\sectionlinesformat
\renewcommand{\sectionlinesformat}[4]{%
  \ifx\@currentHref\@empty
    \FB@orig@sectionlinesformat{#1}{#2}{#3}{#4}%
  \else
    \leavevmode
    \hyper@linkstart{link}{toc:\@currentHref}%
      \@hangfrom{\hskip #2\relax #3}{#4}%
    \hyper@linkend
    \par
  \fi
}
\makeatother

\usepackage[
  type     = {CC},
  modifier = {by},
  version  = {4.0},
]{doclicense}
\usepackage{cleveref}

\makeatletter
\renewcommand{\theHequation}{\thesection.\arabic{equation}}
\renewcommand{\theHtheorem}{\thesection.\arabic{theorem}}
\renewcommand{\theHlemma}{\theHtheorem}
\renewcommand{\theHcorollary}{\theHtheorem}
\renewcommand{\theHproposition}{\theHtheorem}
\renewcommand{\theHconjecture}{\theHtheorem}
\renewcommand{\theHclaim}{\theHtheorem}
\renewcommand{\theHfact}{\theHtheorem}

\renewcommand{\theHdefinition}{\thesection.\arabic{definition}}
\renewcommand{\theHproblem}{\theHdefinition}

\renewcommand{\theHexercise}{\thesection.\arabic{exercise}}

\renewcommand{\theHremark}{\thesection.\arabic{remark}}
\renewcommand{\theHexample}{\theHremark}
\renewcommand{\theHcaution}{\theHremark}
\renewcommand{\theHsmallremark}{\theHremark}
\renewcommand{\theHdigression}{\theHremark}
\makeatother


\begin{document}

\maketitle

\tableofcontents




\clearpage

\section{Polynomial Regression with Gradient Descent}

\subsection{Model and data}
\begin{itemize}
  \item Polynomial model $p(z)=\sum_{k=0}^4 w_k z^k$.
  \item Feature vector $\vect{x}=[1, z, z^2, z^3, z^4]^T$ and prediction $\hat y=\vect{x}^T\vect{w}$.
  \item Data generation $y_i=p(z_i)+\varepsilon_i$ with $\varepsilon_i\sim\mathcal N(0,\sigma^2)$.
  \item Design matrix $\matr{X}$ contains rows $\vect{x}_i^T$.
\end{itemize}

\subsection{Gradient descent and implementation details}
\begin{itemize}
  \item Mean-squared error
  \[
    \mathcal{L}(\vect{w})=\frac{1}{N}\sum_{i=1}^N \bigl(y_i-\vect{x}_i^T\vect{w}\bigr)^2
  \]
  \item Gradient in matrix form
  \[
    \nabla_{\vect{w}}\mathcal{L}(\vect{w})=-\frac{2}{N}\matr{X}^T\bigl(\vect{y}-\matr{X}\vect{w}\bigr)
  \]
  \item Update rule
  \[
    \vect{w}^{(t+1)}=\vect{w}^{(t)}-\eta\,\nabla_{\vect{w}}\mathcal{L}\bigl(\vect{w}^{(t)}\bigr)
  \]

  \item Learning rate $\eta>0$ sets the step size.
  Too small $\eta$ gives slow convergence.
  Too large $\eta$ yields oscillation or divergence.
  In code, $\eta=0.01$ gave a smooth decrease and reached near noise-level MSE in reasonable steps.

  \item Bias handling in \texttt{nn.Linear}:
  since $\vect{x}$ already contains a constant feature, using \texttt{bias=False} makes $w_0$ act as the bias.
  If \texttt{bias=True} is used while keeping the constant feature, the model has two redundant bias terms.

  \item Training bookkeeping:
  \begin{itemize}
    \item A \textbf{step} is one parameter update (one batch forward/backward plus \texttt{optimizer.step()}).
    \item An \textbf{epoch} is one full pass over the training set.
    \item For mini-batches, steps per epoch equal $\text{num\_samples}/\text{batch\_size}$.
  \end{itemize}

  \item Optimizer calls:
  \begin{itemize}
    \item \texttt{optimizer.zero\_grad()} clears accumulated gradients.
    \item \texttt{optimizer.step()} updates parameters using the stored gradients (here, the SGD rule).
  \end{itemize}
\end{itemize}

\subsection{Generalization, expected optimum loss, and common losses}
\begin{itemize}
  \item Validation set:
  held-out data (no gradient updates) to monitor generalization and tune hyperparameters without touching the test set.
  Overfitting appears as training loss decreasing while validation loss increases.
  Typical countermeasures are smaller models, explicit regularization such as weight decay ($L_2$), more data or augmentation, and early stopping.

  \item Expected MSE at the optimum:
  if the model recovers $\vect{w}^*$ and $y=\vect{x}^T\vect{w}^*+\varepsilon$ with $\varepsilon\sim\mathcal N(0,\sigma^2)$ then
  \[
    \Exp\bigl[\bigl(y-\vect{x}^T\vect{w}^*\bigr)^2\bigr]=\sigma^2
  \]
  with $\sigma=0.5$ giving an expected optimum MSE of about $0.25$.

  \item Classification losses:
  binary cross-entropy for binary classification and cross-entropy for multi-class classification.

  \item Adaptive optimizers use per-parameter learning rates based on past gradients.
  Examples are AdaGrad, RMSProp, and Adam.
  SGD uses a fixed learning rate and is a simple baseline.
\end{itemize}

\pagebreak
\section{Convolutional Neural Networks}

\subsection{Dataset \& Basics}

\begin{itemize}
  \item CIFAR-10: 50k train, 10k test, RGB images of size \(3 \times 32 \times 32\), 10 classes.
  \item Normalization: per-channel mean and standard deviation so inputs have roughly mean 0 and variance 1.
  \item Train/val split: original test set split into validation and final test.
\end{itemize}

\subsection{Parameters}

\begin{itemize}
  \item Convolutional layers: filter weight tensors +\tikzmark{convline} optional bias vectors.
  \item Fully connected layers: weight matrices \(\matr{W}\) + bias vectors.
  \item BatchNorm: scale \(\gamma\) and shift \(\beta\) parameters.
  \item All trainable parameters are in \texttt{model.parameters()} (tensors on the chosen device).
\end{itemize}


\begin{tikzpicture}[remember picture,overlay, font=\footnotesize, ann/.style={Stealth-,rounded corners=2pt,line cap=round}]
  % start point: at the end of the "Convolutional layers" line
  \coordinate (start) at ($(pic cs:convline)+(0,0.2)$);

  % place the handwritten-style note up-right
  \node[anchor=west, color=funblue] (note) at ($(start)+(1,0.6)$)
  {
    \(\#\text{params}=(k_w k_h\,C_{\text{in}} \,+\tikzmarknode{plusone}{1}\,)\,C_{\text{out}}\)
  };

  % curvy arrow up-right
  \draw[-{Stealth[length=2mm]}, funblue]
    (start) .. controls +(0.9,0.1) and +(-1.2,0.1) .. (note.west);


  % ---- padding (numerator): go UP then RIGHT
  \coordinate (biasA) at ($(plusone.north) + (0,3mm)$);    % vertical end
  \coordinate (biasB) at ($(biasA) + (10mm,0)$);        % right end (tip)
  \draw[ann, funblue] (plusone.north) -- (biasA) -- (biasB);
  \node[anchor=west,inner sep=1pt, color=funblue] at (biasB) {bias};
\end{tikzpicture}


\[
n_{\text{out}}
=
\left\lfloor
\frac{
n_{\text{in}}
+2\tikzmarknode{pad}{p}
-d\bigl(\tikzmarknode{ker}{k}-1\bigr)
-1
}{
\tikzmarknode{str}{s}
}
+1
\right\rfloor
% --- reserve space so annotations fit nicely
% \vphantom{\rule{0pt}{16mm}}
% \hphantom{\rule{32mm}{0pt}}
\]

\begin{tikzpicture}[overlay,remember picture, font=\footnotesize,
  >={Stealth[length=2mm]},
  ann/.style={Stealth-,rounded corners=2pt,line cap=round}
]
  % ---- padding (numerator): go UP then RIGHT
  \coordinate (padA) at ($(pad.north) + (0,6mm)$);    % vertical end
  \coordinate (padB) at ($(padA) + (10mm,0)$);        % right end (tip)
  \draw[ann, funblue] (pad.north) -- (padA) -- (padB);
  \node[anchor=west,inner sep=1pt, color=funblue] at (padB) {padding};

  % ---- kernel size (numerator): go UP then RIGHT
  \coordinate (kerA) at ($(ker.north) + (0,3mm)$);
  \coordinate (kerB) at ($(kerA) + (14mm,0)$);
  \draw[ann, funblue] (ker.north) -- (kerA) -- (kerB);
  \node[anchor=west,inner sep=1pt, color=funblue] at (kerB) {kernel size};

  % ---- stride (denominator): go DOWN then RIGHT
  \coordinate (strA) at ($(str.south) + (0,-4mm)$);
  \coordinate (strB) at ($(strA) + (10mm,0)$);
  \draw[ann, funblue] (str.south) -- (strA) -- (strB);
  \node[anchor=west,inner sep=1pt, color=funblue] at (strB) {stride};
\end{tikzpicture}

\subsection{Architecture choices and training practice}

\paragraph{Why convolutional layers instead of fully connected}
\begin{itemize}
  \item Local receptive fields exploit spatial locality (neurons see small patches).
  \item Weight sharing reuses each filter across spatial positions, giving far fewer parameters than a dense layer on raw pixels.
  \item Translation equivariance means a shift in the input induces a corresponding shift in feature maps.
  \item A fully connected model on $3\times 32\times 32$ inputs would have a huge number of parameters and would ignore spatial structure.
\end{itemize}

\paragraph{Pooling layers}
\begin{itemize}
  \item Role:
  downsample feature maps (e.g. max pool $2\times 2$), increase the effective receptive field in deeper layers, reduce activations before fully connected layers, and provide some invariance to small translations.
  \item If pooling is removed:
  feature maps stay large, which increases parameters and memory, raises overfitting risk, and slows training.
  A common substitute is strided convolutions or other downsampling.
\end{itemize}

\paragraph{Dropout, BatchNorm, and other regularization}
\begin{itemize}
  \item Dropout during training zeros activations with probability $p$.
  It reduces co-adaptation and overfitting.
  Typically $p$ is smaller in convolutional blocks and larger before the final dense output layer.
  \item Batch Normalization normalizes mini-batch activations to roughly zero mean and unit variance and then applies learnable scale and shift parameters $\gamma$ and $\beta$.
  It stabilizes gradients, speeds up training, allows larger learning rates, and adds some regularization due to batch noise.
  \item Additional regularization includes weight decay ($L_2$ penalty), data augmentation, early stopping, and using a smaller CNN.
\end{itemize}

\paragraph{Data augmentation}
\begin{itemize}
  \item \texttt{RandomCrop(32, padding=4)} introduces small translations and zoom-like effects.
  \item \texttt{RandomHorizontalFlip} encourages left--right invariance.
  \item \texttt{RandomPerspective} adds mild geometric distortions.
  \item Overall, augmentation makes the effective training set larger and more diverse and improves validation and test generalization.
\end{itemize}

\paragraph{Activation functions}
\begin{itemize}
  \item ReLU uses $f(x)=\max(0,x)$ with a hard cutoff at $0$.
  \item GeLU is smooth and down-weights negative values rather than discarding them.
  \item GeLU can yield smoother optimization and sometimes a small accuracy gain in vision settings.
\end{itemize}

\paragraph{Optimization settings in the experiment}
\begin{itemize}
  \item Base model: learning rate about $0.03$, training for $5$ epochs, batch size $32$.
  This learns quickly (reaching at least $65\%$ test accuracy) but tends to overfit if trained much longer without strong regularization.
  \item Improved model: learning rate about $0.01$ together with BatchNorm and a scheduler, training for $40$ epochs, batch size $64$.
  Strong regularization (dropout, augmentation, weight decay) makes longer training beneficial.
\end{itemize}

\paragraph{Momentum, weight decay, and learning-rate scheduling}
\begin{itemize}
  \item Momentum (e.g. $0.9$) keeps an exponential moving average of gradients, giving smoother updates, less oscillation, and faster progress along consistent directions.
  \item Weight decay (e.g. $10^{-6}$) adds an $L_2$ penalty and encourages smaller weights, which reduces overfitting.
  \item \texttt{ReduceLROnPlateau} monitors validation loss and decreases the learning rate when validation performance stops improving.
  A common strategy is a larger learning rate early and a smaller one later for fine-tuning.
\end{itemize}

\paragraph{Overfitting and run-to-run variability}
\begin{itemize}
  \item If training loss decreasing while validation loss flattens or increases \(\Rightarrow\) overfitting.
  \item Try using a deeper network with more channels and add augmentation, dropout, BatchNorm, weight decay, scheduler.
  This reduces the train-validation gap and substantially improves test accuracy.
  \item Different random seeds change initialization $\vect{w}$ and mini-batch order.
  Test accuracy can vary by a few percentage points, so comparisons should average over runs or at least account for variance.
\end{itemize}

\clearpage

\section{Recurrent models (RNN and LSTM)}



% \subsection{Core definitions (what to say in an oral exam)}

\textbf{Sequence processing}:
Given an input sequence $(\vect{x}_1,\dots,\vect{x}_T)$, a recurrent layer processes it \emph{step by step} while carrying a state forward in time.

\subsection{RNN}

\textbf{Hidden state (RNN)}:
The \emph{hidden state} $\vect{h}_t\in\R^h$ is the internal state of a recurrent layer at time $t$.
It summarizes past information and is used (i) to produce outputs and (ii) to influence the next state.

\textbf{Vanilla RNN update rule}:
A standard RNN layer is defined by one parameter set (shared across time):
\[
\vect{h}_t
=
\tanh \bigl(\matr{W}_{xh}\vect{x}_t+\matr{W}_{hh}\vect{h}_{t-1}+\vect{b}_h\bigr)
\]
where \(\matr{W}_{xh}\in\R^{h\times d},\ \matr{W}_{hh}\in\R^{h\times h},\ \vect{b}_h\in\R^h\).
At the first step one initializes $\vect{h}_0$ (typically $\vect{0}$, sometimes learned).
% RNN
\[
\begin{tikzpicture}[thick, node distance=30pt and 30pt, on grid, font=\small]
    \node[cell, minimum width=200pt, minimum height=110pt, anchor=north west] (b) at (-2pt,0pt) {};
    
    \coordinate (hin) at (0pt,-20pt);
    \draw[signal] (hin) +(-\iolen, 0pt) node[above] {$\vect{h}_{t-1}$} -- (hin);
    \coordinate (hout) at (200pt,-20pt);
    \draw[signal] (hout) -- +(\iolen,0pt) node[above left] {$\vect{h}_{t}$};
    \coordinate (h) at (184pt,0pt);
    \draw[signal] (h) -- +(0,\iolen) node[left] {$\vect{h}_{t}$};
    \coordinate (x) at (14pt,-110pt);
    \draw[signal, -] (x) +(0pt,-\iolen) node[left] {$\vect{x}_{t}$} -- (x);
    
    \node[celllayer] at (b) (a) {$\tanh$};
    
    \draw[signal, -] (a) |- (hout);
    \draw[signal, -] (hout -| h) +(-10pt,0pt) -| (h);
    \coordinate (hx) at (60pt,-100pt);
    \draw[signal, -] (x) |- (hx);
    \draw[signal, -] (hx) -| (a);
    \draw[signal, -] (hin) +(-10pt,0pt) -| +(50pt,-50pt) |- (hx) +(10pt,0pt);
\end{tikzpicture}
\]
\textbf{Unrolling and weight sharing}:
A \emph{single} recurrent layer is \emph{unrolled} over timesteps:
the boxes drawn at different $t$ are \emph{copies of the same layer}.
Thus, $\matr{W}_{xh},\matr{W}_{hh},\vect{b}_h$ are \emph{shared across all timesteps}.
(Weights are \emph{not} shared across \emph{stacked} layers.)



\begin{itemize}
    \item issue: exploding gradient. solution: gradient clipping.
    \item issue: vanishing gradient. solution: LSTM.
\end{itemize}




\subsection{LSTM}

\textbf{two states}:
An LSTM keeps
\(
\vect{h}_t\in\R^h
\) (hidden/output state; what is exposed),
and
\(
\vect{c}_t\in\R^h 
\) (cell state; long-term memory).
Initialize $(\vect{h}_0,\vect{c}_0)$ (typically $(\vect{0},\vect{0})$).
% LSTM
\[
\begin{tikzpicture}[thick, node distance=30pt and 30pt, on grid, font=\small]
    \node[cell, minimum width=200pt, minimum height=110pt, anchor=north west] (b) at (-2pt,0pt) {};
    
    \coordinate (cin) at (0pt,-20pt);
    \draw[signal] (cin) +(-\iolen, 0pt) node[above] {$\vect{c}_{t-1}$} -- (cin);
    \coordinate (cout) at (200pt,-20pt);
    \draw[signal] (cout) -- +(\iolen,0pt) node[above left] {$\vect{c}_{t}$};
    \coordinate (hin) at (0pt,-100pt);
    \draw[signal] (hin) +(-\iolen, 0pt) node[above] {$\vect{h}_{t-1}$} -- (hin);
    \coordinate (hout) at (200pt,-100pt);
    \draw[signal] (hout) -- +(\iolen,0pt) node[above left] {$\vect{h}_{t}$};
    \coordinate (h) at (184pt,0pt);
    \draw[signal] (h) -- +(0,\iolen) node[left] {$\vect{h}_{t}$};
    \coordinate (x) at (14pt,-110pt);
    \draw[signal, -] (x) +(0pt,-\iolen) node[left] {$\vect{x}_{t}$} -- (x);
    
    \node[celllayer] (f) at (32pt,-80pt) {$\sigm$};
    \node[celllayer, right=34pt of f] (i) {$\sigm$};
    \node[celllayer, right=34pt of i] (c) {$\tanh$};
    \node[celllayer, right=34pt of c] (o) {$\sigm$};
    
    \node[pointwise, above=60pt of f] (fm) {$\times$};
    
    \node[pointwise, above=30pt of c] (cm) {$\times$};
    \node[pointwise, above=30pt of cm] (cmp) {$+$};
    
    \node[pointwise, above right=20pt and 20 pt of o] (om) {$\times$};
    \node[pointwise, ellipse, above=20pt of om] (omt) {$\tanh$};
    
    \draw[signal] (f) edge node[near start,left] {$\vect{f}_t$} (fm);
    
    \draw[signal, -] (c) edge node[pos=0.5,left] {$\tilde{\vect{c}}_t$} (cm); 
    \draw[signal] (cm) to (cmp);
    \draw[signal] (i) |- (cm) node[near start,left] {$\vect{i}_t$};
    
    \draw[signal] (o) |- (om) node[pos=0.3,left] {$\vect{o}_t$};
    
    \draw[signal, -] (fm) -- (cmp);
    
    \draw[signal, -] (cmp) -| (omt);
    \draw[signal, -] (omt) -- (om);
    
    \draw[signal] (cin) +(-\iolen, 0) node[above] {$\vect{c}_{t-1}$} -- +(0,0);
    
    \draw[signal, -] (cin) +(-10pt,0) -- (fm);
    
    \draw[signal] (hin) +(-\iolen, 0) node[above] {$\vect{h}_{t-1}$} -- +(0,0);
    
    \draw[signal, -] (hin) +(-10pt,0) -| (o);
    \draw[signal, -] (hin) -| (c);
    \draw[signal, -] (hin) -| (i);
    \draw[signal, -] (hin) -| (f);
    
    \draw[signal] (cout) -- +(\iolen,0) node[above left] {$\vect{c}_{t}$};
    
    \draw[signal, -] (cmp) -- (cout);
    
    \draw[signal] (hout) -- +(\iolen,0) node[above left] {$\vect{h}_{t}$};
        
    \draw[signal, -] (om) |- (hout);
    
    \draw[signal, -, shorten >=\intergape] (h |- hout) +(-10pt,0) -| (h |- cout);
    \draw[signal, shorten <=\intergape] (h |- cout) -- +(0,\iolen+20pt) node[left] {$\vect{h}_{t}$};
    
    \draw[signal, -] (x) |- (f |- hin);
\end{tikzpicture}
\]
\textbf{LSTM gates and update}:
Gates are elementwise values in $[0,1]$ (via $\sigm$) that control \emph{forget, write, expose}:
\[
\begin{aligned}
\vect{f}_t &= \sigm(\matr{W}_{xf}\vect{x}_t+\matr{W}_{hf}\vect{h}_{t-1}+\vect{b}_f)\\
\vect{i}_t &= \sigm(\matr{W}_{xi}\vect{x}_t+\matr{W}_{hi}\vect{h}_{t-1}+\vect{b}_i)\\
\tilde{\vect{c}}_t &= \tanh(\matr{W}_{xc}\vect{x}_t+\matr{W}_{hc}\vect{h}_{t-1}+\vect{b}_c)\\
\vect{o}_t &= \sigm(\matr{W}_{xo}\vect{x}_t+\matr{W}_{ho}\vect{h}_{t-1}+\vect{b}_o)
\end{aligned}
\qquad
\begin{aligned}
\vect{c}_t &= \vect{f}_t\odot \vect{c}_{t-1} + \vect{i}_t\odot \tilde{\vect{c}}_t\\
\vect{h}_t &= \vect{o}_t\odot \tanh(\vect{c}_t)
\end{aligned}
\]
\emph{Forgetting} is $\vect{f}_t\odot\vect{c}_{t-1}$, \emph{adding/writing} is $\vect{i}_t\odot\tilde{\vect{c}}_t$.

\textbf{Why LSTM helps vanishing gradients (one line)}:
\[
\frac{\partial \vect{c}_t}{\partial \vect{c}_{t-1}}=\operatorname{diag}(\vect{f}_t)
\]
so if $\vect{f}_t\approx \vect{1}$, gradients can flow through many timesteps.




\medskip
long sequences could require too much memory\\
\(\rightarrow\) truncated backpropagation through time (TBPTT):
break sequences into chunks: fixed-length segments from the original sequences.
TBPTT keeps dependencies within chunks while avoiding excessive memory usage.



\subsection{Hidden state in an RNN}
$\vect{h}_t$ is the internal state at time $t$; it summarizes past inputs and is used to compute (i) outputs and (ii) the next hidden state. It is updated recurrently: $\vect{h}_t=f(\vect{x}_t,\vect{h}_{t-1})$ with shared weights across time.

\subsection{Padding for variable-length sequences}
Batches require equal tensor sizes, so shorter sequences are padded (usually with a special PAD token) to the max length in the batch.
In practice one uses a \emph{mask} to ignore padded positions in loss/metrics/attention, or uses packed sequences (concept: process only true lengths, then unpack).

\subsection{Stacked RNN}
A stacked RNN has multiple recurrent layers.
Layer $\ell$ takes input $\vect{h}^{(\ell-1)}_t$ (from the layer below at the same $t$) and its own previous state $\vect{h}^{(\ell)}_{t-1}$:
\[
\vect{h}^{(\ell)}_t = f^{(\ell)}\!\bigl(\vect{h}^{(\ell-1)}_t,\vect{h}^{(\ell)}_{t-1}\bigr)
\]
Why it helps: higher layers can learn more abstract / longer-range temporal features.
Weights are shared across time \emph{within a layer}, but \emph{not shared across layers}.

\subsection{Bidirectional RNN}
A bidirectional RNN runs one RNN forward ($1\to T$) and one backward ($T\to 1$), then combines states (often concatenation):
\[
\vect{h}^{\text{bi}}_t = [\vect{h}^{\to}_t;\ \vect{h}^{\leftarrow}_t]
\]
Advantage when the whole sequence is available (tagging, classification).
Not suitable for strictly causal/online prediction where future inputs are unknown.

\subsection{Exploding gradients in RNNs}
Backprop through time multiplies many Jacobians; norms can grow exponentially, causing unstable updates (very large gradients, NaNs/divergence).

\subsection{Vanishing gradients (especially severe in RNNs)}
The same repeated multiplication can make gradients shrink exponentially, so early timesteps get almost no learning signal.
RNNs are particularly affected because the effective depth is the sequence length $T$.
Mitigations: LSTM/GRU gating, careful initialization, normalization, residual connections, shorter BPTT windows.

\subsection{Gradient clipping}
Clip gradients to prevent exploding updates, e.g. clip global norm:
\[
\vect{g}\leftarrow \vect{g}\cdot \min\left(1,\frac{\tau}{\|\vect{g}\|}\right)
\]
Used commonly for RNN/LSTM training stability. (Helps exploding, not vanishing.)

\subsection{Activation functions and vanishing/exploding}
Sigmoid/tanh saturate: derivatives become small $\Rightarrow$ vanishing gradients in deep/time-unrolled networks.
ReLU avoids saturation in the positive region (often helps in feedforward nets) but can still explode in recurrent settings.
LSTMs use sigmoid/tanh but avoid catastrophic vanishing via the \emph{additive} cell update and the forget gate.

\subsection{Truncated backpropagation through time (TBPTT)}
Instead of backpropagating through all $T$ steps, backprop only through the last $K$ steps, detaching the graph periodically.
Why: reduces memory/time and stabilizes training.
Tradeoff: harder to learn dependencies longer than $K$.

\subsection{One-hot encoding and limitations}
Represent a token $w$ from a vocabulary of size $V$ as $\vect{e}_w\in\{0,1\}^V$ with exactly one 1.
Limitations: high-dimensional and sparse; no notion of similarity between words; expensive for large $V$.

\subsection{\texttt{nn.Embedding}}
An embedding layer is a learnable lookup table $\matr{E}\in\R^{V\times d}$.
Given token id $w$, it outputs the dense vector $\vect{x}=\matr{E}_{w,:}\in\R^d$.
Preferred over one-hot: efficient and learns semantic similarity (nearby vectors for related words).

\subsection{Gating mechanisms (LSTM/GRU)}
Gates are elementwise controllers in $[0,1]$ (computed via $\sigm$) that decide how much information to keep, write, and expose.
They allow context-dependent time scales (keep some components for long, overwrite others quickly).

\subsection{How LSTM gates mitigate vanishing gradients}
Core cell update:
\[
\vect{c}_t = \vect{f}_t\odot\vect{c}_{t-1} + \vect{i}_t\odot\tilde{\vect{c}}_t
\quad\Rightarrow\quad
\frac{\partial \vect{c}_t}{\partial \vect{c}_{t-1}}=\operatorname{diag}(\vect{f}_t)
\]
If $\vect{f}_t\approx\vect{1}$ for relevant components, gradients can propagate through many timesteps; the model learns when to remember/forget.



\clearpage
\section{TSP with Transformers (encoder-decoder heuristic)}

\subsection{Problem setup (Euclidean TSP)}
\begin{itemize}
  \item Cities are points $p_i=(x_i,y_i)\in\R^2$.
  \item Goal: shortest Hamiltonian cycle (visit each city once, return to start).
  \item Edge costs: Euclidean distance $w_{uv}=\|p_u-p_v\|_2$.
  \item TSP is NP-hard $\Rightarrow$ use heuristics / learned approximations.
\end{itemize}

\subsection{Dataset structure (NetworkX graphs)}
Each sample is a tuple $(G,\ \texttt{opt\_tour})$ where $G$ is a complete weighted graph and \texttt{opt\_tour} is the optimal cycle as a node list (starting/ending at 0).

\begin{itemize}
  \item Node attribute \texttt{pos}: $\texttt{pos}[i]=(x_i,y_i)$ = city coordinates $p_i\in\R^2$.
  \item Edge attribute \texttt{weight}: $\texttt{weight}(u,v)=\|p_u-p_v\|_2$.
  \item Boolean edge attribute \texttt{tour}: \texttt{True} iff edge $(u,v)$ lies on the provided optimal tour.
\end{itemize}

\subsection{Data pipeline (PyTorch view)}
\begin{itemize}
  \item Model input:
  \[
    \matr{X}\in\R^{n\times 2},\qquad
    \matr{X}_{i,:}=(x_i,y_i)
  \]
  \item Target tour (closed cycle):
  \[
    \vect{y}\in\{0,\ldots,n-1\}^{n+1},\qquad y_0=y_n=0
  \]
  \item Batch shapes: $\matr{X}\in\R^{B\times n\times 2}$, $\vect{y}\in\Z^{B\times(n+1)}$.
\end{itemize}

\subsection{Sinusoidal positional encoding}
For $\mathrm{pos}\in\{0,\ldots,T-1\}$ and $i\in\{0,\ldots,\frac{d_{\text{model}}}{2}-1\}$:
\[
\mathrm{PE}_{\mathrm{pos},2i}
=
\sin \left(\frac{\mathrm{pos}}{10000^{2i/d_{\mathrm{model}}}}\right)
\qquad
\mathrm{PE}_{\mathrm{pos},2i+1}
=
\cos \left(\frac{\mathrm{pos}}{10000^{2i/d_{\mathrm{model}}}}\right)
\]
Purpose: attention alone is permutation-equivariant; positional information breaks symmetry so the model can represent a sequence/tour.

\subsection{Transformer heuristic for TSP (encoder--decoder)}
\begin{itemize}
  \item Encoder builds context vectors for all cities from $(x_i,y_i)$ (often with an extra node-id embedding).
  \item Decoder generates the tour autoregressively: given prefix $(y_0,\ldots,y_{t-1})$, predict $y_t$.
  \item Causal mask in decoder self-attention blocks access to future steps.
\end{itemize}

\subsection{Training objective (teacher forcing)}
Let $\vect{y}=[y_0,\ldots,y_n]$ be the optimal closed tour. Use
\[
\vect{y}^{\text{src}}=[y_0,\ldots,y_{n-1}],
\qquad
\vect{y}^{\text{tgt}}=[y_1,\ldots,y_n]
\]
Train with token-level cross-entropy (next-node prediction):
\[
\mathcal{L}
=
-\frac{1}{Bn}\sum_{b=1}^B\sum_{t=1}^{n}
\log p_\theta\!\bigl(y^{(b)}_t \mid y^{(b)}_{<t}, \matr{X}^{(b)}\bigr)
\]
Best checkpoint selected by validation loss.

\subsection{Inference and evaluation}
\begin{itemize}
  \item Greedy autoregressive decoding starts with prefix $[0]$.
  For $t=1,\ldots,n-1$ choose the highest-scoring unvisited node
  \[
    y_t=\argmax_{j\notin\{y_0,\ldots,y_{t-1}\}} \ \ell_\theta\bigl(j \mid y_{<t},\matr{X}\bigr)
  \]
  where $\ell_\theta(\cdot)$ denotes the decoder logits.
  Close the cycle by appending $0$.

  \item Tour length is $L(\text{tour})=\sum_{(u,v)\ \text{on tour}} w_{uv}$.

  \item Gap to optimum is $\mathrm{gap}=\bigl(L(\text{heuristic})-L(\text{opt})\bigr)/L(\text{opt})$.

  \item Baselines include a random tour and greedy nearest-neighbor (NetworkX \texttt{greedy\_tsp}).
\end{itemize}

\subsection{Practical extensions and core Transformer concepts}

\paragraph{Scaling to larger instances and improving decoding}
\begin{itemize}
  \item As written, the approach typically does not scale directly if $n=20$ is hard-coded in \texttt{nn.Embedding(n,\dots)} or in the output head dimension.
  \item To support $n=50$ and variable $n$:
  \begin{itemize}
    \item Train or finetune on $n=50$ or on variable $n$ with padding and masks.
    \item Replace a fixed-$n$ output head with a pointer-style attention scoring mechanism.
    The decoder scores each city by compatibility with its current state, which supports variable $n$.
    \item Improve decoding with beam search, sampling plus best-of-$k$, and post-optimization such as 2-opt or local search.
    \item Active search or test-time finetuning on each instance can further reduce tour length.
  \end{itemize}
\end{itemize}

\paragraph{Attention and multi-head attention}
Attention computes content-dependent weighted averages.
Each token or node aggregates information from others using similarity scores, enabling long-range interactions without recurrence.
Multi-head attention runs several attention heads in parallel using different learned projections and then concatenates the results.
This is useful because different heads can capture different relations such as local geometry, global structure, and symmetries.

\paragraph{Positional encodings}
Without positional information, attention is permutation-equivariant.
Positional encodings inject order so the decoder can represent the step $t$ in a tour.

\paragraph{Feed-forward network}
A position-wise MLP adds nonlinearity and feature mixing within each token embedding
\[
\mathrm{FFN}(\vect{h})=\matr{W}_2\,\phi\bigl(\matr{W}_1\vect{h}+\vect{b}_1\bigr)+\vect{b}_2
\]

\paragraph{LayerNorm}
For $\vect{h}=(h_1,\dots,h_d)\in\R^d$, LayerNorm normalizes across the feature dimension.
Define
\[
\mu=\frac{1}{d}\sum_{i=1}^d h_i
\qquad
\sigma^2=\frac{1}{d}\sum_{i=1}^d (h_i-\mu)^2
\]
then
\[
\hat h_i=\frac{h_i-\mu}{\sqrt{\sigma^2+\varepsilon}}
\qquad
\mathrm{LN}(\vect{h})=\vect{\gamma}\odot \hat{\vect{h}} + \vect{\beta}
\]
with $\vect{\gamma},\vect{\beta}\in\R^d$.
For a sequence tensor $\matr{H}\in\R^{B\times T\times d}$, LayerNorm is applied independently to each token $\matr{H}_{b,t,:}$.
It is independent of batch size and works well for variable-length sequences and autoregressive inference.
BatchNorm depends on batch statistics and is awkward or unstable in sequence models.

\paragraph{Queries, keys, values, scaling, and masking}
Given token embeddings $\matr{H}\in\R^{B\times T\times d}$ and per-head dimension $d_k$,
\[
\matr{Q}=\matr{H}\matr{W}_Q
\qquad
\matr{K}=\matr{H}\matr{W}_K
\qquad
\matr{V}=\matr{H}\matr{W}_V
\]
with $\matr{Q},\matr{K},\matr{V}\in\R^{B\times T\times d_k}$.
Queries ask what to attend to, keys describe content, and values are aggregated.
Dot products grow with $d_k$, so scaling by $\sqrt{d_k}$ avoids softmax saturation and tiny gradients.
Causal masking blocks access to future positions so that step $t$ depends only on $<t$.

\paragraph{Teacher forcing, tokenization, and training paradigms}
Teacher forcing feeds the ground-truth prefix $(y_0,\ldots,y_{t-1})$ during training to predict $y_t$.
At inference time the model conditions on its own predictions, so errors can accumulate.
Tokenization maps text to discrete symbols and affects sequence length, vocabulary size, and what can be represented efficiently.
Pretraining uses a large-scale generic objective, usually self-supervised, to learn broad representations.
Finetuning adapts the model to a specific task or domain using task data and objectives.

\paragraph{Decoding strategies}
Greedy decoding chooses $\argmax$ at each step and is fast but can be suboptimal.
Sampling draws from the distribution, optionally with temperature, top-$k$, or top-$p$, and can improve diversity.
Beam search keeps the top $k$ partial sequences, often improving quality at increased compute cost.

\end{document}