% author: Fabian Bosshard
% © CC BY 4.0

% compile with:
% pdflatex -> biber -> pdflatex -> pdflatex



\begin{filecontents}[overwrite]{\jobname.bib}
@book{clrs2022,
  author    = {Thomas H.~Cormen and Charles E.~Leiserson and Ronald L.~Rivest and Clifford Stein},
  title     = {Introduction to Algorithms},
  edition   = {Fourth},
  year      = {2022},
  publisher = {MIT Press},
  isbn      = {9780262046305},
  url       = {https://mitpress.mit.edu/9780262046305/introduction-to-algorithms/},
}
\end{filecontents}

\documentclass[twocolumn, a3paper, fontsize=9pt, headings=standardclasses, parskip=half]{scrartcl}
% \documentclass[a4paper, fontsize=9pt, headings=standardclasses, parskip=half]{scrartcl}

\usepackage{csquotes}



\usepackage[automark]{scrlayer-scrpage}
\clearpairofpagestyles
\ofoot{\pagemark} % für ein einseitiges Dokument: \ofoot platziert den Inhalt in der äußeren Fußzeile (unten rechts)
% \pagestyle{scrheadings}

% \renewcommand{\familydefault}{\sfdefault} % sans serif font for text (math font is still serif)

\usepackage{graphicx}
\usepackage[dvipsnames, table]{xcolor}

% \usepackage[left=43mm, right=43mm, top=20mm, bottom=30mm]{geometry} % for A4
\usepackage[left=25mm, right=25mm, top=25mm, bottom=35mm]{geometry} % for A3
% \usepackage{showframe}


% float management ––––––––––––––––––––––––––––––––––––––––––––––––
\usepackage{float}
\usepackage{placeins} 
\usepackage{etoolbox}

% \makeatletter
%   % single-column floats
%   \def\fps@figure {htb} 
%   \def\fps@table  {htb}

%   % double-column floats
%   \def\fps@figure*{htb} 
%   \def\fps@table* {htb}
% \makeatother

\preto\section{\FloatBarrier}
\preto\subsection{\FloatBarrier}
\preto\subsubsection{\FloatBarrier} 

\setcounter{topnumber}{8}
\setcounter{bottomnumber}{8}
\setcounter{totalnumber}{20}
\renewcommand{\textfraction}{0.0}
\renewcommand{\topfraction}{1.0}
\renewcommand{\floatpagefraction}{1.0}
\renewcommand{\dblfloatpagefraction}{1.0}

% On a float‐only page, kill the default “centered” glue
\makeatletter
  \setlength{\@fptop}{0pt} % no extra space above
  \setlength{\@fpsep}{\floatsep} % between floats: same as \floatsep
  \setlength{\@fpbot}{0pt plus 1fil} % infinite stretch below, so floats are pushed up
\makeatother
% ––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––


% \newlength\tindent
% \setlength{\tindent}{\parindent}
% \setlength{\parindent}{0pt}
% \renewcommand{\indent}{\hspace*{\tindent}}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{mathdots} % without this package, only \vdots and \ddots are taken from the text font (not the math font), which looks bad if the text font is different from the math font



\usepackage{enumitem}
\renewcommand{\labelitemi}{\textbullet}
\renewcommand{\labelitemii}{\raisebox{0.1ex}{\scalebox{0.8}{\textbullet}}}
\renewcommand{\labelitemiii}{\raisebox{0.2ex}{\scalebox{0.6}{\textbullet}}}
\renewcommand{\labelitemiv}{\raisebox{0.3ex}{\scalebox{0.4}{\textbullet}}}




\usepackage{pifont}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{ragged2e}
\usepackage{makecell}

\usepackage{tikz}
\usetikzlibrary{arrows, arrows.meta, shapes, positioning, calc, fit, patterns, intersections, math, 3d, tikzmark, decorations.pathreplacing, backgrounds, chains, svg.path}
\usepackage{forest}
\usepackage{tikz-3dplot}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}


% define colors
\definecolor{funblue}{rgb}{0.10, 0.35, 0.66}
\definecolor{alizarincrimsonred}{rgb}{0.85, 0.17, 0.11}
\definecolor{amethyst}{rgb}{0.6, 0.4, 0.8}
\definecolor{mypurple}{rgb}{128, 0, 128} 
\renewcommand{\emph}[1]{\textbf{#1}}

\usepackage{thmtools}
\declaretheoremstyle[headfont=\bfseries,bodyfont=\normalfont,spaceabove=6pt,spacebelow=6pt,qed=\ensuremath{\vartriangleleft},postheadspace=1em]{assertionstyle}
\declaretheorem[style=assertionstyle,name=Theorem]{theorem}
\declaretheorem[style=assertionstyle,name=Lemma,sibling=theorem]{lemma}
\declaretheorem[style=assertionstyle,name=Corollary,sibling=theorem]{corollary}
\declaretheorem[style=assertionstyle,name=Proposition,sibling=theorem]{proposition}
\declaretheorem[style=assertionstyle,name=Conjecture,sibling=theorem]{conjecture}
\declaretheorem[style=assertionstyle,name=Claim,sibling=theorem]{claim}
\declaretheoremstyle[headfont=\bfseries,bodyfont=\normalfont,spaceabove=6pt,spacebelow=6pt,qed=\ding{45},postheadspace=1em]{definitionstyle}
\declaretheorem[style=definitionstyle,name=Definition]{definition}
\declaretheorem[style=definitionstyle,name=Axiom,sibling=definition]{axiom}
\declaretheoremstyle[headfont=\bfseries\color{funblue},bodyfont=\normalfont\normalsize,spaceabove=6pt,spacebelow=6pt,qed=\ensuremath{\color{funblue}\blacktriangleleft},postheadspace=1em]{examplestyle}
\declaretheorem[style=examplestyle,name=Example]{example}
\declaretheoremstyle[headfont=\bfseries,bodyfont=\normalfont\normalsize,spaceabove=6pt,spacebelow=6pt,qed=\ensuremath{\blacktriangleleft},postheadspace=1em]{remarkstyle}
\declaretheorem[style=remarkstyle,name=Remark]{remark}
\declaretheoremstyle[headfont=\color{alizarincrimsonred}\bfseries,bodyfont=\normalfont\normalsize,spaceabove=6pt,spacebelow=6pt,qed=\ensuremath{\color{alizarincrimsonred}\blacktriangleleft},postheadspace=1em]{cautionstyle}
\declaretheorem[style=cautionstyle,name=Caution,sibling=remark]{caution}
\declaretheoremstyle[headfont=\bfseries,bodyfont=\normalfont\footnotesize,spaceabove=6pt,spacebelow=6pt,postheadspace=1em]{smallremarkstyle}
\declaretheorem[style=smallremarkstyle,name=Remark,sibling=remark]{smallremark}
\declaretheoremstyle[headfont=\bfseries\color{amethyst},bodyfont=\normalfont,spaceabove=6pt,spacebelow=6pt,qed=\ensuremath{\color{amethyst}\blacktriangleleft},postheadspace=1em]{digressionstyle}
\declaretheorem[style=digressionstyle,name=Digression]{digression}
\let\proof\relax
\let\endproof\relax
\declaretheoremstyle[
  headfont=\bfseries,
  bodyfont=\normalfont,
  spaceabove=6pt,
  spacebelow=6pt,
  qed=\ensuremath{\square},
  postheadspace=1em
]{proofstyle}
\declaretheorem[
  style=proofstyle,
  name=Proof,
  numbered=no
]{proof}
\newenvironment{verticalhack}
  {\begin{array}[b]{@{}c@{}}\displaystyle}
  {\\\noalign{\hrule height0pt}\end{array}} % to make qed symbol aligned

\usepackage{algorithm}
\usepackage[italicComments=false]{algpseudocodex}




% notation makros ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% commands for functions etc.
\newcommand{\im}{\operatorname{Im}}

% commands for vectors and matrices
\newcommand{\matr}[1]{\underline{\boldsymbol{#1}}}
\newcommand{\vect}[1]{\vec{\boldsymbol{#1}}}
% \newcommand{\vect}[1]{\vec{{#1}}\,}

% commands for derivatives
\newcommand{\dif}{\mathrm{d}}

% commands for number sets
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}

% commands for probability
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Exp}{\operatorname{E}}
% \newcommand{\P}{\operatorname{P}} % this is already defined in amsmath/amsopn
\newcommand{\Prob}{\operatorname{P}}
\newcommand{\numof}{\ensuremath{\# \,}} % number of elements in a set
\newcommand{\blackheight}{\operatorname{bh}}

% \algnewcommand{\LeftComment}[1]{\(\triangleright\) #1}
\algnewcommand{\TO}{, \ldots ,}
\algnewcommand{\DOWNTO}{, \ldots ,}
\algnewcommand{\OR}{\vee}
\algnewcommand{\AND}{\wedge}
\algnewcommand{\NOT}{\neg}
\algnewcommand{\LEN}{\operatorname{len}}
\algnewcommand{\tru}{\ensuremath{\mathrm{\texttt{true}}}}
\algnewcommand{\fals}{\ensuremath{\mathrm{\texttt{false}}}}
\algnewcommand{\append}{\circ}

\algnewcommand{\nil}{\ensuremath{\mathrm{\textsc{nil}}}}
\algnewcommand{\red}{\ensuremath{\mathrm{\textsc{red}}}}
\algnewcommand{\black}{\ensuremath{\mathrm{\textsc{black}}}}
\algnewcommand{\gray}{\ensuremath{\mathrm{\textsc{gray}}}}
\algnewcommand{\white}{\ensuremath{\mathrm{\textsc{white}}}}

\newcommand{\attribute}[1]{\ensuremath{\mathtt{#1}}}
\newcommand{\attrib}[2]{\ensuremath{#1\mathtt{.}\mathtt{#2}}} % object.field with field in math typewriter (like code)
\newcommand{\attribnormal}[2]{\ensuremath{#1\mathtt{.}#2}} 

% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


\usepackage{caption, subcaption}

\usepackage[backend=biber,style=numeric]{biblatex}
\addbibresource{\jobname.bib}


% \usepackage[colorlinks=true, linkcolor=black, urlcolor=black, citecolor=black]{hyperref}
\usepackage[
  pdfauthor={Fabian Bosshard},
  pdftitle={USI - Algorithms and Data Structures - Course Summary},
  pdfkeywords={USI, algorithms and data structures, course summary, informatics},
  colorlinks=false,        % don't wrap links in a colour
  pdfborder={0 0 0}        % no border around links
]{hyperref}
\usepackage[
  type     = {CC},
  modifier = {by},
  version  = {4.0},
]{doclicense}



\title{Algorithms \& Data Structures - Summary}
\author{Fabian Bosshard}
\date{\today}


\begin{document}
\pagenumbering{arabic}




\maketitle
\tableofcontents

\section*{Preface}
\addcontentsline{toc}{section}{Preface}

This document is an unofficial student-made summary of the course
\href{https://search.usi.ch/en/courses/35270741/algorithms-data-structures}{{Algorithms \& Data Structures}} taught by \href{https://www.inf.usi.ch/carzaniga/}{Antonio~Carzaniga} in Spring~2025 at the \href{https://www.usi.ch/en}{Università della Svizzera italiana}.
It is based on the \href{https://www.inf.usi.ch/carzaniga/edu/algo24s/index.html}{slides and other course materials}, as well as \cite{clrs2022}.
The summary is not exhaustive and may contain errors.
If you find any, please report them to \href{mailto:fabianlucasbosshard@gmail.com}{fabianlucasbosshard@gmail.com} or open an issue at \url{https://github.com/fabianbosshard/usi-informatics-course-summaries}.
The \LaTeX{} source is also available there.

\doclicenseThis

% ----- Keep the following commands unchanged -------------------
% \renewcommand{\emph}[1]{\textcolor{black}{#1}}
\printbibliography[heading=bibintoc,title={References}]
% \renewcommand{\emph}[1]{\textbf{#1}}



\section{Introduction}

$T(n) :=$ number of \textbf{basic steps} needed to compute the result of a problem of size $n$

\begin{example}
\label{ex:pingala}
How many $1,2$-beats can one compose over a total of $n$ beats?

Idea: the number of $1,2$-beats over $n$ beats is the sum of the number of $1,2$-beats over $n-1$ beats and the number of $1,2$-beats over $n-2$ beats
\def\TA{\raise0.5ex\hbox{\vrule width1pt depth0pt height1ex\vrule width2em depth0pt height0.5ex}}
\def\TAA{\raise1ex\hbox{\color{gray}\vrule width2pt depth0.5ex height0.5ex\vrule width4em depth0pt height0.5ex}}
\begin{center}
    \vspace{0em}
    \begin{tabular}[b]{l}
        \TA\hspace{1em}\TA\TA\TA\TA \\
        \TA\hspace{1em}\TA\TA\TAA \\
        \TA\hspace{1em}\TA\TAA\TA \\
        \TA\hspace{1em}\TAA\TA\TA \\
        \TA\hspace{1em}\TAA\TAA \\
    
        \TAA\hspace{1em}\TA\TA\TA \\
        \TAA\hspace{1em}\TA\TAA \\
        \TAA\hspace{1em}\TAA\TA \\
      \end{tabular}
\end{center}
\begin{algorithm}[htb] % wihtout [H], it is not possible to place the algorithm in the minipage
    \caption{Pingala}
    \begin{algorithmic}[1]
    \Function{Pingala}{$n$} \Comment{Number of $1,2$-beats in $n$ beats}
        \If{$n \leq 2$}
            \State \Return $n$
        \EndIf
        \State \Return \Call{Pingala}{$n-1$} + \Call{Pingala}{$n-2$}
    \EndFunction
    \end{algorithmic}
\end{algorithm}


% pseudocode:
% if n <= 2
%     return n
% return Pngala(n-1) + Pingala(n-2)
\vspace{-0.1em}
\[
T(1) = 1, \quad T(2) = 1, \quad T(n) = T(n-1) + T(n-2) + 2
\]
\[
 T(n) \geq \underbrace{T(n-1)}_{\geq T(n-2)} + T(n-2)  \geq 2T(n-2)
\]
\[
T(n) \geq 2T(n-2) \geq 2(2T(n-4)) = 2^2T(n-4) \geq \ldots \geq 2^{n/2}T(0) = \sqrt{2}^n T(0)  
\]
This naïve recursive implementation has exponential running time, $T(n) = \Omega(\sqrt{2}^n)$.
For this problem, we can do better if we avoid repeating the same computations over and over again (using Dynamic Programming, see \ref{sec:dynamic_programming}).
\end{example}


\section{Basics}
\begin{definition}
We define the following families of functions:
\begin{itemize}%[leftmargin=*]
    \item $O(g(n)) := \{ f(n) \mid \exists c > 0, n_0 \in \mathbb{N} \mid0 \leq f(n) \leq cg(n) \text{ for all } n \geq n_0 \}$
    \item $\Omega(g(n)) := \{ f(n) \mid \exists c > 0, n_0 \in \mathbb{N} \mid 0 \leq cg(n) \leq f(n) \text{ for all } n \geq n_0 \}$
    \item $\Theta(g(n)) := \{ f(n) \mid \exists c_1, c_2 > 0, n_0 \in \mathbb{N} \mid 0 \leq c_1g(n) \leq f(n) \leq c_2g(n) \; \forall n \geq n_0 \}$
    \item $o(g(n)) := \{ f(n) \mid \forall c > 0, \exists n_0 \in \mathbb{N} \mid 0 \leq f(n) < cg(n) \text{ for all } n \geq n_0 \}$
    \item $\omega(g(n)) := \{ f(n) \mid \forall c > 0, \exists n_0 \in \mathbb{N} \mid 0 \leq cg(n) < f(n) \text{ for all } n \geq n_0 \}$ \qedhere
\end{itemize}
\end{definition}



The notation `$f(n) =$' is used to denote that $f(n)$ is an element of the set of functions on the right-hand side.
\begin{example}
    Let $\pi(n)$ be the number of primes less than or equal to $n$. Then 
    \[
    \pi(n) = \Theta\left(\frac{n}{\log n}\right)
    \text{.} \qedhere
    \]
\end{example}

The $\Theta$-notation, $\Omega$-notation, and $O$-notation can be viewed as the ``asymptotic'' $=$, $\geq$, and $\leq$ relations for functions.
The $o$-notation and $\omega$-notation can be viewed as asymptotic $<$ and $>$.



\begin{theorem}
    \label{thm:O-and-Omega-imply-Theta}
    \leavevmode\setlength{\abovedisplayskip}{0pt}\vspace{-\baselineskip}
    \[
    f(n) = \Omega(g(n)) \wedge f(n) = O(g(n)) \Leftrightarrow f(n) = \Theta(g(n))
    \qedhere
    \]
\end{theorem}
The above theorem can be interpreted as saying
\[
f \geq g \wedge f \leq g \Leftrightarrow f = g
\] 

\begin{example}
    \label{ex:logarithm-factorial}
    $\log(n!) \in \Theta(n \log n)$. We can rewrite $\log(n!)$ as
    \begin{equation}
    \label{eq:logarithm-factorial}
    \log(n!) = \log\left(\prod_{i=1}^n i\right) = \sum_{i=1}^n \log i
    \end{equation}

    Clearly, $\log(n!) \in O(n \log n)$, since
    \(
     n \log n = \log(n^n) = \sum_{i=1}^n \log n \geq \sum_{i=1}^n \log i 
    \).

    One way to understand why $\log(n!) \in \Omega(n \log n)$ is to interpret \eqref{eq:logarithm-factorial} as a sum of areas, each rectangle has width $1$ and height $\log i$:
    \begin{center}

    \begin{tikzpicture}[xscale=1, yscale=1.6]
        %--------------------------------------------------
        % change this to any positive integer
        \def\n{8}
        \def\m{4}
        %--------------------------------------------------
      
        % helpful macro for the top of the y–axis
        \pgfmathsetmacro{\Ymax}{ln(\n)+0.5}
      
        % axes -------------------------------------------------
        \draw[->] (0,0) -- (\n+2,0) node[right] {$x$};
        \draw[->] (0,0) -- (0,\Ymax)  node[right] {};
      
      
        % rectangles  -----------------------------------------
        \foreach \i in {1,...,\n}{
            \draw[fill=gray!40] (\i-1,0) rectangle  (\i,{ln(\i)});
        }
      
        %  y = ln x  -------------------------------------------
        \draw[very thick, green!0!black, domain=1:\n+1.2, samples=140]
              plot (\x,{ln(\x)});
        \node[green!0!black, right] at (\n+1.2, {ln(\n+1.2)}) {$\log x$};
      
        % %  y = ln(x+1)  ----------------------------------------
        % \draw[very thick, green!0!black, domain=0:\n+2, samples=140]
        %       plot (\x,{ln(\x+1)});
        % \node[green!0!black, above=3pt] at (\n+2, {ln(\n+2)}) {$\log(x+1)$};
      
        %  horizontal red lines + red dots  --------------------
        \foreach \i in {1,...,\n}{
            % dotted helper line
            \draw[red, dotted, line width=0.6pt] (0,{ln(\i)}) -- (\i,{ln(\i)});
            \ifnum\i<\numexpr\m+1\relax
            % label on the left
            \node[left=3pt, red] at (0,{ln(\i)}) {$\log \i$};
            \fi
            % red point on the curve
            % \fill[red] (\i,{ln(\i)}) circle (1);
            \node [draw, circle, red, fill=red, inner sep=0pt, minimum size=4pt] at (\i,{ln(\i)}) {};
        }
        \node[left=3pt, red, yshift=1ex] at (0,{(ln(\n)+ln(\m))/2}) {$\vdots$};
        \node[left=3pt, red] at (0,{ln(\n)}) {$\log n$};

        % x–ticks 0,1,2, …, n  -------------------------------
        \foreach \x in {0,...,\m}{
            \node[below=3pt] at (\x,0) {$\x$};
        }
        % \draw (\n,0) -- ++(0,.12) node[below=4pt] {$n$};
        \node[below=3pt] at ({(\n+\m)/2},0) {$\ldots$};
        \node[below=3pt] at (\n,0) {$n$};

        % label for the area of the gray rectangles
        \draw[thin, black, {Circle[open,scale=.8]}-] (\m+1.6,{ln(\m+0.5)/2}) to[out=0,in=180] (\n+1.2,{ln(\m-0.5)}) node[right] {$\displaystyle \log(n!)$};

      \end{tikzpicture}
      \end{center}

    We can see that the area of the gray rectangles is bounded from below by the area under the curve $y = \log x$, i.e.
    \begin{equation}
    \label{eq:logarithm-factorial-lowerbound}
    \int_1^n \log x \, \dif x \leq \log(n!)
    \end{equation}
    Using integration by parts, we can express the left-hand side of \eqref{eq:logarithm-factorial-lowerbound} as
    \begin{align*}
        \int_1^n \log x \, \dif x &= \int_1^n \underset{\uparrow}{1} \cdot \underset{\downarrow}{\log x} \, \dif x = \left[ x \log x - \int x \cdot \frac{1}{x} \, \dif x \right]_1^n = \left[ x \log x - x \right]_1^n \\
        &= n \log n - n + 1 \in \Omega(n \log n)
    \end{align*}
    Therefore, $\log(n!) \in \Omega(n \log n) \wedge \log(n!) \in O(n \log n)$.
    Using Theorem \ref{thm:O-and-Omega-imply-Theta}, we conclude that $\log(n!) \in \Theta(n \log n)$.
\end{example}

\begin{table}[htb]
    % sources: https://en.wikipedia.org/wiki/Big_O_notation, https://en.wikipedia.org/wiki/Time_complexity
    \centering
    \begin{tabularx}{\columnwidth}{%
        |>{\RaggedRight\arraybackslash}p{0.28\columnwidth}%
        |>{\RaggedRight\arraybackslash}p{0.20\columnwidth}%
        |>{\small\RaggedRight\arraybackslash}X| }
      \hline
      \textbf{Notation} 
        & \textbf{Name} 
        & {\normalsize\textbf{Example}} \\
        % change to \small here
      \hline
      $O(1)$
        & constant
        & Finding median of \emph{sorted} array; computing $(-1)^n$; using a fixed-size lookup table. \\
      \hline
      $O(\alpha(n))$
        & inverse Ackermann
        & Amortized cost per operation in a disjoint-set data structure. \\
      \hline
        $O(\log^* n)$
            & iterated logarithmic
            & Distributed coloring of cycles (Cole-Vishkin algorithm). \\
        \hline
      $O(\log\log n)$
        & double logarithmic
        & Interpolation search on uniformly distributed data. \\
      \hline
      $O(\log n)$
        & logarithmic
        & Binary search; operations in balanced trees or a binomial heap. \\
      \hline
      $O(\log^c n)$, $c>1$
        & polylogarithmic
        & Matrix-chain ordering on a PRAM. \\
      \hline
      $O(n^c)$, $0<c<1$
        & fractional power
        & Searching in a $k$-d tree. \\
      \hline
        $O(\frac{n}{\log n})$
            & & $\#\mathrm{Primes} \leq n$ (Example \ref{ex:pingala}). \\
        \hline
      $O(n)$
        & linear
        & Scanning an unsorted list; ripple-carry addition of two $n$-bit integers. \\
      \hline
      $O(n\,\log^* n)$
        &
        & Seidel's polygon-triangulation algorithm. \\
      \hline
      $O(n\log n)=O(\log n!)$
        & linearithmic
        & Fastest comparison sorts; Fast Fourier transform. \\
      \hline
      $O(n^2)$
        & quadratic
        & Schoolbook multiplication; bubble/selection/insertion sort; worst-case quicksort; direct convolution. \\
      \hline
        $O(n^3)$
            & cubic
            & Naive $n\times n$ matrix multiplication; partial correlation. \\
        \hline
      $O(n^c)$, $c>1$
        & polynomial
        & TAG parsing; bipartite matching; determinant via LU. \\
      \hline
        $L_{n}[\alpha, c]$
        & sub-exponential
        & Factoring via number-field sieve. \\
      \hline
      $O(c^n)$, $c>1$
        & exponential
        & Exact TSP by DP; brute-force logical equivalence checking. \\
      \hline
      $O(n!)$
        & factorial
        & Brute-force TSP; enumerating permutations or partitions; determinant by Laplace expansion. \\
        % switch back to \normalsize here
      \hline
    \end{tabularx}
  \end{table}

        
When $f(n) = O(g(n))$, we say that $g(n)$ is an \textbf{upper bound} for $f(n)$, and that $g(n)$ \textbf{dominates} $f(n)$.

When $f(n) = \Omega(g(n))$, we say that $g(n)$ is a \textbf{lower bound} for $f(n)$.

When $f(n) = \Theta(g(n))$, we say that $g(n)$ is a \textbf{tight bound} for $f(n)$.

We use the $o$-notation to denote an upper bound that is not asymptotically tight, and the $\omega$-notation to denote a lower bound that is not asymptotically tight.
The following two implications hold:
\[
f(n) = o(g(n)) \Rightarrow \lim_{n \to \infty} \frac{f(n)}{g(n)} = 0
\qquad \quad
f(n) = \omega(g(n)) \Rightarrow \lim_{n \to \infty} \frac{f(n)}{g(n)} = \infty
\]



\subsection{Incremental Algorithms}
\label{sec:incremental_algorithms}
\begin{example}[Hand of cards]
    \label{ex:insertion_sort}
    % Sorting a hand of cards using insertion sort:
    % \begin{center}
    % \includegraphics[width=0.3\columnwidth]{images/sorting_cards_woutBG.pdf}
    % \end{center}
    \hyperref[sec:insertion_sort]{Insertion sort} (Algorithm \ref{alg:insertion_sort}) uses an algorithm design technique called \textbf{incremental} method: for each element $A[i]$, it inserts it into its proper place in the subarray $A[1:i]$, having already sorted $A[1:i-1]$.
    This is reminiscent of how one might sort a hand of cards, where you pick up a card and insert it into the correct position in the already sorted hand.
    
    At the start of each iteration of the for loop, the subarray $A[1:i-1]$ consists of the elements originally in $A[1:i-1]$, but in sorted order.
    This is a \hyperref[sec:loop_invariant]{loop invariant} (Section \ref{sec:loop_invariant}).
\end{example}

\subsection{Loop Invariant}
\label{sec:loop_invariant}
When using a \textbf{loop invariant}, 3 things need to be shown:
\begin{enumerate}
\item \textbf{Initialization:} It is true prior to the first iteration of the loop.
\item \textbf{Maintenance:} If it is true before an iteration of the loop, it remains true before the next iteration.
\item \textbf{Termination:} When the loop terminates, the invariant gives a useful property that helps show that the algorithm is correct.
\end{enumerate}


A loop-invariant proof is a form of \textbf{mathematical induction}, where to prove that a property holds, you prove a base case and an inductive step.
Here, showing that the invariant holds before the first iteration corresponds to the base case, and showing that the invariant holds from iteration to iteration corresponds to the inductive step.
The third property is perhaps the most important one, since you are using the loop invariant to show correctness. 
Typically, you use the loop invariant along with the condition that caused the loop to terminate. 
Mathematical induction typically applies the inductive step infinitely, but in a loop invariant the ``induction'' stops when the loop terminates.



\subsection{Correctness}

You are given a problem $P$ and an algorithm $A$. 
$P$ formally defines a \textbf{correctness} condition.
Assume, for simplicity, that $A$ consists of one loop.
\begin{enumerate}
    \item Formulate an invariant $C$
    \item \textbf{Initialization}: prove that $C$ holds right before the first execution of the first instruction of the loop
    \item \textbf{Management}: prove that if $C$ holds right before the first instruction of the loop, then it holds also at the end of the loop
    \item \textbf{Termination}: prove that the loop terminates, with some exit condition $X$
    \item Prove that $X\wedge C \Rightarrow P$, which means that $A$ is correct
\end{enumerate}


\subsection{Divide-and-Conquer Algorithms}
\label{sec:divide_and_conquer}

Many useful algorithms are \textbf{recursive}:
they \textbf{recurse} (call themselves) one or more times to handle closely related subproblems.
These algorithms typically follow the \textbf{divide-and-conquer} method:
they break the problem into several subproblems that are similar to the original problem but smaller in size,
solve the subproblems recursively, and then combine these solutions to create a solution to the original problem.

In the divide-and-conquer method, if the problem is small enough (the \textbf{base case}), you just solve it directly without recursing.
Otherwise (the \textbf{recursive case}), you perform three characteristic steps:
\begin{enumerate}
    \item \textbf{Divide} the problem into one or more subproblems that are smaller instances of the same problem.
    \item \textbf{Conquer} the subproblems by solving them recursively.
    \item \textbf{Combine} the subproblem solutions to form a solution to the original problem.
\end{enumerate}


% \begin{center}
%     \begin{tikzpicture}[
%         every node/.style={draw, rounded corners=2pt, align=center, minimum width=2.4cm, minimum height=1.0cm},
%         arrow/.style={->, thick}
%     ]
    
%     %--- NODES ---
%     % Top: Main Problem
%     \node (main) {\footnotesize Main Problem};
    
%     % Bottom: Final solution
%     \node (final) [below =3.2cm of main] {\footnotesize Solution of\\\footnotesize Main Problem};
    
%     \def\distance{3.4cm}
%     % Compute relative positions for subproblems dynamically
%     \node (sub1) at ($(main)!0.3!(final) + (-\distance,0)$) {\footnotesize Sub-Problem};
%     \node (sub2) at ($(main)!0.3!(final) + (\distance,0)$) {\footnotesize Sub-Problem};

%     % Compute relative positions for solutions of subproblems dynamically
%     \node (sol1) at ($(main)!0.7!(final) + (-\distance,0)$) {\footnotesize Solution of\\\footnotesize Sub-Problem};
%     \node (sol2) at ($(main)!0.7!(final) + (\distance,0)$) {\footnotesize Solution of\\\footnotesize Sub-Problem};
    
%     %--- ARROWS ---
%     % From main problem to sub-problems
%     \draw[arrow] (main) -- (sub1);
%     \draw[arrow] (main) -- (sub2);
    
%     % From sub-problems to their solutions
%     \draw[arrow] (sub1) -- node[midway, left, xshift=0.34cm, draw=none]
%        {\tiny Solve\\[-5pt] \tiny Sub-Problem} (sol1);
%     \draw[arrow] (sub2) -- node[midway, right, xshift=-0.34cm, draw=none]
%        {\tiny Solve\\[-5pt] \tiny Sub-Problem} (sol2);
    
%     % From solutions to final solution
%     \draw[arrow] (sol1) -- (final);
%     \draw[arrow] (sol2) -- (final);
    
%     %--- BOLD TEXT IN THE MIDDLE ---
%     % We place three labels along the vertical between 'main' and 'final'.
%     \node[font=\bfseries, draw=none] at ($(main)!0.20!(final)$) {Divide};
%     \node[font=\bfseries, draw=none] at ($(main)!0.50!(final)$) {Conquer};
%     \node[font=\bfseries, draw=none] at ($(main)!0.80!(final)$) {Combine};
    
%     \end{tikzpicture}
% \end{center}


When an algorithm contains a recursive call, we can often describe its running time with a \textbf{recurrence relation}, which expresses the overall running time of a problem of size $n$ in terms of the running time of the same algorithm on smaller inputs.


% \begin{digression}[Master Theorem]
% \label{digression:master_theorem}
Let $T(n)$ be the worst case running time on an input of size $n$.
If the problem is small enough, say $n \leq n_0$, for some constant $n_0$, the straightforward solution takes constant time $\Theta(1)$.
Suppose that the division of the problem yields $a$ subproblems, each of size $n/b$.%
\footnote{For \hyperref[sec:merge_sort]{merge sort} (Algorithm \ref{alg:merge_sort}), $a = b = 2$, but there are other divide-and-conquer algorithms in which $a \neq b$.}
If it takes $D(n)$ time to divide the problem into subproblems and $C(n)$ time to combine the solutions to the subproblems into the solution to the original problem, we get the recurrence
\begin{align}
\label{eq:recurrence_relation}
T(n) &=
\begin{cases}
    \Theta(1) & \text{if } n \leq n_0 \\
    D(n) + aT(n/b) + C(n) & \text{if } n > n_0
\end{cases}
\end{align}

\begin{theorem}[Master Theorem]
\label{thm:master_theorem}
Let $a>0$ and $b>1$ be constants, and let $f(n)$ be a driving function that is defined and nonnegative on all sufficiently large reals. Define the recurrence $T(n)$ on $n \in \mathbb{N}$ by
\[
T(n)=a T(n / b)+f(n)
\]
where $a T(n / b)$ actually means $a^{\prime} T(\lfloor n / b\rfloor)+a^{\prime \prime} T(\lceil n / b\rceil)$ for some constants $a^{\prime} \geq 0$ and $a^{\prime \prime} \geq 0$ satisfying $a=a^{\prime}+a^{\prime \prime}$. Then the asymptotic behavior of $T(n)$ can be characterized as follows:
\begin{enumerate}
    \item If there exists a constant $\epsilon>0$ such that $f(n)=O\left(n^{\log _b a-\epsilon}\right)$, then $T(n)=$ $\Theta\left(n^{\log _b a}\right)$.
    \item If there exists a constant $k \geq 0$ such that $f(n)=\Theta\left(n^{\log _b a} \lg ^k n\right)$, then $T(n)=$ $\Theta\left(n^{\log _b a} \lg ^{k+1} n\right)$.
    \item If there exists a constant $\epsilon>0$ such that $f(n)=\Omega\left(n^{\log _b a+\epsilon}\right)$, and if $f(n)$ additionally satisfies the regularity condition $a f(n / b) \leq c f(n)$ for some constant $c<1$ and all sufficiently large $n$, then $T(n)=\Theta(f(n))$. \qedhere
\end{enumerate}
\end{theorem}
% \end{digression}

\subsection{Binary Search}
\label{subsec:binary_search}

% \begin{figure}[htb]
%   % image from: https://cpc-utec.github.io/blog/web/class-11.html
%   \centering
%   \begin{subfigure}[b]{0.49\columnwidth}
%     \includegraphics[width=\linewidth, height=0.5\linewidth]{images/linear_s.png}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[b]{0.49\columnwidth}
%     \includegraphics[width=\linewidth, height=0.5\linewidth]{images/binary_s.png}
%   \end{subfigure}
% \end{figure}


Algorithm~\ref{alg:binary_search} is an efficient method for finding an element \(x\) in a sorted array \(A\). 
By repeatedly halving the search interval, it reduces the problem size exponentially: at each step, it compares \(x\) to the middle element of the current interval and discards the half in which \(x\) cannot lie. 
This yields a worst-case running time of \(O(\log n)\), a dramatic improvement over a linear search's \(O(n)\) behavior.


\begin{algorithm}[htb]
    \caption{Binary Search}
    \label{alg:binary_search}
    \begin{algorithmic}[1]
        \Function{BinarySearch}{$A, x$}
            \State $l \gets 1$ \Comment{leftmost index}
            \State $r \gets \LEN(A)$ \Comment{rightmost index}
            \While{$l \leq r$}
                \State $m \gets \lfloor (l + r) / 2 \rfloor$ \Comment{midpoint of $A[l:r]$}
                \If{$A[m] < x$}
                    \State $l \gets m + 1$ \Comment{search in right half}
                \ElsIf{$A[m] > x$}
                    \State $r \gets m - 1$ \Comment{search in left half}
                \Else
                    \State \Return $m$ \Comment{found $x$ at index $m$}
                \EndIf
            \EndWhile
            \State \Return $0$ \Comment{not found: $0$ is not a valid index}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

%\clearpage
\section{Sorting}

\subsection{Insertion Sort}
\label{sec:insertion_sort}
Was already used to sort cards in Section \ref{sec:incremental_algorithms}, Example \ref{ex:insertion_sort}.
\begin{algorithm}[htb]
    \caption{Insertion Sort}
    \label{alg:insertion_sort}
    \begin{algorithmic}[1]
    \Function{InsertionSort}{$A$}
        \For{$i = 2 \TO \LEN(A)$}
            \State $j \gets i$
            \While{$j > 1 \AND A[j-1] > A[j]$}
                \State swap $A[j]$ and $A[j-1]$
                \State $j \gets j - 1$
            \EndWhile
        \EndFor
    \EndFunction
\end{algorithmic}
\end{algorithm}


\subsection{Merge Sort}
\label{sec:merge_sort}


\begin{algorithm}[htb]
    %\scriptsize
    \caption{Merge Sort}
    \label{alg:merge_sort}
    \begin{algorithmic}[1]
\Function{MergeSort}{$A$}
    \If{$\operatorname{len}(A) \leq 1$} \Comment{base case (array is trivially sorted)}
        \State \Return $A$
    \EndIf
    \State $m = \lfloor \operatorname{len}(A) / 2 \rfloor$ \Comment{midpoint of $A$} \label{alg:merge_sort:midpoint}
    \State $A_L \gets$ \Call{MergeSort}{$A[1:m]$} \Comment{recursively sort $A[1:m]$} \label{alg:merge_sort:recurse_left}
    \State $A_R \gets$ \Call{MergeSort}{$A[m\!+\!1:|A|]$} \Comment{recursively sort $A[m\!+\!1:|A|]$} \label{alg:merge_sort:recurse_right}
    \State \Return \Call{Merge}{$A_L, A_R$} \Comment{merge the two sorted arrays}
\EndFunction
    \end{algorithmic}
\end{algorithm}


\begin{algorithm}[htb]
    %\scriptsize
    \caption{Merge}
    \label{alg:merge}
    \begin{algorithmic}[1]
\Function{Merge}{$A, B$}
    \State $i,j \gets 1$
    \State $C \gets []$
    \While{$i \leq \LEN(A) \OR j \leq \LEN(B)$}
        \If{$i \leq \LEN(A) \AND (j > \LEN(B) \OR A[i] < B[j])$}
            \State append $A[i]$ to $C$
            \State $i \gets i + 1$
        \Else
            \State append $B[j]$ to $C$
            \State $j \gets j + 1$
        \EndIf
    \EndWhile
    \State \Return $C$
\EndFunction
\end{algorithmic}
\end{algorithm}


We describe the running time of merge sort (Algorithm \ref{alg:merge_sort}) as follows:
\begin{enumerate}
    \item \textbf{Divide:} compute the middle of the array (Algorithm \ref{alg:merge_sort}, Line \ref{alg:merge_sort:midpoint}), which takes constant time, $D(n) = \Theta(1)$
    \item \textbf{Conquer:} recursively solve two subproblems (Algorithm \ref{alg:merge_sort}, Line \ref{alg:merge_sort:recurse_left}, \ref{alg:merge_sort:recurse_right}), each of size $n/2$, contributes $2T(n/2)$ %to the total running time (ignoring the floors and ceilings)
    \item \textbf{Combine:} merge (Algorithm \ref{alg:merge}) the two sorted subarrays, which takes $\Theta(n)$ time, $C(n) = \Theta(n)$
\end{enumerate}

Using the so called `master theorem':
\[
T(n) = 2T(n/2) + \Theta(n) \xRightarrow{\text{\hyperref[thm:master_theorem]{master theorem}}} T(n) = \Theta(n \log_2 n)
\]


Intuitively we can also understand why that is the case without the master theorem. Assume for simplicity that $n$ is an exact power of 2 and that the implicit base case is $n = 1$:
\begin{align*}
    T(n) &= 
    \begin{cases}
        c_1 & \text{if } n = 1 \\
        2T(n/2) + c_2n & \text{if } n > 1
    \end{cases}
\end{align*}
where $c_1 > 0$ represents the time to solve the base case ($n = 1$) and $c_2 > 0$ is the time per element of the divide and combine steps.
\begin{center}
\begin{tikzpicture}[
        %>=stealth,
        level distance=1.2cm,
        sibling distance=2.5cm,
        every node/.style={draw=none},
        font=\small,
        scale=0.85,
    ]
    
    %--- Top (root) node ---
    \node (top) at (0,0) {\(c_2 \cdot n\)};
    
    %--- Second level (2 children) ---
    \node (l1) at (-2,-1) {\(c_2 \cdot n/2\)};
    \node (r1) at ( 2,-1) {\(c_2 \cdot n/2\)};
    \draw (top) -- (l1);
    \draw (top) -- (r1);
    
    %--- Third level (4 children) ---
    \node (l2) at (-3,-2) {\(c_2 \cdot n/4\)};
    \node (l3) at (-1,-2) {\(c_2 \cdot n/4\)};
    \node (r2) at ( 1,-2) {\(c_2 \cdot n/4\)};
    \node (r3) at ( 3,-2) {\(c_2 \cdot n/4\)};
    \draw (l1) -- (l2);
    \draw (l1) -- (l3);
    \draw (r1) -- (r2);
    \draw (r1) -- (r3);
    
    %--- Dotted lines indicating more subdivision below ---
    \draw (l2) -- ($(l2)!0.7!($(l2)+(-0.5,-0.75)$)$); \draw[dotted] ($(l2)!0.7!($(l2)+(-0.5,-0.75)$)$) -- ($(l2)+(-0.5,-0.75)$);
    \draw (l2) -- ($(l2)!0.7!($(l2)+(0.5,-0.75)$)$); \draw[dotted] ($(l2)!0.7!($(l2)+(0.5,-0.75)$)$) -- ($(l2)+(0.5,-0.75)$);
    \draw (l3) -- ($(l3)!0.7!($(l3)+(-0.5,-0.75)$)$); \draw[dotted] ($(l3)!0.7!($(l3)+(-0.5,-0.75)$)$) -- ($(l3)+(-0.5,-0.75)$);
    \draw (l3) -- ($(l3)!0.7!($(l3)+(0.5,-0.75)$)$); \draw[dotted] ($(l3)!0.7!($(l3)+(0.5,-0.75)$)$) -- ($(l3)+(0.5,-0.75)$);
    \draw (r2) -- ($(r2)!0.7!($(r2)+(-0.5,-0.75)$)$); \draw[dotted] ($(r2)!0.7!($(r2)+(-0.5,-0.75)$)$) -- ($(r2)+(-0.5,-0.75)$);
    \draw (r2) -- ($(r2)!0.7!($(r2)+(0.5,-0.75)$)$); \draw[dotted] ($(r2)!0.7!($(r2)+(0.5,-0.75)$)$) -- ($(r2)+(0.5,-0.75)$);
    \draw (r3) -- ($(r3)!0.7!($(r3)+(-0.5,-0.75)$)$); \draw[dotted] ($(r3)!0.7!($(r3)+(-0.5,-0.75)$)$) -- ($(r3)+(-0.5,-0.75)$);
    \draw (r3) -- ($(r3)!0.7!($(r3)+(0.5,-0.75)$)$); \draw[dotted] ($(r3)!0.7!($(r3)+(0.5,-0.75)$)$) -- ($(r3)+(0.5,-0.75)$);
    
    
    %--- Bottom row (n leaves, each with cost c1) ---
    \node at (-3.75,-3.5) {\(c_1\)};
    \node at (-3.25,-3.5) {\(c_1\)};
    \node at (-2.75,-3.5) {\(c_1\)};
    \node at (-2.25,-3.5) {\(\dots\)};
    % ...
    \node at ( 2.25,-3.5) {\(\dots\)};
    \node at ( 2.75,-3.5) {\(c_1\)};
    \node at ( 3.25,-3.5) {\(c_1\)};
    \node at ( 3.75,-3.5) {\(c_1\)};
    
    %--- Brace indicating n leaves ---
    \draw[decorate, decoration={brace, amplitude=5pt, mirror}, thick]
      (-4,-4) -- (4,-4)
      node[midway, below=6pt] {\(n\)};
    
    %--- Vertical arrow for log n + 1---
    \draw[<->, thick] (-4.5,0) -- (-4.5,-3.5)
      node[midway, left] {\(\log_2 n + 1\)};
    
    %--- Dotted arrows on the right showing cost per level = c2 * n ---
    \draw[dotted, ->, thick] (1, 0) -- (6.5, 0)   node[right] {\(c_2 \cdot n\)};
    \draw[dotted, ->, thick] (3,-1) -- (6.5,-1)  node[right] {\(c_2 \cdot n\)};
    \draw[dotted, ->, thick] (4,-2) -- (6.5,-2)  node[right] {\(c_2 \cdot n\)};
    \node[right] at (6.5, -2.75) {$\vdots$};
    \draw[dotted, ->, thick] (4.55,-3.5) -- (6.5,-3.5) node[right] {\(c_1 \cdot n\)};
    
    \end{tikzpicture}
\end{center}
\begin{align*}
    T(n) &= 2T(n/2) + c_2n \quad = 2(2T((n/2)/2) + c_2(n/2)) + c_2n \\
    &= 2^2T(n/2^2) + 2c_2n \quad = 2^2(2T((n/2^2)/2) + c_2(n/2^2)) + 2c_2n \\
    &= 2^3T(n/2^3) + 3c_2n \quad = 2^3(2T((n/2^3)/2) + c_2(n/2^3)) + 3c_2n \\
    & \vdots \\
    &= 2^{\log_2(n)}T(n/2^{\log_2(n)}) + \log_2(n)c_2n \\
    &= n T(1) + \log_2(n)c_2n \\
    &= c_1 n + c_2 n \log_2n \\
    &= \Theta(n \log n)
\end{align*}





\subsection{Quick Sort (with Lomuto Partitioning)}

\definecolor{mygruen}{rgb}{0.95,0.95,0.95}
\definecolor{myblue}{rgb}{0.2,0.8,1.0}         % previously dunkelgruen
\definecolor{myorange}{rgb}{1,0.65,0}      % previously hellgruen

\newcommand{\mygruen}{\color{mygruen}}
\newcommand{\myblue}{\color{myblue}}
\newcommand{\myorange}{\color{myorange}}

\[
    \begin{tikzpicture}[scale = 1.2]
      \draw[fill=myorange!30] (0,0) rectangle (2, 0.5);
      \draw[fill=myblue!30] (2,0) rectangle (5, 0.5);
      \draw[fill=myorange!30] (7.5,0) rectangle (8, 0.5);
      \fill[fill=myorange!30] (5,0) rectangle (7.5, 0.25);
      \fill[fill=myblue!30] (5,0.25) rectangle (7.5, 0.5);
      \draw (7.5, 0) -- (7.5, 0.5);
      \draw (2, 0) -- (2, 0.5);
      \draw (5, 0) -- (5, 0.5);
      \node at (7.75, 0.25) {$v$};
      \node at (1, 0.25) {$\leq v$};
      \node at (3.5, 0.25) {$> v$};
      \node at (6.25, 0.25) {? $\cdots$ ?};
      % \node at (1.75, -0.25) {\small$\uparrow$};
      % \node at (1.75, -0.65) {${i}$};
      % \node at (5.25, -0.25) {\small$\uparrow$};
      % \node at (5.25, -0.65) {${j}$};
      % \node at (0.25, -0.25) {\small$\uparrow$};
      % \node at (0.25, -0.65) {${p}$};
      % \node at (7.75, -0.25) {\small$\uparrow$};
      % \node at (7.75, -0.65) {${r}$};
      \node at (1.75, 0.75) {${i}$};
      \node at (5.25, 0.75) {${j}$};
      \node at (0.25, 0.75) {${l}$};
      \node at (7.75, 0.75) {${r}$};
      
      \draw[thick] (0,0) rectangle (8, 0.5);
        %\draw [step = .25, blue, opacity=0.3] (0, -.5) grid (\columnwidth, 0.5);
    \end{tikzpicture}
    % \vspace*{-2.5em}
    % \caption{Lomuto invariant: $A[1..\texttt{i} - 1]$ consists of elements smaller than $p$, 
    % $A[\texttt{i}..\texttt{j}-1]$ consists of elements at least as large as $p$; $A[\texttt{j}..\textit{n}-1]$ has not been looked at, which is depicted by filling this part of the array with both colors.}
    \]
Lomuto partitioning (Algorithm \ref{alg:lomuto_partition}) divides an array $A$ into two subarrays $A[l:q-1]$ and $A[q+1:r]$ such that all elements in $A[l:q-1]$ are less than or equal to $A[q]$ and all elements in $A[q+1:r]$ are greater than $A[q]$.

\begin{algorithm}[h!]
    \caption{Quick Sort}
    \label{alg:quick_sort}
\begin{algorithmic}[1]
    \Function{QuickSort}{$A, l, r$}
        \If{$l < r$}
            \State $q = \Call{Partition}{A, l, r}$
            \State \Call{QuickSort}{$A, l, q - 1$}
            \State \Call{QuickSort}{$A, q + 1, r$}
        \EndIf
    \EndFunction
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[h!]
    \caption{Lomuto Partitioning}
    \label{alg:lomuto_partition}
\begin{algorithmic}[1]
    \Function{Partition}{$A, l, r$}
        \State $v = A[r]$ \Comment{pick last element as pivot}
        \State $i = l - 1$ \Comment{highest index into the less-than-or-equal--partition}
        \For{$j = l \TO r$}
            \If{$A[j] \leq v$}
                \State $i = i + 1$
                \State swap $A[i]$ and $A[j]$
            \EndIf
        \EndFor
        \State \Return $i$ \Comment{index of pivot}
    \EndFunction
\end{algorithmic}
\end{algorithm}
    




\subsection{Heap Sort}
\label{subsec:heap_sort}
\begin{definition}[Heap]
A \emph{binary heap} is a nearly-complete binary tree stored in an array \(A[1:n]\) satisfying the \emph{max-heap property}:
\[
\forall i>1:\quad A[\mathrm{Parent}(i)] \ge A[i]
\]
The relationship between the indices of a binary heap is as follows:
\[
\mathrm{Parent}(i)=\lfloor i/2\rfloor \qquad
\mathrm{Left}(i)=2i \qquad
\mathrm{Right}(i)=2i+1 \qedhere
\]
\end{definition}
Furthermore, an $n$-element heap has height $h=\lfloor\log_2 n\rfloor$ and at most $\lceil n/2^{\tilde{h}+1}\rceil$ nodes at any given height $\tilde{h}$, where $\tilde{h}$ is defined as the longest path from the current node to a leaf node, measured in number of edges.


\definecolor{arraycolor}{rgb}{0.70,0.99,0.70}
\newcommand{\arraycolor}{\color{arraycolor}}


\begin{center}
\begin{tikzpicture}[
  arr/.style={->},
  level distance=1.2cm,
  sibling distance=2.5cm,
  % -------- main tree nodes ---------
  every node/.style={
    draw,circle,
    minimum size = 1.3em,
    inner sep    = 0pt
  },
  % -------- label nodes -------------
  every label/.style={
    draw=none,          % no outline
    shape=rectangle,    % *not* a circle
    inner sep=0pt       % tight around the text
  },
  font=\footnotesize,
  scale=0.9,
]
%--- Top (root) node ---
\node[fill=arraycolor] (top) at (0,1) [label=below:{$1$}] {};
\node[fill=arraycolor] (l1)  at (-2,-1) [label=below:{$2$}] {};
\node[fill=arraycolor] (r1)  at ( 2,-1) [label=below:{$3$}] {};
% arrows between levels
\draw[arr] (top) -- (l1);
\draw[arr] (top) -- (r1);

%--- Third level (4 children) ---
\node[fill=arraycolor] (l2) at (-3,-2) [label=below:{$4$}] {};
\node[fill=arraycolor] (l3) at (-1,-2) [label=below:{$5$}] {};
\node[fill=arraycolor] (r2) at ( 1,-2) [label=below:{$6$}] {};
\node[fill=arraycolor] (r3) at ( 3,-2) [label=below:{$7$}] {};
\draw[arr] (l1) -- (l2);
\draw[arr] (l1) -- (l3);
\draw[arr] (r1) -- (r2);
\draw[arr] (r1) -- (r3);

%--- dots in between ---
\node[draw=none] at (-3,-3) {$\dots$};
\node[draw=none] at (-1,-3) {$\dots$};
\node[draw=none] at (1,-3) {$\dots$};
\node[draw=none] at (3,-3) {$\dots$};

%--- Dotted arrows indicating more subdivision below ---
\newcommand{\splitratio}{0.6}
\newcommand{\subdx}{0.5}
\newcommand{\subdy}{0.5}
\foreach \nodecoord in {l2,l3,r2,r3} {
  \draw[] (\nodecoord) -- ($ (\nodecoord) !\splitratio! ($(\nodecoord)+(-\subdx,-\subdy)$) $); % left edge
  \draw[dotted] ($ (\nodecoord) !\splitratio! ($(\nodecoord)+(-\subdx,-\subdy)$) $ ) -- ($(\nodecoord)+(-\subdx,-\subdy)$); % continuation of left edge as dotted line
  \draw[] (\nodecoord) -- ($ (\nodecoord) !\splitratio! ($(\nodecoord)+(\subdx,-\subdy)$) $); % right edge
  \draw[dotted] ($ (\nodecoord) !\splitratio! ($(\nodecoord)+(\subdx,-\subdy)$) $ ) -- ($(\nodecoord)+(\subdx,-\subdy)$); % continuation of right edge as dotted line
}

%--- Bottom two rows ---
\node (2_power_hminus2) at (-4,-3) [draw=none] {};
% \node (2_power_hminus1) at (-4.75,-3.75) [label={[yshift=-0pt]below:{$2^{h \scalebox{0.5}[1]{$-$} 1}$}}] {}; % this label needs to be shifted down...
\node[fill=arraycolor] (2_power_hminus1) at (-4.75,-3.75) [label=below:{$2^{h\!-\!1}$}] {};
\node[fill=arraycolor] (2_power_h) at (-5.5,-4.5) [label=below:{$2^h$}] {};
\node[fill=arraycolor] (2powerh_plus_1) at (-4,-4.5) [label=below:{$2^h\!+\!1$}] {};

\draw[dotted] (2_power_hminus2) -- ($(2_power_hminus2)!0.5!(2_power_hminus1)$);
\draw[arr] ($(2_power_hminus2)!0.5!(2_power_hminus1)$) -- (2_power_hminus1);
\draw[arr] (2_power_hminus1) -- (2_power_h);
\draw[arr] (2_power_hminus1) -- (2powerh_plus_1);

\node[draw=none] at (-3.75,-3.75) {$\dots$};
\node[draw=none] at (-2.25,-3.75) {$\dots$};
\node[draw=none] at (-3,-4.5) {$\dots$};


\node[fill=arraycolor] (heapsize_minus1) at (-2, -4.5) [label=below:{$n\!-\!1$}] {};
\node[fill=arraycolor] (heapsize) at (-0.5, -4.5) [label=below:{$n$}] {};
\node[fill=arraycolor] (heapsize_parent) at (-1.25, - 3.75) [label={[yshift=-0pt]below:{$\left\lfloor \! \frac{n}{2} \! \right\rfloor$}}] {};
\node (heapsize_pparent) at (-2, -3) [draw=none] {};

\draw[dotted] (heapsize_pparent) -- ($(heapsize_pparent)!0.5!(heapsize_parent)$);
\draw[arr] ($(heapsize_pparent)!0.5!(heapsize_parent)$) -- (heapsize_parent);
\draw[arr] (heapsize_parent) -- (heapsize);
\draw[arr] (heapsize_parent) -- (heapsize_minus1);

\node[draw=none] at (-0.25,-3.75) {$\dots$};
\node[draw=none] at (0.75,-3.75) {$\dots$};
\node[draw=none] at (1.75,-3.75) {$\dots$};
\node[draw=none] at (2.75,-3.75) {$\dots$};
\node[draw=none] at (3.75,-3.75) {$\dots$};
\node[fill=arraycolor] (2powerh_minus_1) at (4.75,-3.75) [label=below:{$2^h\!-\!1$}] {};
\node (2powerh_minus_1_parent) at (4,-3) [draw=none] {};
\draw[dotted] (2powerh_minus_1_parent) -- ($(2powerh_minus_1_parent)!0.5!(2powerh_minus_1)$);
\draw[arr] ($(2powerh_minus_1_parent)!0.5!(2powerh_minus_1)$) -- (2powerh_minus_1);

% \node (heapsize_parent_plus_1) at (0.25, -3.75) [label=below:{$\left\lfloor \! \frac{n}{2} \! \right\rfloor \! + \! 1$}] {};
% \node (heapsize_parent_plus_1_parent) at (1, -3) [draw=none] {};
% \draw[dotted] (heapsize_parent_plus_1_parent) -- ($(heapsize_parent_plus_1_parent)!0.5!(heapsize_parent_plus_1)$);
% \draw[arr] ($(heapsize_parent_plus_1_parent)!0.5!(heapsize_parent_plus_1)$) -- (heapsize_parent_plus_1);

% %--- \tilde{h} on left side ---
% \newcommand{\linestart}{-5.5}
% \newcommand{\labelshift}{-3.5em}
% \draw[dashed, thin, draw=gray] (-0.5, 1) -- (\linestart, 1)node[left, draw=none, outer sep=5ex] {\(\tilde{h} = h\)};
% \draw[dashed, thin, draw=gray] (\linestart, -1) node[right, draw=none, outer sep=5ex, xshift=\labelshift] {\(\tilde{h} = h\!-\!1\)} -- (-2.5, -1);
% \draw[dashed, thin, draw=gray] (\linestart, -2) node[right, draw=none, outer sep=5ex, xshift=\labelshift] {\(\tilde{h} = h\!-\!2\)} -- (-3.5, -2);
% \draw[draw=none] (\linestart, -2.75) node[right, draw=none, outer sep=5ex, xshift=\labelshift] {\(\vdots\)} -- (-4.25, -2.75);
% \draw[draw=none] (\linestart, -3.75) node[right, draw=none, outer sep=5ex, xshift=\labelshift] {\(\tilde{h} = 1\)} -- (-5.25, -3.75);
% \draw[draw=none] (\linestart, -4.5) node[right, draw=none, outer sep=5ex, xshift=\labelshift] {\(\tilde{h} = 0\)} -- (-6, -4.5);

%--- \tilde{h} on left side via east-anchor + x-shift trick ---
\newcommand{\linestart}{-6.8}
\node[draw=none,anchor=west,inner sep=0] (lbl0) at (\linestart,1) {$h = \lfloor \log_2 n \rfloor$};
\draw[dashed,thin,gray] ($(lbl0.east)+(0.8em,0)$) -- (-0.5,1);
\node[draw=none,anchor=west,inner sep=0] (lbl1) at (\linestart,-1) {$\tilde h = h\!-\!1$};
\draw[dashed,thin,gray] ($(lbl1.east)+(0.8em,0)$) -- (-2.5,-1);
\node[draw=none,anchor=west,inner sep=0] (lbl2) at (\linestart,-2) {$\tilde h = h\!-\!2$};
\draw[dashed,thin,gray] ($(lbl2.east)+(0.8em,0)$) -- (-3.5,-2);
\node[draw=none,anchor=west,inner sep=0] (lbldots) at (\linestart,-2.75) {$\vdots$};
\node[draw=none,anchor=west,inner sep=0] (lbl3) at (\linestart,-3.75) {$\tilde h = 1$};
% \draw[dashed,thin,gray] ($(lbl3.east)+(0.8em,0)$) -- (-5.25,-3.75);
\node[draw=none,anchor=west,inner sep=0] (lbl4) at (\linestart,-4.5) {$\tilde h = 0$};
% \draw[dashed,thin,gray] ($(lbl4.east)+(0.8em,0)$) -- (-6,-4.5);


%--- Vertical arrow and vertical (90 degree rotated) label log n on right side ---
\draw[|-|] (6,1) -- (6,-4.5)
node[draw=none, midway, xshift=-0.8em](labelposition) {};
\node[draw=none, rotate=-90] at (labelposition) {$h=\lfloor \log n \rfloor$};

\end{tikzpicture}
\end{center}




\begin{center}

\begin{tikzpicture}[scale=1.2,every node/.style={font=\footnotesize}]
  % -----------------------------------------------------------
  % generic cell helper:  \heapcell{<label>}{<index-in-row>}{<node-name>}
  % -----------------------------------------------------------
  \def\cw{0.5} % cell width
  \newcommand{\heapcell}[3]{%
      \pgfmathsetmacro\x{#2*\cw}%
      \draw[fill=arraycolor] (\x,0) rectangle ++(\cw,0.5);               % rectangle
      \node at (\x+0.5*\cw,0.7) {\footnotesize$#1$};           % index label
      \coordinate (#3) at (\x+0.5*\cw,0);               % anchor for arrows
  }
  
  % array structure of heap
  \heapcell{1}{0}{p1}
  \heapcell{2}{1}{p2}
  \heapcell{3}{2}{p3}
  \heapcell{4}{3}{p4}
  \heapcell{5}{4}{p5}
  \heapcell{6}{5}{p6}
  \heapcell{7}{6}{p7}
  \node at (7.75*\cw,0.25) {$\dots$};
  \heapcell{2^{h\!-\!1}}{8.5}{phm}
  \node at (10.25*\cw,0.25) {$\dots$};
  \heapcell{\left\lfloor \! \frac{n}{2} \! \right\rfloor}{11}{pnpar}
  \node at (12.75*\cw,0.25) {$\dots$};
  \heapcell{2^h\!-\!1}{13.5}{p2h}            
  \heapcell{2^h}{14.5}{plh}             % left  child of 2^{h-1}
  \heapcell{2^h\!+\!1}{15.5}{prh}           % right child of 2^{h-1}
  \node at (17.25*\cw,0.25) {$\dots$};
  \heapcell{n\!-\!1}{18}{pnmone}          % left  child of floor(n/2)
  \heapcell{n}{19}{pn}                % right child of floor(n/2)

   % array rectangle bounding box (needs to be drawn last)
  \draw[thick] (0,0) rectangle ++(20*\cw,0.5);
  
  % -----------------------------------------------------------
  % arrows
  \foreach \p/\l/\r in {p1/p2/p3, p2/p4/p5, p3/p6/p7}{
      \draw[->] (\p)  .. controls +(0,-0.40) and +(0,-0.40) .. (\l);
      \draw[->] (\p)  .. controls +(0,-0.60) and +(0,-0.60) .. (\r);
  }
  % from 2^{h-1} to 2^{h} and 2^{h}+1
  \draw[->] (phm) .. controls +(0,-0.40) and +(0,-0.40) .. (plh);
  \draw[->] (phm) .. controls +(0,-0.60) and +(0,-0.60) .. (prh);
  % from floor(n/2) to n-1 and n
  \draw[->] (pnpar) .. controls +(0,-0.40) and +(0,-0.40) .. (pnmone);
  \draw[->] (pnpar) .. controls +(0,-0.60) and +(0,-0.60) .. (pn);
\end{tikzpicture}
  \end{center}


\begin{algorithm}[htb]
  \caption{Max-Heapify}
  \label{alg:heapify}
  \begin{algorithmic}[1]
    \Function{MaxHeapify}{$A,i$}
      \State \(l \gets \mathrm{Left}(i)\)
      \State \(r \gets \mathrm{Right}(i)\)
      \State \(m \gets i\) \Comment{index of largest element among \(\{A[i], A[l], A[r]\}\)}
      \If{\(l \le \attrib{A}{heap\mbox{-}size} \AND A[l] > A[m] \)}
        \State \(m \gets l\)
      \EndIf
      \If{\(r \le \attrib{A}{heap\mbox{-}size} \AND A[r] > A[m] \)}
        \State \(m \gets r\)
      \EndIf
      \If{\(m \ne i\)}
        \State swap \(A[i]\) and \(A[m]\)
        \State \Call{MaxHeapify}{$A, m$}
      \EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[htb]
  \caption{Build-Max-Heap}
  \label{alg:build_heap}
  \begin{algorithmic}[1]
    \Function{BuildMaxHeap}{$A$}
      \State \(\attrib{A}{heap\mbox{-}size} \gets |A|\)
      \For{\(i=\lfloor|A|/2\rfloor \TO 1\)} \Comment{elements after \(\lfloor|A|/2\rfloor\) are leaves}
        \State \Call{MaxHeapify}{$A,i$}
      \EndFor
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[htb]
  \caption{Heap Sort}
  \label{alg:heap_sort}
  \begin{algorithmic}[1]
    \Function{HeapSort}{$A$}
      \State \Call{BuildMaxHeap}{$A$} \label{alg:heap_sort:build}
      \For{\(i=|A| \DOWNTO 2\)} \label{alg:heap_sort:start_for}
        \State swap \(A[1]\) and \(A[i]\)
        \State \(\attrib{A}{heap\mbox{-}size}\gets \attrib{A}{heap\mbox{-}size}-1\)
        \State \Call{MaxHeapify}{$A,1$}
      \EndFor \label{alg:heap_sort:end_for}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:heap_sort} sorts an array \(A\) \textcolor{red}{in place} by first building a max-heap from the input array and then repeatedly extracting the maximum element (the root of the heap) and placing it at the end of the array.
The complexities of Algorithms \ref{alg:heapify}, \ref{alg:build_heap}, \ref{alg:heap_sort} are:
\[
T_{\textsc{\hyperref[alg:heapify]{MaxHeapify}}}(n)=\Theta(\log n)
\qquad
T_{\textsc{\hyperref[alg:build_heap]{BuildMaxHeap}}}(n)=\Theta(n)
\]
\[
T_{\textsc{\hyperref[alg:heap_sort]{HeapSort}}}(n)=\Theta(n\log n)
\]

The complexity of Algorithm~\ref{alg:heapify} is determined by the height $\tilde{h}$ of the node to be heapified, which is given by 
\begin{equation}
\label{eq:heapify-height}
\tilde{h} = h - \mathrm{Depth}(i) = \lfloor\log n\rfloor - \lfloor\log i\rfloor
\end{equation}
\nopagebreak
for a node at index \(i\).

\pagebreak[3]
Analyzing the complexity of Algorithm~\ref{alg:build_heap} is more involved.
A simple upper bound on the running time is $O(n\lg n)$, since each call to \textsc{\hyperref[alg:heapify]{MaxHeapify}} costs $O(\log n)$ and \textsc{\hyperref[alg:build_heap]{BuildMaxHeap}} makes $O(n)$ such calls.
This upper bound is correct but not asymptotically tight.

We can derive a tighter bound by recalling that the time for \textsc{\hyperref[alg:heapify]{MaxHeapify}} to run at a node $i$ depends on the height $\tilde h$ of that node in the tree, and that the height of most nodes is small.

Calling \textsc{\hyperref[alg:heapify]{MaxHeapify}} on a node of height $\tilde{h}$ costs $c \, \tilde{h}$, and there are at most $\lceil n/2^{\tilde{h}+1}\rceil$ nodes at that height.  
We can sum over all heights, starting from the leaves (with height $0$) up to the root (with height $\lfloor\log n\rfloor$);
\begin{align}
T(n)
&=   \sum_{\tilde{h}=0}^{\lfloor\log n\rfloor}\left\lceil \frac{n}{2^{\tilde{h}+1}}\right\rceil c \, \tilde{h}
\leq \sum_{\tilde{h}=0}^{\lfloor\log n\rfloor} \frac{n}{2^{\tilde{h}}} c \, \tilde{h}
=    c \, n \sum_{\tilde{h}=0}^{\lfloor\log n\rfloor}\frac{\tilde{h}}{2^{\tilde{h}}}
\leq c \, n \sum_{\tilde{h}=0}^{\infty}\frac{\tilde{h}}{2^{\tilde{h}}}
\label{eq:buildheap-sum}
\end{align}
Recall the geometric series:
\[
  \sum_{\tilde{h}=0}^\infty x^{\tilde{h}} = \frac1{1-x}, \quad |x|<1
\]
Differentiating both sides with respect to \(x\) gives:
\[
  \frac{\dif}{\dif x}\!\left(\sum_{\tilde{h}=0}^\infty x^{\tilde{h}}\right)
  = \frac{\dif}{\dif x}\!\left(\frac1{1-x}\right)
  \quad\Longrightarrow\quad
  \sum_{\tilde{h}=0}^\infty \tilde{h}\,x^{\tilde{h}-1} = \frac1{(1-x)^2}
\]
Multiply through by \(x\) to shift the exponent back:
\begin{equation}
\label{eq:geomseries-derivative}
\sum_{\tilde{h}=0}^\infty \tilde{h}\,x^{\tilde{h}}
= x \sum_{\tilde{h}=0}^\infty \tilde{h}\,x^{\tilde{h}-1}
= \frac{x}{(1-x)^2},\quad |x|<1,
\end{equation}
Setting $x=\tfrac12$ in \eqref{eq:geomseries-derivative} and substituting into \eqref{eq:buildheap-sum} gives
\begin{equation}
T(n) \leq c \, n \sum_{\tilde{h}=0}^{\infty}\tilde{h}\left(\frac{1}{2}\right)^{\tilde{h}} = c \, n \cdot \frac{\frac{1}{2}}{\left(1-\frac{1}{2}\right)^2} = O(n)
\end{equation}
Thus \textsc{\hyperref[alg:build_heap]{BuildMaxHeap}} builds a max-heap from an array in $O(n)$ time.


For Algorithm~\ref{alg:heap_sort}, we have a similar situation, but different from \textsc{\hyperref[alg:build_heap]{BuildMaxHeap}}, where the majority of the calls to \textsc{\hyperref[alg:heapify]{MaxHeapify}} are done on nodes at the bottom of the heap (where the height is small), in \textsc{\hyperref[alg:heap_sort]{HeapSort}}, the calls to \textsc{\hyperref[alg:heapify]{MaxHeapify}} are always done on the root of the heap, and the height is large for the majority of the calls.

After \textsc{\hyperref[alg:build_heap]{BuildMaxHeap}} has finished (line \ref{alg:heap_sort:build}), the array $A$ is a valid max-heap of size $n$.  
The \textsc{\hyperref[alg:heap_sort]{HeapSort}} loop (lines \ref{alg:heap_sort:start_for}-\ref{alg:heap_sort:end_for}) then performs exactly $n-1$ iterations.
At the $k^{\text{th}}$ iteration the heap contains $n-k+1$ elements, so the call to \textsc{\hyperref[alg:heapify]{MaxHeapify}} costs $\Theta\!\bigl(\log(n-k+1)\bigr)$.
The total time spent in the loop is therefore
\[
  \sum_{k=1}^{n-1} \Theta \left(\log(n-k+1)\right)
  \;=\;
  \Theta \left(\sum_{l=1}^{n-1}\log l \right)
  \;=\;
  \Theta \left(\log(n!)\right)
  \stackrel{\text{Ex. \ref{ex:logarithm-factorial}}}{=}
\Theta(n\log n)
\]
where the change of index $l=n-k+1$ rewrites the sum in increasing order.



\subsection{k-Smallest Element}
\label{subsec:quickselect}
Algorithm~\ref{alg:quickselect} is a randomized “divide-and-conquer” method for finding the $k$-th smallest element in an unsorted array in expected $O(n)$ time.  
At each step it chooses a pivot uniformly at random, partitions the input into three subsets -- those less than, equal to, and greater than the pivot -- and then recurses only on the subset that must contain the desired element.
\begin{algorithm}[htb]
    % def quickselect(A, k):
    % pivot = A[randint(0, len(A) - 1)]
    % L, M, R = [], [], []
    % for a in A: 
    %     if a < pivot:
    %         L.append(a)
    %     elif a == pivot:
    %         M.append(a)
    %     else:
    %         R.append(a)
    % if k <= len(L):
    %     return quickselect(L, k)
    % elif k <= len(L) + len(M):
    %     return pivot
    % else:
    %     return quickselect(R, k - (len(L) + len(M)))
\caption{Quick Select}
\label{alg:quickselect}
\begin{algorithmic}[1]
\Function{QuickSelect}{$A, k$} \Comment{$A$ is an unordered multiset (`bag') of elements}
\State $v \gets A[\operatorname{randint}(1,|A|)]$ \Comment{pick a random pivot}
\State $A_L, A_M, A_R \gets \{\}_m$ \Comment{three empty multisets}
\ForAll{$a \in A$}
    \If{$a < v$}
        \State add $a$ to $A_L$
    \ElsIf{$a = v$}
        \State add $a$ to $A_M$
    \Else
        \State add $a$ to $A_R$
    \EndIf
\EndFor
% \State $A_L \gets \langle\,x \in A \mid x < v\,\rangle$
% \State $A_M \gets \langle\,x \in A \mid x = v\,\rangle$
% \State $A_R \gets \langle\,x \in A \mid x > v\,\rangle$
\If{$k \le |A_L|$}
\State \Return \Call{QuickSelect}{$A_L, k$}
\ElsIf{$k \le |A_L| + |A_M|$}
\State \Return $v$
\Else
\State \Return \Call{QuickSelect}{$A_R,k - (|A_L| + |A_M|)$}
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

Partitioning around the pivot takes $\Theta(n)$ time, and since the pivot is random the expected size of the recursive call is at most a constant fraction of $n$, yielding an overall expected running time of $O(n)$.

\subsection{Overview of Sorting Algorithms}

% \begin{center}
% \begin{tabular}{lccc c}
%     \toprule
%     \textbf{Algorithm} & \multicolumn{3}{c}{\textbf{Complexity}} & \textbf{In place?} \\
%                         & \emph{worst}      & \emph{average}       & \emph{best}  & \\
%     \midrule
%     \textsc{Insertion-Sort} & $\Theta(n^2)$    & $\Theta(n^2)$        & $\Theta(n)$   & \ding{52} \\
%     \textsc{Selection-Sort} & $\Theta(n^2)$    & $\Theta(n^2)$        & $\Theta(n^2)$ & \ding{52} \\
%     \textsc{Merge-Sort}     & $\Theta(n\log n)$ & $\Theta(n\log n)$     & $\Theta(n\log n)$ & \ding{56} \\
%     \textsc{Quick-Sort}     & $\Theta(n^2)$    & $\Theta(n\log n)$     & $\Theta(n\log n)$ & \ding{52} \\
%     \textsc{Heap-Sort}      & $\Theta(n\log n)$ & $\Theta(n\log n)$     & $\Theta(n\log n)$ & \ding{52} \\
%     \bottomrule
% \end{tabular}
% \end{center}



\newcommand{\mixcolor}{70}

\definecolor{Red}{rgb}{1.0, 0.0, 0.0}
\definecolor{Orange}{rgb}{1.0, 0.5, 0.0}
\definecolor{Yellow}{rgb}{1.0, 1.0, 0.0}
\definecolor{Green}{rgb}{0.0, 1.0, 0.0}
\definecolor{BlueGreen}{rgb}{0.0, 1.0, 0.5}
\definecolor{Blue}{rgb}{0.0, 1.0, 1.0}


\begin{center}
    \begin{tabular}{l c c c c c}
        \toprule
        \textbf{Algorithm} & \multicolumn{3}{c}{\textbf{Time Complexity}} & \textbf{in place?} \\%& \textbf{Space}  \\
                            & {worst}      & {average}       & {best}  & \\%& \emph{worst} \\
        \midrule
        \textsc{InsertionSort} & \cellcolor{Red!\mixcolor}$\Theta(n^2)$    & \cellcolor{Red!\mixcolor}$\Theta(n^2)$        & \cellcolor{Green!\mixcolor}$\Theta(n)$   & \ding{52} \\%& \cellcolor{Blue!\mixcolor}$\mathcal{O}(1)$ \\
        \textsc{SelectionSort} & \cellcolor{Red!\mixcolor}$\Theta(n^2)$    & \cellcolor{Red!\mixcolor}$\Theta(n^2)$        & \cellcolor{Red!\mixcolor}$\Theta(n^2)$ & \ding{52} \\%& \cellcolor{Blue!\mixcolor}$\mathcal{O}(1)$ \\
        \textsc{MergeSort}     & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$ & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$     & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$ & \ding{56} \\%& \cellcolor{Green!\mixcolor}$\mathcal{O}(n)$ \\
        \textsc{QuickSort}     & \cellcolor{Red!\mixcolor}$\Theta(n^2)$    & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$     & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$ & \ding{52} \\%& \cellcolor{BlueGreen!\mixcolor}$\mathcal{O}(\log n)$ \\
        \textsc{HeapSort}      & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$ & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$     & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$ & \ding{52} \\%& \cellcolor{Blue!\mixcolor}$\mathcal{O}(1)$ \\
        \bottomrule
     \end{tabular}
\end{center}


\subsection{Application}
When an array is sorted, many operations and queries (e.g. the one in Example \ref{ex:caterpillar}) can be performed much more efficiently than in an unsorted array.

\begin{example}[Caterpillar Method]
\label{ex:caterpillar}
\definecolor{svgBlue}       {HTML}{0F75BD}
\definecolor{svgRed}        {HTML}{EB1C24}
\definecolor{svgGreenDark}  {HTML}{0E9348}
\definecolor{svgGreenLight} {HTML}{8FC43A}
\definecolor{svgYellow}     {HTML}{FFFF00}
% Checking if there a sorted array contains two elements $A[i]$ and $A[j]$ such that $A[i] + q = A[j]$, can be done in linear running time using the \includegraphics[height=2ex]{images/Hungrycaterpillar.pdf} method (sometimes also called \emph{two-pointer} or \emph{sliding-window} method).
Checking if a sorted array contains two elements $A[i]$ and $A[j]$ such that $A[i] + q = A[j]$, can be done in linear running time using the 
\begin{tikzpicture}[x=1mm,y=1mm,yscale=-1,scale=0.1]
  \path[fill=svgBlue] svg{m 157.18913 191.53421 c -0.65294 -0.39134 -1.41147 -1.16769 -1.81701 -1.85969 -0.59204 -1.01024 -0.67936 -1.42014 -0.58385 -2.74057 0.28372 -3.92242 4.20975 -6.0369 7.42406 -3.99845 1.65793 1.05143 2.05077 1.90213 2.05077 4.44099 0 2.45873 -0.25511 2.92729 -2.20157 4.04364 -1.77317 1.01696 -3.30611 1.05285 -4.8724 0.11408 z};
  \path[fill=svgRed] svg{m 162.16094 202.28463 c -2.31199 -0.80479 -5.51218 -3.34634 -7.18339 -5.70493 -3.3035 -4.66236 -3.51652 -11.58765 -0.5211 -16.94113 1.35536 -2.42231 4.21876 -5.04676 6.76393 -6.19948 1.86325 -0.84386 2.57212 -0.97367 5.319 -0.97393 2.42425 -2.3e-4 3.56364 0.16482 4.84708 0.70214 5.02426 2.10344 8.42886 6.56804 9.31109 12.20999 1.10931 7.09396 -3.21685 14.79046 -9.51276 16.92381 -2.6958 0.91348 -6.37153 0.90677 -9.02385 -0.0165 z m 8.25693 -4.80789 c 2.36514 -1.63378 1.37295 -2.92944 -1.08602 -1.41818 -2.06023 1.26619 -3.50533 1.15136 -6.43916 -0.51164 -0.73798 -0.41832 -0.89462 -0.41747 -1.18574 0.006 -0.48927 0.71217 0.16562 1.59506 1.76792 2.38344 2.0172 0.99255 5.13974 0.78569 6.943 -0.45995 z m -8.46621 -5.94253 c 1.57645 -0.87053 2.30023 -1.93159 2.55869 -3.75121 0.45522 -3.20479 -2.68621 -6.25609 -5.5574 -5.39791 -3.92371 1.17277 -4.86789 6.05337 -1.6771 8.66919 1.5928 1.30578 2.93149 1.44318 4.67581 0.47993 z m 12.92053 0.14281 c 1.77134 -0.78046 2.61059 -2.05403 2.76403 -4.19442 0.16774 -2.34026 -0.70928 -3.93178 -2.67604 -4.85616 -1.20596 -0.56677 -1.5875 -0.60079 -2.8612 -0.25518 -4.07594 1.10596 -4.71289 7.09715 -0.97404 9.16176 1.44063 0.79555 2.20211 0.82482 3.74725 0.144 z};
  \path[fill=black] svg{m 152.97534 209.11734 c -2.58672 -0.33038 -3.06549 -1.12043 -2.9364 -4.84548 l 0.1064 -3.07129 1.21176 -0.1058 1.21178 -0.1058 3.6e-4 2.13184 3.7e-4 2.13187 2.00221 0.017 c 3.22142 0.0274 5.16964 1.57253 3.68178 2.92001 -0.92038 0.83354 -3.0646 1.21037 -5.27832 0.92763 z M 62.15223 208.7513 c -1.146996 -0.61809 -1.29313 -0.97701 -1.299274 -3.19118 -0.0056 -1.87813 0.07652 -2.0902 0.943956 -2.44814 1.295202 -0.53445 1.287451 -0.53893 1.287451 0.74309 v 1.13497 l 2.250435 0.18481 c 3.63979 0.2989 5.09691 1.67883 3.31496 3.13933 -0.9403 0.77067 -5.33048 1.06602 -6.497528 0.43712 z m 10.626288 -1.8632 c -1.61115 -0.58775 -1.86425 -1.15626 -1.86425 -4.18729 v -2.9537 l 1.11855 -0.35315 1.11856 -0.35314 v 2.00849 2.00847 l 1.95747 0.001 c 5.43489 0.004 5.94372 3.73129 0.53818 3.94275 -1.27008 0.0497 -2.56091 -0.001 -2.86851 -0.11371 z m 68.172402 -0.46094 c -1.55643 -0.74466 -2.92323 -2.16671 -2.92323 -3.04138 0 -0.32667 0.87327 -1.97621 1.94062 -3.66564 l 1.94063 -3.07169 0.73652 1.05349 0.73652 1.05351 -0.98396 1.58527 -0.98395 1.58527 1.93923 1.15683 c 1.94737 1.16167 2.8259 2.44329 2.3862 3.48104 -0.32558 0.76838 -3.06002 0.69032 -4.78858 -0.1367 z m -9.69636 -3.86462 c -1.54806 -0.95268 -2.92104 -2.77809 -2.92104 -3.88359 0 -0.46288 0.85684 -1.55456 2.22151 -2.83036 l 2.22152 -2.07686 0.83922 0.7553 0.83923 0.75531 -1.27088 1.1875 -1.27088 1.1875 1.36754 1.26835 c 1.6764 1.55482 2.42386 3.24327 1.75762 3.97031 -0.682 0.74422 -2.2584 0.6053 -3.78384 -0.33346 z m -50.746042 -3.64127 c -2.80105 -3.16931 -3.22887 -3.81905 -3.01116 -4.57303 0.42317 -1.46552 1.78251 -0.71903 4.09885 2.25087 0.054 0.0693 0.88937 -0.34858 1.85626 -0.9286 2.08798 -1.25254 4.42092 -1.45924 4.78378 -0.42384 0.49634 1.41627 -1.41078 3.61311 -3.98056 4.5853 -2.0089 0.75999 -2.3422 0.67899 -3.74717 -0.9107 z m 43.939112 -5.44051 c -1.34318 -1.14726 -2.45261 -3.04618 -2.45261 -4.19793 0 -0.69946 0.79378 -1.41055 3.54211 -3.17309 1.76551 -1.13225 1.94197 -1.17538 3.33163 -0.81417 l 1.46737 0.38139 -2.25985 1.52088 -2.25984 1.52087 1.06947 1.16019 c 1.14932 1.24682 1.66952 2.89653 1.27147 4.03231 -0.39269 1.12051 -2.12993 0.91893 -3.70975 -0.43045 z m -33.684722 -1.5902 c -2.00487 -1.44994 -2.28209 -1.78076 -1.95547 -2.33348 0.60398 -1.02205 0.91214 -1.0421 2.35642 -0.15326 l 1.35873 0.83616 1.80426 -1.69528 c 1.88862 -1.77454 3.27494 -2.17476 4.1658 -1.20261 0.982975 1.07266 -0.68107 4.47935 -2.74465 5.61898 -1.70901 0.9438 -2.4072 0.79387 -4.98509 -1.07051 z m 72.265932 -21.60387 c -0.49215 -1.30922 -1.66606 -3.62651 -2.60868 -5.14953 -2.4138 -3.90007 -3.41024 -6.13364 -3.64145 -8.16247 -0.26752 -2.34752 0.29026 -3.29198 1.86194 -3.15269 1.46299 0.12965 2.58934 1.54416 3.60799 4.53103 1.20385 3.52985 2.72173 14.31404 2.01473 14.31404 -0.18684 0 -0.74238 -1.07117 -1.23453 -2.38038 z m 6.55805 2.12063 c -0.54796 -0.80297 1.40138 -12.30745 2.61189 -15.4147 1.11712 -2.86751 3.59436 -4.09387 4.82727 -2.38975 0.85516 1.18199 0.34083 2.84039 -2.82711 9.11584 -1.40632 2.78581 -2.85222 5.93885 -3.21312 7.00674 -0.61942 1.83291 -0.99099 2.27964 -1.39893 1.68187 z};
  \path[fill=svgGreenDark] svg{m 53.178419 203.12867 c -2.309776 -1.0424 -3.966718 -2.6754 -5.272052 -5.19583 -0.852006 -1.64512 -0.969704 -2.30336 -0.968746 -5.4182 7.46e-4 -2.91784 0.143541 -3.82512 0.805686 -5.12444 1.007891 -1.97774 3.569898 -4.52319 5.406898 -5.37193 2.384813 -1.10185 2.504316 -0.99651 2.27698 2.00726 -0.238887 3.15633 0.270624 5.43402 1.898528 8.48712 1.371817 2.57279 4.561752 5.62097 6.858296 6.5535 0.869869 0.35323 1.581579 0.75944 1.581579 0.90271 0 0.65872 -2.063219 2.36772 -3.81048 3.15628 -2.690429 1.2142 -6.091082 1.21556 -8.776689 0.004 z m 92.936891 -2.46894 c -3.39904 -1.41212 -5.04705 -4.09346 -5.31749 -8.65161 -0.20518 -3.45855 0.39509 -6.15539 2.10447 -9.45488 2.89056 -5.57958 9.03898 -9.51403 13.74293 -8.7943 2.60031 0.39788 2.66667 0.63514 0.70826 2.5313 -5.59084 5.41314 -6.55695 14.92796 -2.1257 20.93495 1.29547 1.75616 1.19618 1.95873 -1.57288 3.20843 -2.33442 1.05353 -5.33161 1.14343 -7.53959 0.22611 z m -71.130132 -7.77335 c -5.09804 -1.97207 -8.91885 -5.57567 -10.34477 -9.7567 -1.307998 -3.83523 -0.711355 -7.08411 1.68765 -9.18995 1.16472 -1.02238 1.2983 -1.06445 1.50789 -0.47506 0.78926 2.21951 1.73179 3.58296 3.71502 5.37415 3.48939 3.15149 7.31584 4.54719 12.48686 4.5546 1.53935 0.003 3.37387 -0.14534 4.07666 -0.3279 0.7028 -0.18254 1.33991 -0.27568 1.41578 -0.20698 0.0758 0.0687 0.1467 1.16777 0.15743 2.44236 0.0241 2.85653 -0.78381 4.80418 -2.58776 6.23866 -1.88586 1.49956 -3.56872 2.04341 -6.78651 2.19318 -2.44904 0.11397 -3.12186 0.008 -5.32825 -0.84636 z m 52.860352 -7.62911 c -2.87697 -0.91481 -5.03351 -4.28036 -5.03351 -7.85531 0 -1.9487 1.56286 -6.10663 3.11883 -8.29756 1.37058 -1.92992 4.14367 -4.48231 6.33503 -5.83089 6.03783 -3.71571 12.80346 -3.51204 15.77002 0.47475 1.05514 1.41798 1.70938 4.19906 1.41129 5.99926 l -0.23386 1.41232 -3.04043 0.1523 c -2.4959 0.12507 -3.47051 0.35065 -5.44182 1.25986 -4.42075 2.03894 -8.42187 6.58487 -9.8996 11.24752 -0.33844 1.0679 -0.81316 1.93496 -1.05495 1.9268 -0.24176 -0.008 -1.11072 -0.22821 -1.931 -0.48905 z M 80.221698 174.83918 c -6.06516 -1.872 -10.33049 -6.1746 -10.94791 -11.04354 -0.31016 -2.44598 0.8029 -4.91801 2.97123 -6.59892 l 1.52658 -1.1834 0.44992 1.5973 c 1.17667 4.17735 5.09944 8.97869 9.13151 11.17666 1.10149 0.60045 3.13728 1.3453 4.52399 1.65521 2.12791 0.47557 2.83578 0.49975 4.53698 0.15496 1.10863 -0.2247 2.45423 -0.72094 2.99023 -1.10278 0.53598 -0.38183 1.04269 -0.63252 1.12597 -0.55709 0.0833 0.0754 -0.24876 0.91944 -0.7379 1.87558 -2.16692 4.2358 -9.09063 6.02605 -15.5706 4.02602 z m 31.345832 -6.64416 c -3.20176 -1.53187 -4.88522 -4.73878 -4.84409 -9.22755 0.0472 -5.15277 2.55444 -10.30959 6.70433 -13.78913 7.48562 -6.27649 15.40641 -4.18799 17.00263 4.48312 l 0.22338 1.21333 -2.34106 0.90108 c -6.42008 2.47114 -11.04471 8.31638 -11.71018 14.80087 l -0.2361 2.30068 -1.7175 -0.0149 c -0.94839 -0.008 -2.32827 -0.30714 -3.08141 -0.66747 z m -16.763352 -2.1475 c -2.20523 -0.7137 -3.93548 -1.83417 -5.87387 -3.8038 -7.15303 -7.26834 -7.18378 -19.73184 -0.0568 -22.99723 3.07303 -1.40797 7.02216 -0.85044 9.97688 1.40851 l 1.206002 0.92201 -0.933312 1.24067 c -1.38833 1.84555 -3.0214 5.95917 -3.54451 8.92843 -0.86913 4.93332 0.16142 10.57414 2.42234 13.25867 0.50392 0.59833 0.81163 1.18263 0.68376 1.29841 -0.38239 0.34632 -2.43916 0.21082 -3.88057 -0.25567 z};
  \path[fill=svgGreenLight] svg{m 45.359131 202.6977 c -3.294933 -1.38158 -5.20006 -4.48 -5.212216 -8.47702 -0.007 -2.08279 0.157828 -2.80011 0.924414 -4.03903 1.13705 -1.83767 3.025286 -3.05214 4.745404 -3.05214 1.258378 0 1.274372 0.0181 0.970199 1.09744 -0.170057 0.6036 -0.309243 2.541 -0.309243 4.30535 0 2.80692 0.130647 3.46117 1.044806 5.23395 0.574679 1.11434 1.653453 2.59224 2.397293 3.28423 l 1.352411 1.25818 -1.359942 0.51462 c -1.762065 0.66678 -2.726894 0.64017 -4.553126 -0.12558 z m 19.015476 -4.04542 c -9.12575 -3.95957 -12.230232 -16.79128 -5.352409 -22.123 1.53317 -1.18853 3.398215 -1.88803 5.044208 -1.89184 0.775722 -0.001 0.786492 0.0414 0.252757 1.01029 -1.565906 2.84283 -0.705212 7.62933 2.003295 11.14042 1.88768 2.44706 5.25875 4.85389 8.32394 5.94302 l 2.49647 0.88704 -0.41849 1.061 c -0.23016 0.58355 -1.17352 1.749 -2.09633 2.58988 -2.81955 2.56922 -6.40516 3.05292 -10.253441 1.38319 z m 72.130493 -2.8544 c -3.63894 -0.91169 -5.86541 -3.71583 -6.23171 -7.84845 -0.4519 -5.09858 3.97703 -12.29038 9.28026 -15.06952 4.68292 -2.45404 9.3092 -2.52169 12.15186 -0.17768 l 0.82915 0.68368 -1.90453 0.79913 c -4.07745 1.71087 -8.40746 7.04175 -9.81754 12.08681 -0.62829 2.24807 -0.72323 6.06964 -0.20135 8.10818 0.18885 0.73767 0.31066 1.36992 0.27074 1.405 -0.38963 0.34228 -3.03103 0.35004 -4.37688 0.0128 z M 79.198888 182.41954 c -4.96159 -1.2143 -8.67654 -3.90959 -10.64826 -7.72558 -0.92538 -1.79091 -1.07255 -2.42287 -0.90547 -3.88873 0.1862 -1.6339 1.26162 -4.27386 1.741 -4.27386 0.11712 0 0.4526 0.49384 0.74574 1.09743 0.29314 0.60361 1.49875 1.97445 2.6792 3.04635 5.99439 5.4433 16.17929 6.51135 21.08617 2.21123 l 1.18586 -1.03922 0.25432 1.22761 c 0.8158 3.93837 -2.23029 8.0837 -6.8716 9.35133 -2.56884 0.70159 -6.38451 0.6989 -9.26696 -0.006 z m 40.983252 -8.11713 c -2.83148 -1.8576 -4.10451 -5.24048 -3.45142 -9.17156 0.90738 -5.46129 5.0649 -10.62906 10.55239 -13.11651 3.2678 -1.48125 5.37329 -1.72037 8.26308 -0.93839 3.16608 0.85675 5.82247 4.46965 5.82247 7.91903 0 1.05744 -0.0741 1.12058 -1.31677 1.12058 -1.8718 0 -5.52172 1.18759 -7.9198 2.57689 -4.16681 2.414 -8.18671 7.20369 -9.08004 10.81883 -0.24458 0.9898 -0.64451 1.79963 -0.88868 1.79963 -0.24422 0 -1.13575 -0.45383 -1.98123 -1.0085 z m -33.436372 -4.53009 c -5.43371 -1.56041 -10.13765 -5.96028 -12.12924 -11.34514 -0.64757 -1.75093 -0.77922 -2.74128 -0.65093 -4.89628 0.29322 -4.92454 2.75475 -7.53964 7.56022 -8.03183 l 1.86428 -0.19095 -0.11851 3.16975 c -0.20427 5.46592 1.62735 10.00549 5.5816 13.8338 2.50131 2.42163 5.32071 3.88203 7.49455 3.88203 h 1.43929 l -0.61576 0.94403 c -1.70755 2.61795 -6.37568 3.7976 -10.42555 2.63459 z m 14.502102 -2.5157 c -2.013482 -0.85298 -2.997732 -1.82651 -4.265732 -4.2193 -2.7323 -5.15604 -2.3228 -12.15855 1.08917 -18.62364 3.988112 -7.55672 11.303672 -10.35266 15.761452 -6.02386 1.11774 1.08538 2.55128 3.14683 2.5534 3.67176 0 0.1921 -0.47523 0.60428 -1.05778 0.91593 -3.21601 1.72049 -6.64948 6.022 -8.07971 10.12224 -1.6187 4.64068 -1.09164 10.54045 1.14126 12.775 0.5849 0.58533 0.5412 0.64845 -0.81896 1.18334 -2.23111 0.87736 -4.54896 0.95013 -6.3231 0.19853 z};
  \path[fill=svgBlue] svg{m 170.63883 191.67702 c -3.3344 -2.53777 -2.39326 -7.45897 1.67222 -8.74401 4.313 -1.36327 8.16626 3.80777 5.72402 7.68159 -1.5299 2.42671 -4.95688 2.91897 -7.39624 1.06242 z};
  \path[fill=svgYellow] svg{m 159.00066 180.68498 c -1.73762 0.2329 -3.52017 1.9332 -4.15438 3.9617 -1.93334 6.18748 4.17728 11.41752 8.24309 7.05589 2.01369 -2.16033 2.33252 -5.93765 0.72019 -8.53574 -1.11593 -1.79827 -2.92915 -2.73437 -4.8089 -2.48185 m 13.72912 0.0343 c -4.32849 0.93564 -5.80559 7.56268 -2.46731 11.07237 3.31797 3.48832 8.4807 0.55579 8.48805 -4.82121 0.007 -3.90291 -2.90496 -6.92495 -6.02074 -6.25116 m -11.91158 1.96839 c 2.12471 0.75795 3.1901 4.07658 2.072 6.45468 -1.44298 3.06881 -5.04047 3.09642 -6.42381 0.0494 -1.65339 -3.64193 1.04938 -7.68202 4.35181 -6.50411 m 14.10412 0.0815 c 1.92986 0.88445 2.87946 3.68291 2.04479 6.02672 -1.34361 3.77289 -5.89501 3.44691 -6.89042 -0.49343 -0.90725 -3.59206 1.95363 -6.85862 4.84563 -5.53329};
  \path[fill=black] svg{m 163.74844 198.05128 c -1.44602 -0.71168 -2.35134 -1.80511 -1.96423 -2.37239 0.34368 -0.50361 0.76995 -0.40708 2.26783 0.51349 1.8136 1.11463 3.6586 1.10304 5.50715 -0.0346 1.0614 -0.65319 1.53608 -0.78082 1.84922 -0.49722 0.29948 0.27121 0.30756 0.54483 0.028 0.94623 -1.36659 1.96211 -5.18022 2.67862 -7.68796 1.44445 z};
  \end{tikzpicture}
method (sometimes also called ``{two-pointer}'' or ``{sliding-window}'' method).

\begin{center}
\begin{tikzpicture}[xscale=0.3, yscale=0.3]

  % data & bounds
  \def\A{3,5,10,11,13,16,20,21,24}
  \def\q{4}
  \def\xmin{2}   % = min(A)-1
  \def\xmax{25}  % = max(A)+1
  \def\ymin{2}
  \def\ymax{25}

  %--- grid at each A[k] ---
  \foreach \a in \A {
    \draw[gray!30, thin] (\a,\ymin) -- (\a,\ymax);
    \draw[gray!30, thin] (\xmin,\a) -- (\xmax,\a);
  }

  %--- axes ---
  \draw[->] (\xmin,\ymin) -- (\xmax+0.5,\ymin) node[below right] {$x$};
  \draw[->] (\xmin,\ymin) -- (\xmin,\ymax+0.5) node[above left]  {$y$};

  %--- ticks: first, last, and A[i] / A[j] ---
  \node[below] at (3,\ymin)  {\small $A[1]$};
  \node[below] at (13.5,\ymin) {\small $\ldots$};
  \node[below] at (24,\ymin) {\small $A[n]$};

  \node[left] at (\xmin,3)  {\small $A[1]$};
  \node[left] at (\xmin,13.5) {\small $\vdots$};
  \node[left] at (\xmin,24) {\small $A[n]$};

  \node[below] at (16,\ymin) {\small\textcolor{Orange}{$A[i]$}};
  \node[left] at (\xmin,20) {\small\textcolor{Orange}{$A[j]$}};

  % Orange dotted line from (16,\ymin) to (16,20) and from (\xmin,20) to (16,20)
  \draw[Orange, dotted, thick] (16,\ymin) -- (16,20);
  \draw[Orange, dotted, thick] (\xmin,20) -- (16,20);

  %--- dashed line y = x + q ---
  \draw[Magenta, dashed, thick, domain=\xmin:{\ymax -\q}] 
        plot (\x,{\x+\q}) node[above right, Magenta] {$y = x + q$};

  %--- caterpillar arrows ---
  \draw[->, black, thick] (3,3)   -- (3,5);
  \draw[->, black, thick] (3,5)   -- (3,10);
  \draw[->, black, thick] (3,10)  -- (5,10);
  \draw[->, black, thick] (5,10)  -- (10,10);
  \draw[->, black, thick] (10,10) -- (10,11);
  \draw[->, black, thick] (10,11) -- (10,13);
  \draw[->, black, thick] (10,13) -- (10,16);
  \draw[->, black, thick] (10,16) -- (11,16);
  \draw[->, black, thick] (11,16) -- (13,16);
  \draw[->, black, thick] (13,16) -- (13,20);
  \draw[->, black, thick] (13,20) -- (16,20);

  %--- final red dot ---
  \fill[Orange] (16,20) circle (6pt);

\end{tikzpicture}
\end{center}

\begin{algorithm}[htb]
  \caption{Checking for two elements with difference $q$}
  \label{alg:caterpillar}
\begin{algorithmic}[1]
  \Function{Caterpillar}{$A, q$}
    \State $l, r \gets 1$ \label{line:init-indices}
    \While{$r \leq n$}
      \If{$A[r] < A[l] + q$}
        \State $r \gets r + 1$
      \ElsIf{$A[r] > A[l] + q$}
        \State $l \gets l + 1$
      \Else
        \State \Return \tru
      \EndIf
    \EndWhile
    \State \Return \fals
  \EndFunction
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:caterpillar} maintains two indices \(l\) and \(r\), both starting at the left end of \(A\) (Line \ref{line:init-indices}).
At each step, it compares \(A[r]\) and \(A[l]\) and checks if the difference \(A[r] - A[l]\) is less than, greater than, or equal to \(q\):

\begin{itemize}%[before={\parskip = 0em}, nosep]
  \item If \(A[r] - A[l] < q\), move \(r\) one step to the right (to increase the difference).
  \item If \(A[r] - A[l] > q\), move \(l\) one step to the right (to decrease the difference).
  \item If \(A[r] - A[l] = q\), we found a valid pair and stop. \qedhere
\end{itemize}
\end{example}


\clearpage
% \pagebreak[3]
\section{Data Structures}
\label{sec:data_structures}

Operations on a dynamic set can be grouped into two categories, \textit{queries}, which return information about the set, and \textit{modifying operations}, which change the set:
\begin{description}[style=nextline, font={$\; \bullet$~\normalfont}]
\item[\textsc{Search$(S, k)$}]
\emph{query} that, given a set $S$ and a key value $k$, returns a pointer $x$ to an element in $S$ such that $x.\text{key} = k$, or \textsc{nil} if no such element belongs to $S$
\item[\textsc{Insert$(S, x)$}]
\emph{modifying operation} that adds the element pointed to by $x$ to the set $S$ (we usually assume that any attributes in element $x$ needed by the set implementation have already been initialized)
\item[\textsc{Delete$(S, x)$}]
\emph{modifying operation} that, given a pointer $x$ to an element in the set $S$, removes $x$ from $S$ (note that this operation takes a pointer to an element $x$, \underline{not} a key value\textcolor{red}{!})
\item[\textsc{Minimum$(S)$} and \textsc{Maximum$(S)$}]
\emph{queries} on a totally ordered set $S$ that return a pointer to the element of $S$ with the smallest (for \textsc{Minimum}) or largest (for \textsc{Maximum}) key
\item[\textsc{Successor$(S, x)$}]
\emph{query} that, given an element $x$ whose key is from a totally ordered set $S$, returns a pointer to the next larger element in $S$, or \textsc{nil} if $x$ is the maximum element
\item[\textsc{Predecessor$(S, x)$}]
\emph{query} that, given an element $x$ whose key is from a totally ordered set $S$, returns a pointer to the next smaller element in $S$, or \textsc{nil} if $x$ is the minimum element
\end{description}

% stack and queue are dynamic sets in which the element removed is prespecified:
% \begin{description}[before={\parskip=0pt}, nosep, leftmargin=1em, labelindent=1em, font={\normalfont}]
% \item[\textbf{Stack}:] element deleted is the one most recently inserted (\textbf{LIFO}: last-in, first-out)
% \item[\textbf{Queue}:] element deleted is the one that has been in the set the longest (\textbf{FIFO}: first-in, first-out)
% \end{description}


%--------------------------------------------------------------------
\subsection{Stacks}
\label{subsec:stacks}

\begin{definition}[Stack]
A \emph{stack} is a LIFO container that
supports constant-time insertion and deletion at one end.
\end{definition}

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}[
    % font=\footnotesize,
    scale=1.2]
    \draw[fill=arraycolor] (0,0) rectangle (3,0.5);
    \draw[]   (3,0) rectangle (5,0.5);
    \draw[thick] (0,0) rectangle (5,0.5);
    % \foreach \x in {1,2,3,4} \draw (\x,0)--(\x,0.5);
    draw (3,0) -- (3,0.5);
    \node at (4,0.25) {free};
    \node[left, outer sep=0.5ex] at (0,0.25) {$S$};
    \node[above] at (0.25,0.5) {$1$};
    \draw[dotted] (2.5,0.5) -- (2.5,0);
    \node[above] at (2.75,0.5) {$\attrib{S}{top}$};
    % \draw[<-] (2.75,0.55) -- ++(0,0.2) node[above] {$\attrib{S}{top}$};
    \node[above] at (4.75,0.5) {$N$};
  \end{tikzpicture}
  %\vspace*{-1.0em}
  %\caption{Stack after three \textsc{Push} operations.}
  \label{fig:stack}
\end{figure}

Interface (Algorithm \ref{alg:stack_ops}):
\begin{itemize}%[before={\parskip=0pt},nosep]
  \item \textsc{StackEmpty}\((S)\): returns \tru{} iff the stack \(S\) contains no elements.
  \item \textsc{Push}\((S,x)\): places element \(x\) on the top of the stack \(S\).
  \item \textsc{Pop}\((S)\): removes and returns the top element of the stack \(S\).
\end{itemize}

The stack has attributes \(\attrib{S}{top}\), indexing the most recently inserted element, and \(\attrib{S}{length}\), equaling the size \(N\) of the array.



\begin{algorithm}[htb]
  \caption{Stack Operations (array-based)}
  \label{alg:stack_ops}
  \begin{algorithmic}[1]
    \Function{StackEmpty}{$S$}
      \State \Return $\bigl(\attrib{S}{top}=0\bigr)$
    \EndFunction
    \Function{Push}{$S,x$}
      \If{$\attrib{S}{top}=\attrib{S}{length}$}
        \State \textbf{error} ``overflow''
      \EndIf
      \State $\attrib{S}{top}\gets\attrib{S}{top}+1$
      \State $S[\attrib{S}{top}] \gets x$
    \EndFunction
    \Function{Pop}{$S$}
      \If{\Call{StackEmpty}{$S$}}
        \State \textbf{error} ``underflow''
      \EndIf
      \State $\attrib{S}{top}\gets\attrib{S}{top}-1$
      \State \Return $S[\attrib{S}{top}+1]$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

%--------------------------------------------------------------------
\subsection{Queues}
\label{subsec:queues}

\begin{definition}[Queue]
A \emph{queue} is a FIFO container with constant-time insertion at the tail and deletion at the head.
\end{definition}


Interface (Algorithms~\ref{alg:enqueue} and \ref{alg:dequeue}):
\begin{itemize}%[before={\parskip=0pt},nosep]
  \item \textsc{Enqueue}\((Q,x)\): insert \(x\) at the tail of \(Q\).
  \item \textsc{Dequeue}\((Q)\): remove and return the head element of \(Q\).
\end{itemize}

The classic fixed-length \emph{circular-array} implementation keeps two indices \(\attrib{Q}{head}\) and \(\attrib{Q}{tail}\) (\(\attrib{Q}{head}\) points to the first element, \(\attrib{Q}{tail}\) to the first free slot).  


\begin{figure}[htb]
  \centering
  \begin{tikzpicture}[
    % font=\footnotesize,
    scale=1.2]
    \draw[fill=arraycolor] (2,0) rectangle (5.5,0.5);
    \draw[thick] (0,0) rectangle (8,0.5);
    draw (2,0) -- (2,0.5);
    \node at (1,0.25) {free};
    \node at (6.75,0.25) {free};
    \node[left, outer sep=0.5ex] at (0,0.25) {$Q$};
    \node[above] at (0.25,0.5) {$1$};
        % ----- head / tail arrows -----
        % \draw[<-] (2.25,0.55) -- ++(0,0.2) node[above] {$\attrib{Q}{head}$};
        % \draw[<-] (5.75,0.55) -- ++(0,0.2) node[above] {$\attrib{Q}{tail}$};
    \node[above] at (2.25,0.5) {$\attrib{Q}{head}$};
    \draw[dotted] (2.5,0.5) -- (2.5,0);
    \node[above] at (5.75,0.5) {$\attrib{Q}{tail}$};
    \draw[dotted] (6,0.5) -- (6,0);
    \node[above] at (7.75,0.5) {$N$};
  \end{tikzpicture}
  \label{fig:queue}
\end{figure}

\begin{algorithm}[htb]
  \caption{Enqueue (circular array)}
  \label{alg:enqueue}
  \begin{algorithmic}[1]
    \Function{Enqueue}{$Q,x$}
      \If{$\attrib{Q}{queue\mbox{-}full}$}
        \State \textbf{error} ``overflow''
      \EndIf
      \State $Q[\attrib{Q}{tail}] \gets x$
      \State $\attrib{Q}{tail} \gets (\attrib{Q}{tail} \bmod \attrib{Q}{length}) + 1$
      \State $\attrib{Q}{queue\mbox{-}empty} \gets \fals$
      \If{$\attrib{Q}{tail} = \attrib{Q}{head}$}
        \State $\attrib{Q}{queue\mbox{-}full} \gets \tru$
      \EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[htb]
  \caption{Dequeue (circular array)}
  \label{alg:dequeue}
  \begin{algorithmic}[1]
    \Function{Dequeue}{$Q$}
      \If{$\attrib{Q}{queue\mbox{-}empty}$}
        \State \textbf{error} ``underflow''
      \EndIf
      \State $x \gets Q[\attrib{Q}{head}]$
      \State $\attrib{Q}{head} \gets (\attrib{Q}{head} \bmod \attrib{Q}{length}) + 1$
      \State $\attrib{Q}{queue\mbox{-}full} \gets \fals$
      \If{$\attrib{Q}{tail} = \attrib{Q}{head}$}
        \State $\attrib{Q}{queue\mbox{-}empty} \gets \tru$
      \EndIf
      \State \Return $x$
    \EndFunction
  \end{algorithmic}
\end{algorithm}


%--------------------------------------------------------------------
\subsection{Linked Lists}
\label{subsec:linked_lists}

\begin{definition}[Doubly-Linked List]
  A \emph{doubly-linked list} \(L\) consists of nodes \(x\) that store a key \(\attrib{x}{key}\) and two links \(\attrib{x}{prev}\), \(\attrib{x}{next}\).
  A special sentinel node \(\attrib{L}{nil}\) simplifies boundary cases because the list is empty iff \(\attrib{\attrib{L}{nil}}{next} = \attrib{L}{nil}\).
  \end{definition}


\begin{figure}[htb]
  \centering

  \begin{tikzpicture}[scale=0.65,every node/.style={font=\normalsize}]
    \def\cw{1} % cell width
    
    \newcommand{\cell}[5]{%
        \pgfmathsetmacro\x{#2*\cw}%
        \pgfmathsetmacro\y{#3*\cw}%
        \draw[fill=#5] (\x-0.5*\cw,\y-0.5*\cw) rectangle ++(\cw,\cw);
        \node at (\x,\y) {#1};
        \coordinate (#4) at (\x,\y); % anchor for arrows
    }
  
    % sentinel
    \cell{}{0}{1}{nil_next}{white!25}
    \cell{\textsc{nil}}{0}{0}{nil_key}{white!25}
    \cell{}{0}{-1}{nil_prev}{white!25}
    
    % first element
    \cell{}{3}{1}{x1_next}{white!25}
    \cell{$k_1$}{3}{0}{x1_key}{arraycolor}
    \cell{}{3}{-1}{x1_prev}{white!25}
  
    % second element
    \cell{}{6}{1}{x2_next}{white!25}
    \cell{$k_2$}{6}{0}{x2_key}{arraycolor}
    \cell{}{6}{-1}{x2_prev}{white!25}
    
    % elements in between
    \node[] at (9*\cw,0) (n2) {$\hdots$};
    
    % last element
    \cell{}{12}{1}{xn_next}{white!25}
    \cell{$k_n$}{12}{0}{xn_key}{arraycolor}
    \cell{}{12}{-1}{xn_prev}{white!25}
  
    % pointer to sentinel
    \coordinate (nil_label_coord) at ($(nil_prev) + (-1*\cw,0*\cw)$);
    \node[left=0em of nil_label_coord] (nil_label) {$\attrib{L}{nil}$};
    \draw[->] (nil_label) -- ($(nil_prev) + (-0.5*\cw,0*\cw)$);
    
    % dots and arrows  
    % next
    \draw[->] (nil_next) node[circle, fill=black, inner sep=0pt, minimum size=2pt] {} -- ($(x1_next) - (0.5*\cw,0)$);
    \draw[->] (x1_next) node[circle, fill=black, inner sep=0pt, minimum size=2pt] {} -- ($(x2_next) - (0.5*\cw,0)$);
    % \draw[-] (x2_next) node[circle, fill=black, inner sep=0pt, minimum size=2pt] {} -- ($(x2_next)!0.2!($(xn_next) - (0.5*\cw,0)$)$);
    % \draw[dotted] ($(x2_next)!0.2!($(xn_next) - (0.5*\cw,0)$)$) -- ($(x2_next)!0.3!($(xn_next) - (0.5*\cw,0)$)$);
    % \draw[dotted] ($(x2_next)!0.7!($(xn_next) - (0.5*\cw,0)$)$) -- ($(x2_next)!0.8!($(xn_next) - (0.5*\cw,0)$)$);
    % \draw[->] ($(x2_next)!0.8!($(xn_next) - (0.5*\cw,0)$)$) -- ($(xn_next) - (0.5*\cw,0)$);
    \draw[-] (x2_next) node[circle, fill=black, inner sep=0pt, minimum size=2pt] {} -- ($(x2_next)!0.25!(xn_next)$);
    \draw[dotted] ($(x2_next)!0.25!(xn_next)$) -- ($(x2_next)!0.35!(xn_next)$);
    \draw[dotted] ($(x2_next)!0.65!(xn_next)$) -- ($(x2_next)!0.75!(xn_next)$);
    \draw[->] ($(x2_next)!0.75!(xn_next)$) -- ($(xn_next) - (0.5*\cw,0)$);
    \draw[->,rounded corners=4pt] (xn_next) node[circle, fill=black, inner sep=0pt, minimum size=2pt] {} -- +(0,1) -- ($(nil_next) + (-1*\cw,1)$) -- ($(nil_next) + (-1*\cw,0)$) -- ($(nil_next) - (0.5*\cw,0)$); % I do not want a bezier, but straight line going through the control points (maybe with a little bit of curvature to make it look nice)
    % prev
    \draw[->] (x1_prev) node[circle, fill=black, inner sep=0pt, minimum size=2pt] {} -- ($(nil_prev) + (0.5*\cw,0)$);
    \draw[->] (x2_prev) node[circle, fill=black, inner sep=0pt, minimum size=2pt] {} -- ($(x1_prev) + (0.5*\cw,0)$);
    % \draw[-]  (xn_prev) node[circle, fill=black, inner sep=0pt, minimum size=2pt] {} -- ($(xn_prev)!0.2!($(x2_prev) + (0.5*\cw,0)$)$);
    % \draw[dotted] ($(xn_prev)!0.2!($(x2_prev) + (0.5*\cw,0)$)$) -- ($(xn_prev)!0.3!($(x2_prev) + (0.5*\cw,0)$)$);
    % \draw[dotted] ($(xn_prev)!0.7!($(x2_prev) + (0.5*\cw,0)$)$) -- ($(xn_prev)!0.8!($(x2_prev) + (0.5*\cw,0)$)$);
    % \draw[->] ($(xn_prev)!0.8!($(x2_prev) + (0.5*\cw,0)$)$) -- ($(x2_prev) + (0.5*\cw,0)$);
    \draw[-] (xn_prev) node[circle, fill=black, inner sep=0pt, minimum size=2pt] {} -- ($(xn_prev)!0.25!(x2_prev)$);
    \draw[dotted] ($(xn_prev)!0.25!(x2_prev)$) -- ($(xn_prev)!0.35!(x2_prev)$);
    \draw[dotted] ($(xn_prev)!0.65!(x2_prev)$) -- ($(xn_prev)!0.75!(x2_prev)$);
    \draw[->] ($(xn_prev)!0.75!(x2_prev)$) -- ($(x2_prev) + (0.5*\cw,0)$);
    \draw[->,rounded corners=4pt] (nil_prev) node[circle, fill=black, inner sep=0pt, minimum size=2pt] {}
              -- +(0,-1)                             % drop a little
              -- ($(xn_prev) + (1.0*\cw,-1)$)       % travel under the diagram
              -- ($(xn_prev) + (1.0*\cw,0)$)        % come up again
              -- ($(xn_prev) + (0.5*\cw,0)$);       % arrow-head at last element
  
  \end{tikzpicture}
  
  
\end{figure}

\begin{algorithm}[htb]
  \caption{Doubly-Linked List Operations (with sentinel)}
  \label{alg:list_ops}
  \begin{algorithmic}[1]
    \Function{ListInit}{$L$}
      \State $\attrib{\attrib{L}{nil}}{prev} \gets \attrib{L}{nil}$
      \State $\attrib{\attrib{L}{nil}}{next} \gets \attrib{L}{nil}$
    \EndFunction
    \Function{ListInsert}{$L,x$} \Comment{insert at front}
      \State $\attrib{x}{next} \gets \attrib{\attrib{L}{nil}}{next}$
      \State $\attrib{\attrib{\attrib{L}{nil}}{next}}{prev} \gets x$
      \State $\attrib{\attrib{L}{nil}}{next} \gets x$
      \State $\attrib{x}{prev} \gets \attrib{L}{nil}$
    \EndFunction
    \Function{ListDelete}{$x$}
      \State $\attrib{\attrib{x}{prev}}{next} \gets \attrib{x}{next}$
      \State $\attrib{\attrib{x}{next}}{prev} \gets \attrib{x}{prev}$
    \EndFunction
    \Function{ListSearch}{$L,k$}
      \State $x \gets \attrib{\attrib{L}{nil}}{next}$
      \While{$x \ne \attrib{L}{nil} \AND \attrib{x}{key} \ne k$}
        \State $x \gets \attrib{x}{next}$
      \EndWhile
      \State \Return $x$
    \EndFunction
  \end{algorithmic}
\end{algorithm}



Algorithm~\ref{alg:list_ops} shows the basic operations on a doubly-linked list.
\textsc{ListInsert} and \textsc{ListDelete} take \(O(1)\) time,
whereas \textsc{ListSearch} takes \(\Theta(n)\) time in the worst case,
with \(n\) the current length of \(L\).


%--------------------------------------------------------------------
\subsection{Dictionaries}
\begin{definition}[Dictionary]
\label{def:dictionary}
A \emph{dictionary} is an abstract data structure that represents a set of elements (or keys). 
It is a \emph{dynamic set} that supports the following operations:
\begin{itemize}%[before={\parskip=0pt}, nosep]
\item \textsc{Insert}: insert element into the set
\item \textsc{Delete}: delete element from the set
\item \textsc{Search}: test memebership of an element in the set \qedhere
\end{itemize}
\end{definition}

\subsection{Direct-Address Tables}
\label{subsec:direct_address}
\begin{definition}[Direct-Address Table]
A \emph{direct-address table} implements a dictionary.
Suppose the key universe is the set \(U=\{1,\dots,M\}\).
A \emph{direct-address table} is an array \(T[1:M]\) where slot \(T[k]\) stores a Boolean indicating membership of key \(k\) in the represented set.
\end{definition}
\begin{algorithm}[htb]
  \caption{Direct-Address Table Operations}
  \label{alg:direct_address}
  \begin{algorithmic}[1]
    \Function{DirectAddressInsert}{$T,k$}  \State $T[k] \gets \tru$  \EndFunction
    \Function{DirectAddressDelete}{$T,k$}  \State $T[k] \gets \fals$ \EndFunction
    \Function{DirectAddressSearch}{$T,k$}  \State \Return $T[k]$     \EndFunction
  \end{algorithmic}
\end{algorithm}
All direct-address table operations (Algorithm \ref{alg:direct_address}) cost \(O(1)\) time, but the table occupies \(\Theta(|U|)\) space, which is prohibitive when the universe is large and the actual set is sparse.
i.e., direct-address tables usually waste a lot of space.


%--------------------------------------------------------------------
\subsection{Hash Tables}
\label{subsec:hash_tables}

To reduce the space overhead we use a smaller table \(T\) with \(|T| \ll |U|\) and map each key \(k\in U\) to a position in \(T\) using a
\emph{hash function} \(h:U\rightarrow\{1,\dots,|T|\}\).

\begin{definition}[Load Factor]
For a table of size \(|T|\) that currently stores \(n\) keys, the
\emph{load factor} is \(\alpha = \frac{n}{|T|}\).
\end{definition}

\subsubsection{Chaining}
Each slot \(T[i]\) stores a linked list of keys that hash to \(i\).




% \begin{figure}[htb]
% \centering
% % stolen from https://tex.stackexchange.com/questions/636419/draw-hash-table, who stole it from CLRS
% \begin{tikzpicture}[scale=0.8,
% node distance = 7mm and 4mm,
%   start chain = going right, 
%     arr/.style = {semithick, ->},
%     dot/.style = {circle, fill, inner sep=1.2pt,label={[fill=none]left:#1}},
% every label/.append style = {font=\footnotesize, fill=white, align=center,
%                               fill opacity=0.5, text opacity=1, 
%                               inner sep=1pt},
%       E/.style = {ellipse, draw, fill=#1},
%   mpnh/.style = {rectangle split, rectangle split horizontal, 
%                   rectangle split parts=3, draw, fill=arraycolor,
%                   inner sep=2pt,
%                   on chain},
%   mpnv/.style = {rectangle split, rectangle split parts=10,
%       rectangle split part fill={myblue!30,myorange!30,myblue!30,myblue!30,myblue!30,
%                                 myorange!30,myblue!30,myorange!30,myorange!30,myblue!30},
%       draw, minimum height=2ex},
%     sym/.style = {yshift=-1mm},
%     syp/.style = {yshift=+1mm},
%                         ]
% \node[mpnv, label=$T$] (H) 
%     {\nodepart{one}     $\diagup$
%       \nodepart{two}     \vphantom{$\diagup$} 
%       \nodepart{three}   $\diagup$
%       \nodepart{four}    $\diagup$
%       \nodepart{five}    $\diagup$
%       \nodepart{six}     \vphantom{$\diagup$} 
%       \nodepart{seven}   $\diagup$
%       \nodepart{eight}   \vphantom{$\diagup$} 
%       \nodepart{nine}    \vphantom{$\diagup$} 
%       \nodepart{ten}     $\diagup$
%     };
% %
% \node[mpnh, right=of H.two east] (A1) 
%     {\nodepart{one}  $\diagup$
%     \nodepart{two}  $k_1$
%     \nodepart{three}    \hphantom{$\diagup$}  
%     };
% \node[mpnh] (A2)
%     {\nodepart{one}      \hphantom{$\diagup$}
%     \nodepart{two}      $k_4$
%     \nodepart{three}    $\diagup$
%     };
% %
% \node[mpnh, right=of H.six east] (B1)
%     {\nodepart{one}      $\diagup$
%     \nodepart{two}      $k_5$
%     \nodepart{three}    \hphantom{$\diagup$}
%     };
% \node[mpnh] (B2)
%     {\nodepart{one}      \hphantom{$\diagup$}
%     \nodepart{two}      $k_2$
%     \nodepart{three}    \hphantom{$\diagup$}
%     };
% \node[mpnh] (B3)
%     {\nodepart{one}      \hphantom{$\diagup$}
%     \nodepart{two}      $k_7$
%     \nodepart{three}    $\diagup$ 
%     };
% %
% \node[mpnh, right=of H.eight east] (C1)
%     {\nodepart{one}      $\diagup$
%     \nodepart{two}      $k_3$
%     \nodepart{three}    $\diagup$
%     };
% %
% \node[mpnh, right=of H.nine east] (D1)
%     {\nodepart{one}  $\diagup$
%     \nodepart{two}  $k_8$
%     \nodepart{three}    \hphantom{$\diagup$}
%     };
% \node[mpnh] (D2)
%     {\nodepart{one}      \hphantom{$\diagup$}
%     \nodepart{two}      $k_6$
%     \nodepart{three}    $\diagup$
%     };
% %% arrows (right)
% \draw[arr]  (H |- H.two east)   edge (A1)
%             (H |- H.six east)   edge (B1)
%             (H |- H.eight east) edge (C1)
%             (H |- H.nine east)   to (D1)
%             ;
% \draw[arr, transform canvas={yshift=1mm}]  
%             (A1.three north |- A1.east)  edge (A2)
%             (B1.three north |- B1.east)  edge (B2)
%             (B2.three north |- B2.east)  edge (B3)
%             (D1.three north |- D2)   to   (D2)
%             ;
% \draw[arr, transform canvas={yshift=-1mm}]
%             (A2.one north |- A2)  edge  (A1)
%             (B2.one north |- B2)  edge  (B1)
%             (B3.one north |- B3)  edge  (B2)
%             (D2.one north |- D2)   to   (D1)
%             ;
% %% dots, ellipses
% \pgfmathsetseed{3}
% Explicitly sets the seed for
% \foreach \i in {1,2,...,8}
%     \node (k\i) [
%       dot=$k_{\i}$,
%       label={[fill=none]left:$k_\i$} 
%       ] at (-33mm +40*rand,0.9*rand) {};


% \scoped[on background layer]
% {
% \draw[fill=myblue!30]  (-4,0.4) ellipse (3 and 2.4);
% \path (-4,1.6)
%   node[label={
%      [fill=none,                % no white box
%       text opacity=1,
%       align=center,
%       inner sep=1pt]
%      above:{$U$\\(universe of keys)}  % position: text
%   }]
%   {};

% \draw[fill=myorange!30]   (-4,0) ellipse (2.4 and 1.6);
% \path   (-6.2,0) node[label={[fill=none]right:$K$\\ (actual\\ keys)}] {};

% \draw[arr]  (k1)    edge ([syp] H.two west)
%             (k4)    edge ([sym] H.two west)

%             (k2)    edge ([syp] H.six west)
%             (k5)    edge (H.six west)
%             (k7)    edge ([sym] H.six west)
            
%             (k3)    edge (H.eight west)
            
%             (k8)    edge ([syp] H.nine west)
%             (k6)    edge ([sym] H.nine west)
%             ;
% }
% \end{tikzpicture}
% \end{figure}


\[
\begin{tikzpicture}[scale=0.8, 
  every node/.style={font=\footnotesize}]

\draw[fill=white!30]  (-4,0.4) ellipse (2.6 and 2.0);
\path (-4,1.0)
  node[label={
     [fill=none,                % no white box
      text opacity=1,
      align=center,
      inner sep=1pt]
     above:{$U$\\(universe of keys)}  % position: text
  }]
  {};

\draw[fill=arraycolor]   (-4,-0.2) ellipse (2.0 and 1.2);
\path (-4,-0.2)
  node[label={
     [fill=none,                % no white box
      text opacity=1,
      align=center,
      inner sep=1pt]:{$K$\\(actual keys)}  % position: text
  }]
  {};

  % arrow between actual keys and array
\draw[->] (-1.8,0.4) -- (0.8,0.4) node[pos=0.5, above] {$h$};

% array $T$
\draw[] (2,-2) rectangle (2.8,2.8);
\node[above] at (2.4,2.8) (T) {$T$};



% put this somewhere in your preamble, after \usepackage{tikz}
\newcommand{\linkedlist}[2]{%
  \begin{scope}[shift={(#1,#2)}]

    %--- nodes ----------------------------------------------------
    \draw (0.8,-0.2) rectangle (1.6, 0.2);
    \draw[fill=arraycolor] (1.0,-0.2) rectangle (1.4, 0.2);

    \draw (2.0,-0.2) rectangle (2.8, 0.2);
    \draw[fill=arraycolor] (2.2,-0.2) rectangle (2.6, 0.2);

    \draw (4.0,-0.2) rectangle (4.8, 0.2);
    \draw[fill=arraycolor] (4.2,-0.2) rectangle (4.6, 0.2);

    %--- arrows & bullet ------------------------------------------
    \node[circle, fill, inner sep=.7pt] at (0,0) {};
    \draw[->] (0,0) -- (0.8,0);

    \draw[-{>[scale=0.6]}]   (1.5, 0.1) -- (2.0, 0.1);
    \draw[{<[scale=0.6]}-]   (1.6,-0.1) -- (2.1,-0.1);

    \draw[-]                 (2.7, 0.1) -- (3.0, 0.1);
    \draw[densely dotted]    (3.0, 0.1) -- (3.2, 0.1);
    \draw[densely dotted]    (3.0,-0.1) -- (3.2,-0.1);
    \draw[{<[scale=0.6]}-]   (2.8,-0.1) -- (3.0,-0.1);

    \draw[densely dotted]    (3.6, 0.1) -- (3.8, 0.1);
    \draw[densely dotted]    (3.6,-0.1) -- (3.8,-0.1);
    \draw[-{>[scale=0.6]}]   (3.8, 0.1) -- (4.0, 0.1);
    \draw[-]                 (3.8,-0.1) -- (4.1,-0.1);

  \end{scope}%
}

\node[left] at (2,-1.6) (T) {$i=1$};
\linkedlist{2.4}{-1.6}
\draw[-] (2.0, -1.2) -- (2.8, -1.2);
\node[left] at (2,-0.8) (T) {$i=2$};
\linkedlist{2.4}{-0.8}
\draw[-] (2.0, -0.4) -- (2.8, -0.4);
\node[left] at (2,-0.0) (T) {$i=3$};
\linkedlist{2.4}{0.0}
\draw[-] (2.0, 0.4) -- (2.8, 0.4);
\node at (2.4, 1.3) {$\vdots$};
\draw[-] (2.0, 2.0) -- (2.8, 2.0);
\node[left] at (2,2.4) (T) {$i=|T|$};
\linkedlist{2.4}{2.4}

\end{tikzpicture}
\]



\begin{algorithm}[htb]
  \caption{Chained Hash Operations}
  \label{alg:chained_hash}
  \begin{algorithmic}[1]
    \Function{ChainedHashInsert}{$T,k$}
      \State \Call{ListInsert}{$T[h(k)],k$}
    \EndFunction
    \Function{ChainedHashSearch}{$T,k$}
      \State \Return \Call{ListSearch}{$T[h(k)],k$}
    \EndFunction
    \Function{ChainedHashDelete}{$T,k$}
      \State $x \gets$ \Call{ChainedHashSearch}{$T,k$}
      \If{$x \ne \textsc{nil}$} \State \Call{ListDelete}{$x$} \EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}

We assume \emph{uniform hashing} i.e.
\[
\Prob[h(k) = i] = \frac{1}{|T|} \quad \forall i \in \{1,\dots,|T|\}
\]
So, given \(n\) distinct keys, the expected length \(n_i\) of the linked list at position \(i\) is
\[
\Exp[n_i] = \frac{n}{|T|} = \alpha
\]
If we further assume \(h(k)\) can be computed in \(O(1)\), the expected running time of \textsc{ChainedHashSearch} is
\[\boxed{
\Theta(1+\alpha)
}\]

\subsubsection{Open Addressing}

Instead of using linked lists, keys are stored directly in the array. 
On a collision we \emph{probe} other slots in \(T\) using a permutation \(h(k,1),\dots,h(k,|T|)\). 
So \(h(k,i)\) is a function of both \(k\) and \(i\), where \(i\) is the probe number.

\(h(k,\cdot)\) must be a \emph{permutation} of \(\{1,\dots,|T|\}\), i.e., \(h(k,1), \dots, h(k,|T|)\) must cover all slots in \(T\) exactly once.


We assume \emph{independent uniform permutation hashing}: the probe sequence of each key is equally likely to be any of the \textcolor{red}{\(|T|!\)} permutations of \(\{1,\dots,|T|\}\).
Independent uniform permutation hashing generalizes the notion of independent uniform hashing introduced earlier to a hash function that produces not just a single slot number, but a whole probe sequence. 
True independent uniform permutation hashing is difficult to implement, however, and in practice suitable approximations (such as double hashing,  Example~\ref{ex:double_hashing}) are used.

Neither double hashing nor its special case, linear probing, meets the assumption of independent uniform permutation hashing. 
Double hashing cannot generate more than \textcolor{red}{\(|T|^2\)} different probe sequences.
Nonetheless, double hashing has a large number of possible probe sequences and seems to give good results. 
Linear probing is even more restricted, capable of generating only \textcolor{red}{\(|T|\)} different probe sequences.

\begin{example}[Double Hashing]
  \label{ex:double_hashing}
  Double hashing offers one of the best methods available for open addressing because the permutations produced have many of the characteristics of randomly chosen permutations. 
  Double hashing uses a hash function of the form
  \begin{equation}
    \label{eq:double_hashing}
  h(k,i) = (h_1(k) + i \cdot h_2(k)) \bmod |T|
  \end{equation}
  where \(h_1\) and \(h_2\) are two different hash functions.
  The second hash function \(h_2(k)\) must be \emph{relatively prime} to \(|T|\) (i.e., \(\gcd(h_2(k),|T|)=1\)) to ensure that all slots in \(T\) are probed.
  A convenient way to achieve this is to let \(|T|\) be an exact power of \(2\) and to design \(h_2(k)\) so that it always produces an odd number.
  Another way is to let \(|T|\) be prime and to design \(h_2(k)\) so that it always returns a positive integer less than \(|T|\).
  In~\eqref{eq:double_hashing} we can also set \(h_2(k) = 1\) for all \(k\), in which case we get \emph{linear probing}, a special case of double hashing.
\end{example}

\begin{algorithm}[htb]
  \caption{Open-Address Hash Insert (generic probing)}
  \label{alg:open_address_insert}
  \begin{algorithmic}[1]
    \Function{HashInsert}{$T,k$}
      \For{$i=1 \TO |T|$}
        \State $j \gets h(k,i)$
        \If{$T[j]=\textsc{nil}$}
          \State $T[j] \gets k$
          \State \Return $j$
        \EndIf
      \EndFor
      \State \textbf{error} ``overflow''
    \EndFunction
  \end{algorithmic}
\end{algorithm}

To analyze the time complexity of Algorithm~\ref{alg:open_address_insert}, we assume independent uniform permutation hashing for the hash function.
We also assume that at least one slot is empty, i.e., \(\alpha < 1\).
Because deleting from an open-address hash table does not really free up a slot, we assume as well that no deletions occur.

If we denote by \(X\) the number of probes performed until an empty slot is found, then on each probe we hit an occupied slot with probability \(\alpha = n/|T|\) and an empty slot with probability \(1-\alpha\).  
Hence
\[
\Prob[X=i]= \alpha^{i-1}(1-\alpha), \quad i=1,2,\dots
\]
so that \(X\) is geometrically distributed with success probability \(1-\alpha\).
Hence
\[
\Exp[X]=\sum_{i=1}^{\infty} i\alpha^{i-1}(1-\alpha)=\frac{1}{1-\alpha}
\]
Thus an insertion (or an unsuccessful search) requires on average \(\tfrac{1}{1-\alpha}\) probes; this grows rapidly as the load factor \(\alpha\) approaches \(1\).

% \[
% \Prob[X>i] = \alpha^{i}, \quad 0 \leq i < |T|
% \]
% and the expectation is the tail-sum of this geometric distribution:
% \[
% \Exp[X]
%    =
%    \sum_{i=0}^{\infty}\Prob[X>i]
%    \leq
%    \sum_{i=0}^{\infty}\alpha^{i}
%    =
%    \frac{1}{1-\alpha}
% \]
% Thus an insertion (or any \emph{unsuccessful} search) in an open-address hash table takes
% \[
% \boxed{\Exp[X]=\dfrac{1}{1-\alpha}}
% \]
% probes on average, which grows sharply as the table fills up and \(\alpha\) approaches 1.


% %--------------------------------------------------------------------
% \subsection{Complexity Summary}
% \label{subsec:ds_complexity}

% \begin{center}
% \begin{tabularx}{\linewidth}{l *{3}{>{\centering\arraybackslash}X} c}
% \toprule
% \textbf{Structure / Operation} &
% \textbf{Insert} & \textbf{Delete} & \textbf{Search} &
% \textbf{Notes}\\
% \midrule
% Stack & \(O(1)\) & \(O(1)\) & \(O(1)\) & array implementation \\
% Queue & \(O(1)\) & \(O(1)\) & \(O(1)\) & circular array \\
% Linked List & \(O(1)\) & \(O(1)\) & \(\Theta(n)\) & sentinel node \\
% Direct-Address & \(O(1)\) & \(O(1)\) & \(O(1)\) & space \(\Theta(|U|)\) \\
% Hash (chaining) & \(O(1)\) & \(O(1)\) & \(\Theta(1+\alpha)\) & \(\alpha = n/m\) \\
% Hash (open addr.) & \(O\!\bigl(\tfrac{1}{1-\alpha}\bigr)\) &
% \(O\!\bigl(\tfrac{1}{1-\alpha}\bigr)\) &
% \(O\!\bigl(\tfrac{1}{1-\alpha}\bigr)\) &
% \(\alpha<1\) \\
% \bottomrule
% \end{tabularx}
% \end{center}

% \begin{example}
% Hash tables with chaining are usually preferable for applications that
% require a dynamically growing dictionary, whereas open addressing is
% often faster in memory-constrained settings as long as a good probe
% sequence is employed.
% \end{example}


%--------------------------------------------------------------------
\subsection{Binary Search Trees}
\label{subsec:bst}

\begin{definition}[Binary Search Tree]
A \emph{binary search tree} implements a dynamic set. It stores keys from a totally ordered domain in nodes linked by \emph{left} and \emph{right} child pointers such that for every node \(x\)
\[
y\in\text{left-subtree}(x) \Leftrightarrow  \attrib{y}{key}\le\attrib{x}{key}
,\quad
z\in\text{right-subtree}(x)\Leftrightarrow  \attrib{z}{key}\ge\attrib{x}{key}%\qedhere
\]
\end{definition}

% Interface:
% \begin{itemize}%[before={\parskip=0pt},nosep]
%   \item \textsc{TreeInsert}\((T,k)\), \textsc{TreeDelete}\((T,k)\), \textsc{TreeSearch}\((T,k)\)
%   \item Traversals: \textsc{InOrder}-, \textsc{Preorder}-, \textsc{Postorder}-, \textsc{ReverseOrderTreeWalk}
%   \item Extremal queries: \textsc{TreeMinimum}, \textsc{TreeMaximum}
%   \item Iteration: \textsc{TreeSuccessor}, \textsc{TreePredecessor}
% \end{itemize}




\begin{example}[Lower Bound]
  Let $t$ be the root of a binary search tree that represents a set $S$ of numbers. 
  % So, all the values in the tree are distinct. 
  The size of the tree is $|S|=n$. 
  The height of the tree is $h$. Algorithm~\ref{alg:bst-lower-bound} returns the node containing the least element $y \in S$ such that $x \leq y$, or \textsc{nil} if no such element exists.
  
  \begin{algorithm}[htb]
    \caption{Lower Bound for Binary Search Tree}
    \label{alg:bst-lower-bound}
    \begin{algorithmic}[1]
      \Function{LowerBound}{$t, x$}
      \If{$t = \textsc{nil}$} \Comment{base case}
      \State \Return \textsc{nil}
      \EndIf
      % \If{$t.\text{key} = x$}
      % \State \Return $t$
      % \ElsIf{$t.\text{key} < x$}
      \If{$t.\text{key} < x$}
      \State \Return \Call{LowerBound}{$t.\text{right}, x$}
      \Else
      \State $y \gets $ \Call{LowerBound}{$t.\text{left}, x$}
      \If{$y \neq \textsc{nil}$}
      \State \Return $y$
      \Else
      \State \Return $t$
      \EndIf
      \EndIf
      \EndFunction
    \end{algorithmic}
  \end{algorithm}
  \FloatBarrier
  The complexity is $O(h)$, where $h$ is the height of the tree. 
  \end{example}
  

%--------------------------------------------------------------------
\subsubsection{Traversals}
\begin{algorithm}[htb]
  \caption{Inorder Tree Walk (recursive)}
  \label{alg:bst_walk}
  \begin{algorithmic}[1]
    \Function{TreeWalk}{$x$}
      \If{$x \ne \textsc{nil}$}
        \State \Call{TreeWalk}{$\attrib{x}{left}$} \label{alg:bst_walk:line:call_left}
        \State print \(\attrib{x}{key}\) \label{alg:bst_walk:line:print}
        \State \Call{TreeWalk}{$\attrib{x}{right}$} \label{alg:bst_walk:line:call_right}
      \EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:bst_walk} shows the \textsc{InOrderTreeWalk} algorithm.
Three other variants can be obtained from Algorithm~\ref{alg:bst_walk} by swapping the order of the recursive calls and the print statement (lines \ref{alg:bst_walk:line:call_left}, \ref{alg:bst_walk:line:print}, and \ref{alg:bst_walk:line:call_right}).
For \textsc{PreOrderTreeWalk}, we swap lines \ref{alg:bst_walk:line:call_left} and \ref{alg:bst_walk:line:print}.
For \textsc{PostOrderTreeWalk}, we swap lines \ref{alg:bst_walk:line:print} and \ref{alg:bst_walk:line:call_right}.
For \textsc{ReverseOrderTreeWalk}, we swap lines \ref{alg:bst_walk:line:call_left} and \ref{alg:bst_walk:line:call_right}.

The general recurrence is 
\[
T(n) = T(n_L) + T(n - n_L - 1) + \Theta(1)
\]
and all four variants run in \(\Theta(n)\) time. 
This can be proven using the \emph{substitution method}.
Can we do better? No, because the length of the output is \(\Theta(n)\).

%--------------------------------------------------------------------
\subsubsection{Basic Queries}

\begin{algorithm}[htb]
  \caption{Searching a Binary Search Tree for a Key}
  \label{alg:bst_search}
  \begin{algorithmic}[1]
    \Function{TreeSearch}{$x,k$}
      \If{$x=\textsc{nil} \OR k=\attrib{x}{key}$} 
        \State \Return $x$ 
      \EndIf
      \If{$k<\attrib{x}{key}$}
        \State \Return \Call{TreeSearch}{$\attrib{x}{left},k$}
      \Else
        \State \Return \Call{TreeSearch}{$\attrib{x}{right},k$}
      \EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}


\begin{algorithm}[htb]
  \caption{Minimum of a Binary Search Tree}
  \label{alg:bst_min}
  \begin{algorithmic}[1]
    \Function{TreeMinimum}{$x$}
      \While{$\attrib{x}{left}\ne\textsc{nil}$} 
      \State $x\gets\attrib{x}{left}$ 
      \EndWhile
      \State \Return $x$
    \EndFunction
    % \Function{TreeMaximum}{$x$}
    %   \While{$\attrib{x}{right}\ne\textsc{nil}$} $x\gets\attrib{x}{right}$ \EndWhile
    %   \State \Return $x$
    % \EndFunction
  \end{algorithmic}
\end{algorithm}
To find the maximum, replace all occurences of \attribute{left} with \attribute{right} in Algorithm~\ref{alg:bst_min}.

\begin{definition}[Successor]
  The \emph{successor} of a node \(x\) is the minimum of the right subtree of \(x\) if it exists, otherwise it is the lowest ancestor \(a\) of \(x\) such that \(x\) falls in the left subtree of \(a\).
\end{definition}

\begin{algorithm}[htb]
  \caption{Successor of a Node in a Binary Search Tree}
  \label{alg:bst_successor}
  \begin{algorithmic}[1]
    \Function{TreeSuccessor}{$x$}
      \If{$\attrib{x}{right}\ne\textsc{nil}$}
        \State \Return \Call{TreeMinimum}{$\attrib{x}{right}$} \Comment{leftmost node in right subtree}
      \EndIf
      \While{$\attrib{x}{parent} \ne \textsc{nil} \AND \attrib{\attrib{x}{parent}}{right} = x$}
        \State $x\gets \attrib{x}{parent}$ 
      \EndWhile
      \State \Return $\attrib{x}{parent}$ \Comment{lowest ancestor of \(x\) whose left child is an ancestor of \(x\)}
    \EndFunction
  \end{algorithmic}
\end{algorithm}
To find the predecessor, replace all occurences of \attribute{right} with \attribute{left} and use \textsc{TreeMaximum} instead of \textsc{TreeMinimum} in Algorithm~\ref{alg:bst_successor}.



%--------------------------------------------------------------------
\subsubsection{Updates}

\begin{algorithm}[htb]
  \caption{Inserting a Node into a Binary Search Tree}
  \label{alg:bst_insert}
  \begin{algorithmic}[1]
    \Function{TreeInsert}{$T,z$}
      \State $y, x \gets \textsc{nil}, \attrib{T}{root}$
      \While{$x\ne\textsc{nil}$}
        \State $y\gets x$
        \If{$\attrib{z}{key} < \attrib{x}{key}$}
          \State $x\gets\attrib{x}{left}$
        \Else
          \State $x\gets\attrib{x}{right}$
        \EndIf
      \EndWhile
      \State $\attrib{z}{parent}\gets y$
      \If{$y=\textsc{nil}$}
        \State $\attrib{T}{root}\gets z$
      \ElsIf{$\attrib{z}{key}<\attrib{y}{key}$}
        \State $\attrib{y}{left}\gets z$
      \Else
        \State $\attrib{y}{right}\gets z$
      \EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}


Deletion distinguishes three cases:
\begin{itemize}%[before={\parskip=0pt},nosep]
  \item \(z\) is a leaf node, i.e. it has no children:
    \begin{itemize}%[before={\parskip=0pt},nosep]
      \item simply remove the node \(z\)
    \end{itemize}
  \item \(z\) has one child:
    \begin{itemize}%[before={\parskip=0pt},nosep]
      \item remove \(z\)
      \item connect its child to its parent
    \end{itemize}
  \item \(z\) has two children:
    \begin{itemize}%[before={\parskip=0pt},nosep]
      \item find the successor \(y\) of \(z\) (since \(z\) has two children, \(y\) is guaranteed to be the minimum of the right subtree of \(z\) and thus have at most one child)
      \item copy the key of \(y\) into \(z\)\nopagebreak
      \item delete \(y\), which is a leaf or has one right child, i.e. connect the child of \(y\) to the parent of \(y\)
    \end{itemize}
\end{itemize}

\begin{algorithm}[htb]
\caption{Deleting a Node from a Binary Search Tree}
\label{alg:bst_delete}
\begin{algorithmic}[1]
  \Function{TreeDelete}{$T,z$}
  \LComment{\(z\) has two children: find successor, copy its key, then delete successor}
    \If{$\attrib{z}{left}\neq\textsc{nil} \AND \attrib{z}{right}\neq\textsc{nil}$}
      \State $s \gets \Call{TreeMinimum}{\attrib{z}{right}}$
      \State $\attrib{z}{key}\gets \attrib{s}{key}$ \Comment{replace the key in \(z\) with the one from the successor}
      \State \Return \Call{TreeDelete}{$T,s$} 
    \EndIf
    \LComment{\(z\) has at most one child: pick it (could be \(\textsc{nil}\))} 
    \If{$\attrib{z}{left}\neq\textsc{nil}$} %\Comment{\(z\) has one child}
      \State $c\gets \attrib{z}{left}$
    \Else
      \State $c\gets \attrib{z}{right}$
    \EndIf
    \If{$c\neq\textsc{nil}$} \Comment{if child exists, update its parent pointer}
      \State $\attrib{c}{parent}\gets \attrib{z}{parent}$
    \EndIf
    \If{$\attrib{z}{parent}=\textsc{nil}$} \Comment{if \(z\) was the root, make child the new root}
      \State $\attrib{T}{root}\gets c$
    \Else \Comment{otherwise, bypass \(z\) by connecting its child to its parent}
      \If{$z = \attrib{\attrib{z}{parent}}{left}$}
        \State $\attrib{\attrib{z}{parent}}{left}\gets c$
      \Else
        \State $\attrib{\attrib{z}{parent}}{right}\gets c$
      \EndIf
    \EndIf
    \State \Return $T$
  \EndFunction
\end{algorithmic}
% to be continued ...
  % \begin{algorithmic}[1]
  %   \Function{TreeDelete}{$T,z$}
  %     \LComment{\(z\) has two children: find successor, copy its key, then delete successor}
  %     \If{$\attrib{z}{left}\neq\textsc{nil}\;\AND\;\attrib{z}{right}\neq\textsc{nil}$}
  %       \State $s\gets \Call{TreeMinimum}{\attrib{z}{right}}$
  %       \LComment{instead of removing the node, replace \(z\)'s key with successor's}
  %       \State $\attrib{z}{key}\gets \attrib{s}{key}$
  %       \State \Return \Call{TreeDelete}{$T,s$}
  %     \EndIf
  %     \LComment{\(z\) has at most one child: pick it (could be \(\textsc{nil}\))} 
  %     \If{$\attrib{z}{left}\neq\textsc{nil}$}
  %       \State $c\gets \attrib{z}{left}$
  %     \Else
  %       \State $c\gets \attrib{z}{right}$
  %     \EndIf
  %     \LComment{if child exists, update its parent pointer}
  %     \If{$c\neq\textsc{nil}$}
  %       \State $\attrib{c}{parent}\gets \attrib{z}{parent}$
  %     \EndIf
  %     \LComment{if \(z\) was the root, make child the new root}
  %     \If{$\attrib{z}{parent}=\textsc{nil}$}
  %       \State $\attrib{T}{root}\gets c$
  %     \Else
  %       \LComment{otherwise, bypass \(z\) by updating the correct child link of \(z\)'s parent}
  %       \If{$z = \attrib{(\attrib{z}{parent})}{left}$}
  %         \State $\attrib{(\attrib{z}{parent})}{left}\gets c$
  %       \Else
  %         \State $\attrib{(\attrib{z}{parent})}{right}\gets c$
  %       \EndIf
  %     \EndIf
  %     \State \Return $T$
  %   \EndFunction
  % \end{algorithmic}
\end{algorithm}


Insertion, search and deletion operations have complexity \(\Theta(h)\) where \(h\) is the height of the tree.
In the average case, the height is \(O(\log n)\) (i.e. with a random insertion order).
In some particular cases, the height can be \(O(n)\) (i.e. with ordered sequence).

The problem is that the `worst case' is not that uncommon.
One way to avoid this is to instead of inserting \(A = [a_1, a_2, a_3, \ldots, a_n]\) in order, insert a random permutation of \(A\).
The problem is that \(A\) is usually not known in advance.
It is the application that calls the insertion procedure.
But we can also obtain a random permutation of \(A\) by using a randomized insertion algorithm (see \ref{subsubsec:bst_rand_insert}).

% %--------------------------------------------------------------------
\subsubsection{Randomized Insertion}
\label{subsubsec:bst_rand_insert}
In order to avoid the linear-height worst case one can insert into a tree using Algorithm~\ref{alg:bst_rand_insert}.
The idea behind the function \textsc{TreeRandomizedInsert} is as follows. 
We insert a new node \(z\) into the tree \(t\) as the new root of \(t\) with probability \(1/(\attrib{t}{size}+1)\).
If \(z\) is not inserted as the new root, we recursively insert it into the appropriate subtree of \(t\).
The additional attribute \(\attrib{t}{size}\) represents the number of nodes in the subtree rooted at \(t\).

\begin{algorithm}[htb]
  \caption{Randomized Insertion into a Binary Search Tree}
  \label{alg:bst_rand_insert}
  \begin{algorithmic}[1]
    \Function{TreeRandomizedInsert}{$t,z$}
      \If {$t=\textsc{nil}$}
        \State \Return $z$
      \EndIf
      \State $r \gets$ \Call{randint}{$1,\attrib{t}{size}+1$} 
      \If{$r=1$} \Comment{$\Prob(r=1)=\Prob(z\text{ is inserted as the new root of }t) = \frac{1}{\attrib{t}{size}+1}$}
        \State $\attrib{z}{size}\gets\attrib{t}{size}+1$
        \State \Return \Call{TreeRootInsert}{$t,z$} \Comment{see \ref{subsubsec:bst_root_insert}}
      \EndIf
      \If{$\attrib{z}{key}<\attrib{t}{key}$}
        \State $\attrib{t}{left}\gets$ \Call{TreeRandomizedInsert}{$\attrib{t}{left},z$}
      \Else
        \State $\attrib{t}{right}\gets$ \Call{TreeRandomizedInsert}{$\attrib{t}{right},z$}
      \EndIf
      \State $\attrib{t}{size}\gets\attrib{t}{size}+1$
      \State \Return $t$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

With Algorithm~\ref{alg:bst_rand_insert} every insertion order is equally likely; the expected height of the tree is \(O(\log n)\) and all operations run in expected \(O(\log n)\) time.


%--------------------------------------------------------------------
\subsubsection{Rotations}
\begin{definition}[Rotation]
A \emph{rotation} is a local restructuring of a BST that exchanges the relative position of a node \(x\) and one of its children while preserving the in-order sequence of keys.
% \begin{itemize}[noitemsep]
%   \item \emph{Left rotation} at \(x\): \(x\) moves to the left of its right child.
%   \item \emph{Right rotation} at \(x\): \(x\) moves to the right of its left child.
% \end{itemize}
Rotations do \emph{not} change the in-order ordering of the keys and they are the basic tool used by self-balancing trees.
\end{definition}


\begin{center}
\begin{tikzpicture}[%
shorten >=0pt,
font=\footnotesize,
every path/.style={line cap=round},
root/.style  ={circle,fill=black,minimum size=2pt,inner sep=0pt},
node/.style  ={circle,draw,minimum size=16,inner sep=0pt, outer sep=0pt},
thickedge/.style   ={thick,->},
lab/.style   ={anchor=west,font=\footnotesize},
rng/.style   ={font=\footnotesize},
scale=0.7
]
\newcommand{\xdistance}{1.8}
\newcommand{\ydistance}{1.4}
\newcommand{\trianglenode}[5]{%
\node[isosceles triangle,
isosceles triangle apex angle=47,
      shape border rotate=90,
      minimum width=8mm,
      minimum height=8mm,
      draw,
      fill=#4!25,
      inner sep=0pt,
      outer sep=0pt,
      below,
      font=\footnotesize] (#1) at (#5) {};
\node at ($(#1.apex)!1.1!(#1.base)$) {$#2$};
  \node[below] at ($(#1.lower side)$) {$#3$};
}

% left  tree
\begin{scope}[shift={(0,0)}, local bounding box=T2, scale=0.75]

\node[root] (x2) {};
\node[lab,left=0pt of x2] {$x$};

\node[node,below right=1mm and 6mm of x2] (b2) {$b$};
\node[node,below left=6mm and 10mm of b2] (a2) {$a$};

\node[root,above left=1mm and 6mm of a2] (l) {};
\node[lab,left=0pt of l] {$l$};

\draw[->,bend left=20] (x2) to (b2);
\draw[->,bend left=20] (l) to (a2);
\draw[thickedge] (b2) to (a2);

\coordinate (alpha2) at ($(a2) + (-\xdistance,-\ydistance)$);
\coordinate (beta2)  at ($(a2) + (\xdistance,-\ydistance)$);
\coordinate (gamma2) at ($(b2) + (\xdistance,-\ydistance)$);

\draw[thick] (a2) -- (alpha2) (a2) -- (beta2) (b2) -- (gamma2);

\trianglenode{alpha2}{\alpha}{k\le a}{gray!75!white}{alpha2}
\trianglenode{beta2}{\beta}{a\le k\le b}{gray!75!white}{beta2}
\trianglenode{gamma2}{\gamma}{k\ge b}{gray!75!white}{gamma2}

\end{scope}

% right tree
\begin{scope}[shift={(8,0)}, local bounding box=T1, scale=0.75]

\node[root] (x1) {};
\node[lab,left=0pt of x1] {$x$};

\node[node,below right=1mm and 6mm of x1] (a1) {$a$};
\node[node,below right=6mm and 10mm of a1] (b1) {$b$};

\node[root,above right=1mm and 6mm of b1] (r) {};
\node[lab,right=0pt of r] {$r$};

\draw[->,bend left=20] (x1) to (a1);
\draw[->,bend right=20] (r) to (b1);
\draw[thickedge] (a1) to (b1);

\coordinate (alpha1) at ($(a1) + (-\xdistance,-\ydistance)$);
\coordinate (beta1)  at ($(b1) + (-\xdistance,-\ydistance)$);
\coordinate (gamma1) at ($(b1) + (\xdistance,-\ydistance)$);

\draw[thick] (a1) -- (alpha1) (b1) -- (beta1) (b1) -- (gamma1);

\trianglenode{alpha1}{\alpha}{k\le a}{gray!75!white}{alpha1}
\trianglenode{beta1}{\beta}{a\le k\le b}{gray!75!white}{beta1}
\trianglenode{gamma1}{\gamma}{k\ge b}{gray!75!white}{gamma1}
\end{scope}

% arrows between the two trees
\draw[draw=black,very thick,->,bend left=10, transform canvas={yshift=-4pt}] ($(T1.west)+(-0.5,0)$) to node[midway,below] {\textsc{\hyperref[alg:line:leftrotate]{LeftRotate}}} ($(T2.east)+(0.5,0)$);
\draw[draw=black,very thick,->,bend left=10, transform canvas={yshift=4pt}] ($(T2.east)+(0.5,0)$) to node[midway,above]  {\textsc{\hyperref[alg:line:rightrotate]{RightRotate}}} ($(T1.west)+(-0.5,0)$);

\end{tikzpicture}
\end{center}
  
\begin{algorithm}[htb]
  \caption{Rotations in a Binary Search Tree}
  \label{alg:bst_rotations}
  \begin{algorithmic}[1]
    \Function{RightRotate}{$x$} \label{alg:line:rightrotate}
      \State $l \gets \attrib{x}{left}$
      \State $\attrib{x}{left} \gets \attrib{l}{right}$
      \State $\attrib{l}{right} \gets x$
      \State \Return $l$
    \EndFunction
    \Function{LeftRotate}{$x$} \label{alg:line:leftrotate}
      \State $r \gets \attrib{x}{right}$
      \State $\attrib{x}{right} \gets \attrib{r}{left}$
      \State $\attrib{r}{left} \gets x$
      \State \Return $r$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Rotations are very useful operations.
For example, we can use it to perform

\subsubsection{Root Insertion}
\label{subsubsec:bst_root_insert}
\begin{algorithm}[htb]
  \caption{Root Insertion into a Binary Search Tree}
  \label{alg:bst_root_insert}
  \begin{algorithmic}[1]
    \Function{TreeRootInsert}{$x,z$}
      \If{$x=\textsc{nil}$}
        \State \Return $z$
      \EndIf
      \If{$\attrib{z}{key}<\attrib{x}{key}$}
        \State $\attrib{x}{left}\gets$ \Call{TreeRootInsert}{$\attrib{x}{left},z$}
        \State \Return \Call{RightRotate}{$x$}
      \Else
        \State $\attrib{x}{right}\gets$ \Call{TreeRootInsert}{$\attrib{x}{right},z$}
        \State \Return \Call{LeftRotate}{$x$}
      \EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}


General strategies to deal with complexity in the worst case:
\begin{itemize}%[before={\parskip=0pt},nosep]
  \item \emph{Randomization}: can make the scenario $h = n$ highly unlikely
  \item \emph{Amortized Maintenance}: relatively expensive but `amortized' operations
  \item \emph{Self-Balancing}: e.g. Red-Black trees (see \ref{subsec:rb_tree})
\end{itemize}



\subsection{Red-Black Trees}
\label{subsec:rb_tree}
A \emph{red-black tree} is a binary search tree with one extra bit of storage per node:
its \attribute{color}, which can be either $\red$ or $\black$.
By constraining the node colors on any simple path from the root to a leaf, red-black trees ensure that no such path is more than twice as long as any other, so that the tree is approximately balanced.
\begin{definition}[Red-Black Tree]
  \label{def:rb_tree_properties}
  A red-black tree is a binary search tree that satisfies the following \emph{red-black properties}:
  \begin{enumerate}%[before={\parskip=0pt},nosep]
    \item Every node is either red or black. \label{def:rb_tree_properties:color}
    \item The root is black. \label{def:rb_tree_properties:root_isblack}
    \item Every leaf ($\nil$) is black. \label{def:rb_tree_properties:leafs_areblack}
    \item If a node is red, then both its children are black. \label{def:rb_tree_properties:redparent_blackchildren}
    \item For each node, all simple paths from the node to descendant leaves contain the same number of black nodes.\label{def:rb_tree_properties:same_blackheight} \qedhere
  \end{enumerate}
\end{definition}
\begin{definition}[Black-Height]
The number of black nodes on any simple path from, but \underline{not} including, a node $x$ down to, and including, a leaf is called the \emph{black-height} of the node $x$:
\[
\blackheight(x) \coloneqq \numof \text{black nodes on path from (excluding) } x \text{ to (including) leafs}
\]
By property \ref{def:rb_tree_properties:same_blackheight}, the notion of black-height is well defined, since all descending paths have the same number of black nodes.
\end{definition}

\begin{lemma}[Height of a Red-Black Tree]
  \label{lem:rb_tree_height}
  The height \(h(x)\) of a red-black tree with \(n = \operatorname{size}(x)\) internal nodes is at most \(2\log(n+1)\).
  % A red-black tree with \(n=\operatorname{size}(x)\) internal nodes has height \(h(x)\) that is at most \(2\log(n+1)\).
\end{lemma}
\begin{proof}
  First, we prove (by induction) that the subtree rooted at any node \(x\) contains at least 
  \begin{equation}
    \label{eq:rb_tree_height:num_nodes_from_blackheight}
    2^{\operatorname{bh}(x)} - 1
  \end{equation}
  internal nodes.
  \begin{enumerate}[partopsep=0em, topsep=0em, label=(\roman*)]
    \item base case: \(x\) is a leaf, so \(\operatorname{size}(x)=0\) and \(\operatorname{bh}(x)=0\), fulfilling \eqref{eq:rb_tree_height:num_nodes_from_blackheight} \textcolor{Green}{\ding{52}}
    \item induction step: consider \(x, y_1, y_2\) such that \(x = \attrib{y_1}{parent} = \attrib{y_2}{parent}\)
    \[
      \operatorname{size}(x) = \operatorname{size}(y_1) + \operatorname{size}(y_2) + 1 \geq (2^{\operatorname{bh}(y_1)} - 1) + (2^{\operatorname{bh}(y_2)} - 1) + 1
    \]
    By rule \ref{def:rb_tree_properties:same_blackheight}, both children must have the same black-height.
    Let \(\operatorname{bh}(y) = \operatorname{bh}(y_1) = \operatorname{bh}(y_2)\). Then 
    \[
      \operatorname{size}(x) \geq 2 \cdot (2^{\operatorname{bh}(y)} - 1) + 1 = 2^{\operatorname{bh}(y) + 1} - 1
    \]
    The black-height of a child \(y\) differs at most by one from the black-height of the parent \(x\), i.e. \(\operatorname{bh}(y)  \in \{\operatorname{bh}(x), \operatorname{bh}(x) - 1\}\).
    In both cases, we have \(\operatorname{size}(x) \geq 2^{\operatorname{bh}(x)} - 1\), fulfilling \eqref{eq:rb_tree_height:num_nodes_from_blackheight} \textcolor{Green}{\ding{52}}
\end{enumerate}
By property \ref{def:rb_tree_properties:redparent_blackchildren}, the black-height of a node \(x\) is at least half the height of the tree, i.e. \(\operatorname{bh}(x) \geq \frac{h(x)}{2}\).
Therefore,
\[
 n = \operatorname{size}(x) \geq 2^{\operatorname{bh}(x)} - 1 \geq 2^{\frac{h(x)}{2}} - 1
\]
which can be rearranged to
\[
  h(x) \leq 2\log(n+1) \qedhere
\]
\end{proof}

  % \begin{proof}
    % Let $h$ be the (ordinary) height of the red-black tree and
    % let $\operatorname{bh}(x)$ denote the black-height of a node $x$.
    % We proceed in two steps.
    % \begin{enumerate}[leftmargin=*]
    %   \item 
    %   \label{step:rb_tree_height:num_nodes_from_blackheight}
    %   A subtree with black-height $k$ stores at least $2^{k}-1$ internal nodes.
    %   We prove the claim by induction on $k$.
    %   \begin{itemize}[leftmargin=*, partopsep=0em, topsep=0em]
    %   \item Base case: $k=0$.
    %     A node of black-height 0 is a $\nil$ leaf (Property \ref{def:rb_tree_properties:leafs_areblack}).
    %     It contributes no internal nodes, so the bound $2^{0}-1=0$ is tight.
    %   \item Induction step: $k\ge 1$.
    %     Assume every node of black-height $k-1$ roots a subtree
    %     with at least $2^{k-1}-1$ internal nodes.  
    %     Let $x$ be a node with $\operatorname{bh}(x)=k$.
    %     \begin{enumerate}[label=(\alph*), leftmargin=*, partopsep=0em, topsep=0em]
    %       \item Either $x$ is black.  
    %             Then \(\operatorname{bh}(\text{left}(x))=\operatorname{bh}(\text{right}(x))=k-1\).
    %       \item Or $x$ is red.  
    %             Property \ref{def:rb_tree_properties:redparent_blackchildren} forces both children to be black, so again each child has black-height $k-1$.
    %     \end{enumerate}
    %     Hence {\em both} children satisfy the inductive hypothesis.  
    %     Counting $x$ itself we get
    %     \[
    %       \# \text{internal nodes in }T(x)
    %       \;\;\ge\;\;
    %       1 + 2\bigl(2^{k-1}-1\bigr)
    %       \;=\;
    %       2^{k}-1
    %     \]
    %   \end{itemize}
    %   Thus every node of black-height $k$ heads a subtree with at least $2^{k}-1$ internal nodes.

    %   \item
    %   \label{step:rb_tree_height:height_from_blackheight}
    %   Now we want to relate the black-height to ordinary height.
    %   Walk from the root to any leaf.  
    %   Consecutive red nodes are forbidden (Property \ref{def:rb_tree_properties:redparent_blackchildren}),
    %   so along every root-leaf path \emph{at most every other} node can be red.
    %   Therefore
    %   \[
    %         \operatorname{bh}(\text{root}) \;\ge\; \frac{h}{2}
    %   \]
    %   \end{enumerate}
    
    %   Let $n$ be the total number of internal nodes.
    %   By Step \ref{step:rb_tree_height:num_nodes_from_blackheight} (applied to the root),
    %   \[
    %     n \;\;\ge\;\; 2^{\operatorname{bh}(\text{root})}-1
    %     \quad\Longrightarrow\quad
    %     \operatorname{bh}(\text{root}) \;\le\; \log_2(n+1)
    %   \]
    %   Combining with the inequality from Step \ref{step:rb_tree_height:height_from_blackheight} we get
    %   \[
    %       h \;\le\; 2\,\operatorname{bh}(\text{root})
    %       \;\le\; 2\log_2(n+1)
    %   \]
    %   Hence a red-black tree with $n$ internal nodes has height at most $2\log_2(n+1)$. 
    % \end{proof}




%\clearpage

\section{Graphs}\label{sec:graphs}
\subsection{Representations of Graphs}
There are two standard ways to represent a graph: as a collection of adjacency lists or as an adjacency matrix.
Because the adjacency-list representation provides a compact way to represent \emph{sparse} graphs (those for which \(|E| \ll |V|^2\)), it is usually the method of choice.
The adjacency-matrix representation might be preferred when the graph is \emph{dense} (i.e., \(|E| \approx |V|^2\)), or you need to be able to tell quickly wether there is an edge connecting two given vertices.

The space required for the adjacency-list representation is \(\Theta(|V| + |E|)\), and finding each edge in the graph also takes \(\Theta(|V| + |E|)\) time, since each of the \(|V|\) adjacency lists must be examined.

The space required for the adjacency-matrix representation is \(\Theta(|V|^2)\), and finding each edge in the graph takes \(\Theta(|V|^2)\) time, since the entire adjacency matrix must be examined.
For an undirected graph, the adjacency matrix is symmetric.%, i.e. $\matr{A} = \matr{A}^T$.

The complexity for different operations is summarized in the following table.


% \newcommand{\mixcolor}{70}

% \definecolor{Red}{rgb}{1.0, 0.0, 0.0}
% \definecolor{Orange}{rgb}{1.0, 0.5, 0.0}
% \definecolor{Yellow}{rgb}{1.0, 1.0, 0.0}
% \definecolor{Green}{rgb}{0.0, 1.0, 0.0}
% \definecolor{BlueGreen}{rgb}{0.0, 1.0, 0.5}
% \definecolor{Blue}{rgb}{0.0, 1.0, 1.0}


% \begin{center}
%     \begin{tabular}{l c c c c c}
%         \toprule
%         \textbf{Algorithm} & \multicolumn{3}{c}{\textbf{Time Complexity}} & \textbf{in place?} \\%& \textbf{Space}  \\
%                             & \emph{worst}      & \emph{average}       & \emph{best}  & \\%& \emph{worst} \\
%         \midrule
%         \textsc{InsertionSort} & \cellcolor{Red!\mixcolor}$\Theta(n^2)$    & \cellcolor{Red!\mixcolor}$\Theta(n^2)$        & \cellcolor{Green!\mixcolor}$\Theta(n)$   & \ding{52} \\%& \cellcolor{Blue!\mixcolor}$\mathcal{O}(1)$ \\
%         \textsc{SelectionSort} & \cellcolor{Red!\mixcolor}$\Theta(n^2)$    & \cellcolor{Red!\mixcolor}$\Theta(n^2)$        & \cellcolor{Red!\mixcolor}$\Theta(n^2)$ & \ding{52} \\%& \cellcolor{Blue!\mixcolor}$\mathcal{O}(1)$ \\
%         \textsc{MergeSort}     & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$ & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$     & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$ & \ding{56} \\%& \cellcolor{Green!\mixcolor}$\mathcal{O}(n)$ \\
%         \textsc{QuickSort}     & \cellcolor{Red!\mixcolor}$\Theta(n^2)$    & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$     & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$ & \ding{52} \\%& \cellcolor{BlueGreen!\mixcolor}$\mathcal{O}(\log n)$ \\
%         \textsc{HeapSort}      & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$ & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$     & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$ & \ding{52} \\%& \cellcolor{Blue!\mixcolor}$\mathcal{O}(1)$ \\
%         \bottomrule
%      \end{tabular}
% \end{center}

% \begin{center}
%     \begin{tabular}{l c c }
%         \toprule
%         \textbf{Algorithm} & \multicolumn{2}{c}{\textbf{Representation}} \\
%                             & Adjadency List      & Adjacency Matrix  \\  
%         \midrule
%         accessing vertex $u$    & \cellcolor{Green!\mixcolor}\(O(1)\)\\ optimal                             & \cellcolor{Green!\mixcolor}\(O(1)\) \\ optimal                  \\
%         iteration through $V$   & \cellcolor{Green!\mixcolor}\(\Theta(|V|)\) \\ optimal                     & \cellcolor{Green!\mixcolor}\(\Theta(|V|)\) \\ optimal           \\
%         iteration through $E$   & \cellcolor{Yellow!\mixcolor}\(\Theta(|V| + |E|)\) okay (not optimal) \\   & \cellcolor{Red!\mixcolor}\(\Theta(|V|^2)\) \\ possibly very bad \\
%         checking $(u,v)\in E$   & \cellcolor{Red!\mixcolor}\(O(|V|)\) \\ bad                                & \cellcolor{Green!\mixcolor}\(O(1)\) \\ optimal                  \\   
%         space complexity & \cellcolor{Green!\mixcolor}\(\Theta(|V| + |E|)\) \\ optimal                      & \cellcolor{Red!\mixcolor}\(\Theta(|V|^2)\) \\ possibly very bad \\
%         \bottomrule
%      \end{tabular}
% \end{center}

\newcolumntype{C}{>{\centering\arraybackslash}m{35mm}}
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}
\begin{table}[h]
\centering
\begin{tabular}{l C @{\hspace{10pt}} C}
\toprule
\textbf{Operation}                                             & \multicolumn{2}{c}{\textbf{Representation}}                                                                           \\
&                             {Adjacency List} &                                                              {Adjacency Matrix}                                                       \\
\midrule
accessing vertex $u$ &        \cellcolor{Blue!\mixcolor}\makecell{$O(1)$\\optimal} &                          \cellcolor{Blue!\mixcolor}\makecell{$O(1)$\\optimal}                     \\
\addlinespace[2pt]
iteration through $V$ &       \cellcolor{Green!\mixcolor}\makecell{$\Theta(|V|)$\\optimal} &                  \cellcolor{Green!\mixcolor}\makecell{$\Theta(|V|)$\\optimal}             \\
\addlinespace[2pt]
iteration through $E$ &       \cellcolor{Yellow!\mixcolor}\makecell{$\Theta(|V|+|E|)$\\okay (not optimal)} &  \cellcolor{Red!\mixcolor}\makecell{$\Theta(|V|^{2})$\\possibly very bad} \\
\addlinespace[2pt]
checking $(u,v)\in E$ &       \cellcolor{Orange!\mixcolor}\makecell{$O(|V|)$\\bad} &                             \cellcolor{Blue!\mixcolor}\makecell{$O(1)$\\optimal}                     \\
\addlinespace[2pt]
space complexity &            \cellcolor{Green!\mixcolor}\makecell{$\Theta(|V|+|E|)$\\optimal} &              \cellcolor{Red!\mixcolor}\makecell{$\Theta(|V|^{2})$\\possibly very bad} \\
\bottomrule
\end{tabular}
\end{table}






\subsection{Breadth-First Search}
\label{sec:bfs}

\nameref{alg:bfs} is one of the simplest but also a fundamental algorithm, as it is the archetype of many important algorithms.
% It explores the graph, touching all vertices that are reachable from the source vertex \(s\) at an increasing edge distance from \(s\).
% It computes the distance of each vertex \(v\) from the source \(s\) and produces a breadth-first tree $G_{\pi}$ rooted at \(s\).


\begin{algorithm}[h]
\caption{Breadth-First Search}\label{alg:bfs}
\begin{algorithmic}[1]
\Function{BFS}{$G,\,s$} \Comment{\(s\) is the source}
  \ForAll{$u\in V \setminus \{s\}$}
    \State $\attrib{u}{color},\attribnormal{u}{\delta},\attrib{u}{\pi}\gets \white,\infty,\nil$
  \EndFor
  % \State Initialize $\attrib{v}{color}\gets\white$, $\attribnormal{v}{\delta}\gets\infty$, $\attrib{v}{\pi}\gets\nil$ for all $v\in V$
  \State $\attrib{s}{color},\attribnormal{s}{\delta},\attrib{s}{\pi}\gets \gray,0,\nil$
  \State $Q\gets\emptyset$ \Comment{initialize empty queue for vertices to visit}
  \State \Call{Enqueue}{$Q,s$}
  \While{$Q\neq\emptyset$} \label{alg:bfs:dequeue-while}
    \State $u\gets$ \Call{Dequeue}{$Q$}
    \ForAll{$v\in\Gamma(u)$} \label{alg:bfs:inner-loop}
      \If{$\attrib{v}{color}=\white$}
        \State $\attrib{v}{color}\gets\gray$
        \State $\attribnormal{v}{\delta}\gets\attribnormal{u}{\delta}+1$
        \State $\attrib{v}{\pi}\gets u$
        \State \Call{Enqueue}{$Q,v$}
      \EndIf
    \EndFor
    \State $\attrib{u}{color}\gets\black$
  \EndWhile
\EndFunction
\end{algorithmic}
\end{algorithm}

We enqueue a vertex only if is $\white$, and we immediately color it $\gray$; thus, we enqueue every vertex at most once.
So the (dequeue) while loop at Line \ref{alg:bfs:dequeue-while} executes \(O(|V|)\) times.
For each vertex \(u\), the inner loop at Line \ref{alg:bfs:inner-loop} executes \(\Theta(|\Gamma(u)|)\) times, for a total of \(O(|E|)\) steps.
Thus, the total running time is \(O(|V| + |E|)\).

% \begin{definition}[Shortest-Path Distance]\label{def:shortest-path-distance}
The minimum number of edges in any path from \(s\) to \(v\) is called the shortest-path distance from \(s\) to \(v\) and is denoted by \(\delta(s,v)\).
If there is no path from \(s\) to \(v\), then \(\delta(s,v) = \infty\).
% \end{definition}
We define the predecessor subgraph as \(G_\pi=(V_\pi,E_\pi)\) where
\[
V_\pi = \{v\in V\mid v.\pi \neq \nil \} \cup \{s\}
\]
and
\[
E_\pi = \{(v.\pi,v)\mid v\in V_\pi \setminus \{s\}\}
\]
The predecessor subgraph \(G_\pi\) is a \emph{breadth-first tree} if \(V_\pi\) consists of all vertices reachable from \(s\) and, for all \(v\in V_\pi\), the subgraph \(G_\pi\) contains a unique simple path from \(s\) to \(v\) that is also a shortest path from \(s\) to \(v\) in \(G\).

\nameref{alg:bfs} constructs \(\pi\) so that the predecessor subgraph \(G_\pi = (V_\pi,E_\pi)\) is a breadth-first tree rooted at \(s\).
Upon termination, \(\attribnormal{v}{\delta} = \delta(s,v)\) for all \(v\in V\).
For any vertex \(v\) reachable from \(s\), the simple path in the breadth-first tree \(G_\pi\) from \(s\) to \(v\) corresponds to a shortest path (that is, a path containing the smallest number of edges) from \(s\) to \(v\) in the original graph \(G\).

Assuming that \nameref{alg:bfs} has computed a breadth-first tree \(G_\pi\), we can print the shortest path from \(s\) to \(v\) using (the recursive) Algorithm~\ref{alg:print_shortest_path}.
\begin{algorithm}[h]
\caption{Print Shortest Path}\label{alg:print_shortest_path}
\begin{algorithmic}[1]
\Function{PrintPath}{$G,s,v$}
  \If{$v=s$}
    \State \Call{Print}{$s$}
  \ElsIf{$\attrib{v}{\pi}=\nil$}
    \State \Call{Print}{``No path exists''}
  \Else
    \State \Call{PrintPath}{$G,s,\attrib{v}{\pi}$}
    \State \Call{Print}{$v$}
  \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}


\subsection{Depth-First Search}
\label{sec:dfs}

As its name implies, \nameref{alg:dfs} searches ``deeper'' in the graph whenever possible.
\nameref{alg:dfs} explores edges out of the most recently discovered vertex \(v\) that still has unexplored edges leaving it.
Once all of \(v\)'s edges have been explored, the search \emph{backtracks} to explore edges leaving the vertex from which \(v\) was discovered.
This process continues until all vertices that are reachable from the original source vertex have been discovered.
If any undiscovered vertices remain, then \nameref{alg:dfs} selects one of them as a new source, repeating the search from that source.
This process is repeated until every vertex has been discovered.%
\footnote{Although \nameref{alg:bfs} could proceed from multiple sources and \nameref{alg:dfs} could be limited to one source source, the approach here reflects how they are typically used. 
\nameref{alg:bfs} usually serves to find shortest-path distances and the associated predecessor subgraph fro a given source, while \nameref{alg:dfs} is often a subroutine in another algorithm.}

\begin{algorithm}
\caption{Depth-First Search}\label{alg:dfs}
\begin{algorithmic}[1]
\Function{DFS}{$G$}
  \ForAll{$u\in V$}
    \State $\attrib{u}{color},\attrib{u}{\pi}\gets \white,\nil$
  \EndFor
  \State $T\gets 0$ \Comment{global time variable, for timestamps}
  \ForAll{$u\in V$} \label{alg:dfs:for_all_vertices}
    \If{$\attrib{u}{color}=\white$}
      \State \Call{DFS-Visit}{$u$} \Comment{new tree in forest} \label{alg:dfs:new_tree}
    \EndIf
  \EndFor
\EndFunction

\Function{DFS-Visit}{$u$}
  \State $\attrib{u}{color}\gets\gray$ \Comment{\(u\) has just been discovered} \label{alg:dfs:discover}
  \State $T\gets T+1$
  \State $\attribnormal{u}{d}\gets T$ \label{alg:dfs:discover_time}
  \ForAll{$v\in\Gamma(u)$} \Comment{explore each edge leaving \(u\)} \label{alg:dfs:explore_edges}
    \If{$\attrib{v}{color}=\white$}
      \State $\attrib{v}{\pi}\gets u$
      \State \Call{DFS-Visit}{$v$} \Comment{recursively visit \(v\) if it undiscovered} \label{alg:dfs:recursive_call}
    \EndIf
  \EndFor
  \State $\attrib{u}{color}\gets\black$ \Comment{blacken \(u\); it is finished} \label{alg:dfs:finish}
  \State $T\gets T+1$
  \State $\attribnormal{u}{f}\gets T$ \label{alg:dfs:finish_time}
\EndFunction
\end{algorithmic}
\end{algorithm}

Since \nameref{alg:dfs} always explores all vertices, we define the predecessor subgraph as \(G_\pi=(V,E_\pi)\), where
\[
E_\pi = \{(v.\pi,v)\mid v\in V \AND v.\pi \neq \nil\}
\]
It is a \emph{depth-first forest}, comprising several \emph{depth-first trees}.

Each vertex is initially white, is grayed when it is discovered in the search (Line~\ref{alg:dfs:discover}), and is blackened when it is finished, that is, when its neighborhood has been examined completely (Line~\ref{alg:dfs:finish}).

Each vertex \(u\) has two timestamps: the first, \(\attribnormal{u}{d}\), records when \(u\) is first discovered (and grayed), and the second, \(\attribnormal{u}{f}\), records when the search finished examining \(u\)'s neighborhood (and blackens \(u\)).
For each vertex \(u \in V\), we have
\[
\attribnormal{u}{d}, \attribnormal{u}{f} \in \{1, \ldots, 2|V|\} \quad \text{and} \quad \attribnormal{u}{d} < \attribnormal{u}{f} 
\]
since there is one discovery and one finishing event for each of the \(|V|\) vertices.
\(u\) is \(\white\) before \(\attribnormal{u}{d}\), \(\gray\) between \(\attribnormal{u}{d}\) and \(\attribnormal{u}{f}\), and \(\black\) after \(\attribnormal{u}{f}\).

Upon every call of \Call{DFS-Visit}{$u$} in Line~\ref{alg:dfs:new_tree}, a new depth-first tree is created, rooted at \(u\).
In each call \Call{DFS-Visit}{$u$}, \(u\) is initially \(\white\).
Lines \ref{alg:dfs:explore_edges}--\ref{alg:dfs:recursive_call} examine each vertex \(v\) adjacent to \(u\) and recursively visit \(v\) if it is \(\white\).

The result depends on the order in which Line~\ref{alg:dfs:for_all_vertices} examines the vertices and upon the order in which Line~\ref{alg:dfs:explore_edges} visits the neighbors of \(u\).
Usually, these different visiation orders tend not to cause problems, because many applications can use any of those.

We call \Call{DFS-Visit}{$u$} exactly once (either in Line~\ref{alg:dfs:new_tree} or recursively in Line~\ref{alg:dfs:recursive_call}) for each vertex \(u\), because we call it only if \(\attrib{u}{color}=\white\), but then we immediately set \(\attrib{u}{color}=\gray\) in Line~\ref{alg:dfs:discover}.
The loop in Lines~\ref{alg:dfs:explore_edges}--\ref{alg:dfs:recursive_call} executes \(\Theta(|\Gamma(u)|)\) times.
So, the total running time is $\Theta(|V|+|E|)$.

\underline{Properties of the Depth-First Forest \(G_\pi\):}
\begin{itemize}
  \item \(v\) is a descendant of \(u\) \(\Longleftrightarrow\) \(v\) is discovered during the time in which \(u\) is gray
  \item discovery and finish times have \emph{parenthesis structure}: if in \textsc{DFS-Visit} we printed `(u' when we discovered \(u\) and `u)' when we finished \(u\), then the printed expression would be well formed in the sense that the parentheses are properly nested
  \item for any two vertices \(u_1\) and \(u_2\), exactly one of the following conditions holds
    \begin{itemize}
      \item \([\attribnormal{u_1}{d}, \attribnormal{u_1}{f}] \cap [\attribnormal{u_2}{d}, \attribnormal{u_2}{f}] = \emptyset\) \(\Longleftrightarrow\) neither is a descendant of the other
      \item \([\attribnormal{u_1}{d}, \attribnormal{u_1}{f}] \subset [\attribnormal{u_2}{d}, \attribnormal{u_2}{f}]\) \(\Longleftrightarrow\) \(u_1\) is a descendant of \(u_2\)
      \item \([\attribnormal{u_1}{d}, \attribnormal{u_1}{f}] \supset [\attribnormal{u_2}{d}, \attribnormal{u_2}{f}]\) \(\Longleftrightarrow\) \(u_2\) is a descendant of \(u_1\)
    \end{itemize}
  \item \(v\) is a descendant of \(u\) \(\Longleftrightarrow\) at the time \attribnormal{u}{d} that the search discovers \(u\), there is a path from \(u\) to \(v\) consisting entirely of white vertices
\end{itemize}

\pagebreak[2]
\underline{Classification of Edges in \(E\):}
% Each edge \((u,v) \in E\) is classified as one of the following:
\nopagebreak
\begin{itemize}
  \item \emph{tree edges} are edges \((u,v) \in E_\pi\) in the depth-first forest \(G_\pi\). \((u,v)\) is a tree edge if \(v\) was first discovered by exploring the edge \((u,v)\) in Line~\ref{alg:dfs:explore_edges}.
  \item \emph{back edges} are edges \((u,v) \notin E_\pi\) connecting \(u\) to an ancestor \(v\) in the depth-first forest \(G_\pi\).
  \item \emph{forward edges} are edges \((u,v) \notin E_\pi\) connecting \(u\) to a descendant \(v\) in the depth-first forest \(G_\pi\).
  \item \emph{cross edges} are edges \((u,v) \notin E_\pi\) are all other edges.
\end{itemize}

\begin{lemma}\label{lem:dag-no_back_edges}
A directed graph is acyclic if and only if a \nameref{alg:dfs} yields no back edges.
\end{lemma}
\begin{theorem}\label{thm:unirected_graph-only_tree_and_back_edges}
In a undirected graph, a \nameref{alg:dfs} yields only tree edges and back edges.
\end{theorem}


When during an \nameref{alg:dfs} an edge \((u,v)\) is first explored, the color of \(v\) yields information about the edge:
\begin{itemize}%[before={\parskip=0pt},nosep]
  \item if \(v\) is \(\white\), then \((u,v)\) is a tree edge
  \item if \(v\) is \(\gray\), then \((u,v)\) is a back edge
  \item if \(v\) is \(\black\), then \((u,v)\) is a
\begin{itemize}%[before={\parskip=0pt},nosep]
    \item forward edge if \(\attribnormal{u}{d} < \attribnormal{v}{d}\)
    \item cross edge if \(\attribnormal{u}{d} > \attribnormal{v}{d}\)
\end{itemize}
\end{itemize}
Observe that the gray vertices always form a linear chain of descendants corresponding to the stack of active \textsc{DFS-Visit} invocations.

\subsubsection{Topological Sort}\label{sec:topological_sort}
A \emph{topological sort} of a directed acyclic graph (``dag'') \(G=(V,E)\) is a linear ordering of the vertices such that if \(G\) contains an edge \((u,v)\), then \(u\) appears before \(v\) in the ordering.
\begin{algorithm}[h]
\caption{Topological Sort}\label{alg:topological_sort}
\begin{algorithmic}[1]
\Function{TopologicalSort}{$G$}
  \State call \Call{DFS}{$G$} to compute the finish time \(\attribnormal{v}{f}\) for each \(v\in V\)
  \State as each vertex \(v\) is finished, insert it onto the front of a linked list \(L\)
  \State \Return \(L\)
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsubsection{Strongly Connected Components}\label{sec:strongly_connected_components}
\begin{definition}[Strongly Connected Component]\label{def:strongly_connected_component}
A \emph{strongly connected component} of a directed graph \(G=(V,E)\) is a maximal set of vertices \(C\subseteq V\) such that for every pair of vertices \(u,v\in C\), both \(u \leadsto v\) and \(v \leadsto u\), that is, \(u\) and \(v\) are reachable from each other.
\end{definition}

Algorithm \ref{alg:strongly_connected_components} uses the transpose of $G$, $G^{\mathrm{T}}=(V, E^{\mathrm{T}})$, where $E^{\mathrm{T}}=\{(u, v):(v, u) \in E\}$. 
That is, $E^{\mathrm{T}}$ consists of the edges of $G$ with their directions reversed. 
Given an adjacency-list representation of $G$, the time to create $G^{\mathrm{T}}$ is $\Theta(V+E)$. 
The graphs $G$ and $G^{\mathrm{T}}$ have exactly the same strongly connected components: $u$ and $v$ are reachable from each other in $G$ if and only if they are reachable from each other in $G^{\mathrm{T}}$. 

The linear-time (i.e., $\Theta(V+E)$-time) Algorithm \ref{alg:strongly_connected_components} computes the strongly connected components of a directed graph $G=(V, E)$ using two depth-first searches, one on $G$ and one on $G^{\mathrm{T}}$.

\begin{definition}[Component Graph]\label{def:component_graph}
Suppose that $G$ has strongly connected components $C_1, \ldots, C_k$. 
The vertex set $V^{\mathrm{SCC}}$ is $\{v_1, \ldots, v_k\}$, and it contains one vertex $v_i$ for each strongly connected component $C_i$ of $G$. 
There is an edge $(v_i, v_j) \in E^{\mathrm{SCC}}$ if $G$ contains a directed edge $(x, y)$ for some $x \in C_i$ and some $y \in C_j$. 

Looked at another way, if we contract all edges whose incident vertices are within the same strongly connected component of $G$ so that only a single vertex remains, the resulting raph is $G^{\mathrm{SCC}}$.
\end{definition}

\begin{algorithm}[h]
\caption{Strongly Connected Components}\label{alg:strongly_connected_components}
\begin{algorithmic}[1]
  \Function{StronglyConnectedComponents}{$G$}
  \State call \Call{DFS}{$G$} to compute the finish time \(\attribnormal{v}{f}\) for each \(v\in V\) \label{alg:strongly_connected_components:first_dfs}
  \State create the transpose graph \(G^{\mathrm{T}}\)
  \State call \Call{DFS}{$G^{\mathrm{T}}$}, but in Line~\ref{alg:dfs:for_all_vertices}, consider the vertices in order of decreasing \(\attribnormal{u}{f}\) \label{alg:strongly_connected_components:second_dfs}
  \State \Return the depth-first trees of \(G_\pi\) formed in Line~\ref{alg:strongly_connected_components:second_dfs}
  \EndFunction
\end{algorithmic}
\end{algorithm}


% The idea behind Algorithm \ref{alg:strongly_connected_components} comes from the key property that t

The \nameref{def:strongly_connected_component}s, defined in Definition~\ref{def:strongly_connected_component} and the \nameref{def:component_graph} $G^{\mathrm{SCC}}=(V^{\mathrm{SCC}}, E^{\mathrm{SCC}})$, defined in Definition~\ref{def:component_graph} have the following properties:

If $C_i$ and $C_j$ are two distinct \nameref{def:strongly_connected_component}s in a directed graph \(G=(V,E)\),...
\begin{enumerate}
  \item ...and \(u_i, v_j \in C_i\) and \(u_j, v_j \in C_j\), and \(G\) contains a path \(u_i \leadsto u_j\), then \(G\) cannot also contain a path \(v_j \leadsto v_i\), i.e. the \nameref{def:component_graph} $G^{\mathrm{SCC}}$ is acyclic. \label{properties_component_graph:acyclic}
  \item ...and there is an edge \((u, v) \in E\) with \(u \in C_j\) and \(v \in C_i\), then \(\attribnormal{C_j}{f} > \attribnormal{C_i}{f}\). \label{properties_component_graph:finish_time}
  \item ...and suppose that \(\attribnormal{C_i}{f} > \attribnormal{C_j}{f}\), then \(E^{\mathrm{T}}\) contains no edge \((v, u)\) such that \(u \in C_j\) and \(v \in C_i\). \label{properties_component_graph:transposed_edges}
\end{enumerate}

Property \ref{properties_component_graph:acyclic} implies that the \nameref{def:component_graph} $G^{\mathrm{SCC}}$ can be topologically sorted (see Section~\ref{sec:topological_sort}).
And in fact, Algorithm~\ref{alg:strongly_connected_components} visits the vertices of the \nameref{def:component_graph} in topologically sorted order, by considering vertices in the second depthfirst search (Line \ref{alg:strongly_connected_components:second_dfs}) in decreasing order of the finish times that were computed in the first depth-first search (Line~\ref{alg:strongly_connected_components:first_dfs}).

% Property \ref{properties_component_graph:transposed_edges} follows from Property~\ref{properties_component_graph:finish_time}.
In Properties \ref{properties_component_graph:finish_time} and \ref{properties_component_graph:transposed_edges}, the finish time of a \nameref{def:strongly_connected_component}, \(\attribnormal{C}{f}\), refers to the maximal finish time of any vertex in \(C\) as computed by Line~\ref{alg:strongly_connected_components:first_dfs} of Algorithm~\ref{alg:strongly_connected_components}, i.e., 
\[
\attribnormal{C}{f} := \max_{v\in C} \left(\attribnormal{v}{f}\right)
\]

Property~\ref{properties_component_graph:transposed_edges} provides the key idea behind Algorithm~\ref{alg:strongly_connected_components}:
When the \nameref{alg:dfs} of \(G^{\mathrm{T}}\) in Line~\ref{alg:strongly_connected_components:second_dfs} of Algorithm~\ref{alg:strongly_connected_components} visits any \nameref{def:strongly_connected_component}, any edges out of that component must be to components that the search has already visited.
Each depth-first tree produced by Line~\ref{alg:strongly_connected_components:second_dfs}, therefore, corresponds to exactly one \nameref{def:strongly_connected_component} of \(G\).

Another way to see this is to consider the component graph $(G^{\mathrm{T}})^{\mathrm{SCC}}$ of $G^{\mathrm{T}}$. 
If we map each strongly connected component visited in the second depth-first search to a vertex of $(G^{\mathrm{T}})^{\mathrm{SCC}}$, the second depth-first search visits vertices of $(G^{\mathrm{T}})^{\mathrm{SCC}}$ in the reverse of a topologically sorted order. 
% If we reverse the edges of $(G^{\mathrm{T}})^{\mathrm {SCC }}$, we get the graph $((G^{\mathrm{T}})^{\mathrm{SCC}})^{\mathrm{T}}$. 
% Because $((G^{\mathrm{T}})^{\mathrm{SCC}})^{\mathrm{T}}=G^{\mathrm{SCC}}$, the second depth-first search visits the vertices of $G^{\mathrm{SCC}}$ in topologically sorted order.


\subsection{Minimum Spanning Tree}\label{sec:minimum_spanning_tree}

Given a graph \(G=(V,E)\) with weight function \(w:E\to\R\) we want to find an acyclic subset \(T\subseteq E\) such that \(T\) touches all vertices in \(V\) and 
\[
w(T) = \sum_{e\in T} w(e)
\]
that is, the total weight of the tree \(T\) is minimal.
This \(T\) is called a \emph{minimum spanning tree} (MST) of \(G\).

Conceptually, \nameref{alg:kruskal}'s algorithm (1956) is similar to the \nameref{alg:strongly_connected_components} algorithm, while \nameref{alg:prim}'s algorithm (1957) resembles \nameref{alg:dijkstra}'s shortest-paths algorithm.
Both
% Algorithm \ref{alg:kruskal} and \ref{alg:prim} 
run in \(O(|E|\log|V|)\) time.
% \nameref{alg:prim} achieves this bound by using a binary heap as a priority queue.
% By using Fibonacci heaps instead, \nameref{alg:prim} can be made to run in \(O(|E|+|V|\log|V|)\) time.
% This is an improvement if \(|E|\)  grows asymptotically faster than \(|V|\).

The general idea is to build \(T\) by adding one edge \(e\in E\) at a time, such that \(e\) is the lightest edge that does not create a cycle.
Both \nameref{alg:kruskal} and \nameref{alg:prim} are greedy algorithms (see \ref{sec:greedy_algorithms}), that is, in each step they make the choice that seems best at the moment.
Even though such a strategy does not generally yield an optimal solution, for the MST problem one can prove that certain greedy strategies yield an optimal solution.


\begin{algorithm}[h]
\caption{Generic Minimum Spanning Tree}\label{alg:generic_mst}
\begin{algorithmic}[1]
\Function{Generic-MST}{$G,w$}
  \State $A\gets\emptyset$ \Comment{invariant: \(A\) is part of a minimum spanning tree}
  \While{$A$ is not a spanning tree} \label{alg:generic_mst:while_start}
    \State find a safe edge \(e=(u,v)\) \Comment{a safe edge maintains the invariant} \label{alg:generic_mst:safe_edge}
    \State $A\gets A\cup\{e\}$
  \EndWhile \label{alg:generic_mst:while_end}
  % \State \Return $A$
\EndFunction
\end{algorithmic}
\end{algorithm}
The strategy is given in Algorithm~\ref{alg:generic_mst}. 
The invariant used is that \(A\) is a subset of a minimum spanning tree and a \emph{safe edge} is an edge that maintains this invariant.

% \begin{definition}[Cut, Crossing, Respecting]\label{def:cut_crossing_respecting}
A \emph{cut} of a graph \(G=(V,E)\) is a partition of \(V\) into two disjoint sets \(S\) and \(V\setminus S\).
An edge \(e\in E\) \emph{crosses} the cut if one of its endpoints is in \(S\) and the other is in \(V\setminus S\).
A cut \emph{respects} a set of edges \(A\) if no edges in \(A\) cross the cut. 
% An edge is a light edge crossing a cut if its weight is minimal among all edges crossing the cut.
% \end{definition}
\begin{theorem}\label{safe_edge_cut}
Let \(A \subseteq E\) be included in a minimum spanning tree and 
\((S, V\setminus S)\) be a cut that respects \(A\).
Then a minimum-weight edge \(e\) crossing the cut \((S, V\setminus S)\) is a safe edge for \(A\).
\end{theorem}
% \begin{proof}
% Let \(T\) be a minimum spanning tree with \(A\subseteq T\).  
% If the minimum-weight edge \(e\) crossing \((S,V\setminus S)\) already lies in \(T\), we are done.  
% Otherwise adding \(e\) to \(T\) creates a cycle, and since the cut respects \(A\), that cycle contains an edge \(f\notin A\) crossing the cut.  
% Because \(e\) minimizes \(w(\cdot)\) over all crossing edges, \(w(e)\le w(f)\).  
% Replacing \(f\) by \(e\) yields a spanning tree \(T'\) with \(w(T')\le w(T)\), so \(T'\) is also an MST containing \(A\cup\{e\}\).  
% Hence \(e\) is safe.
% \end{proof}

Theorem~\ref{safe_edge_cut} provides a rule for recognizing safe edges and is the key to why Algorithm~\ref{alg:generic_mst} works.
At any point in the execution, the graph \(G_A=(V,A)\) is a forest and each of the connected components of \(G_A\) is a tree.
Any safe edge \(e\) for \(A\) connects two distinct components of \(G_A\), since \(A \cup \{e\}\) must be acyclic.

The loop in Lines \ref{alg:generic_mst:while_start}--\ref{alg:generic_mst:while_end} of Algorithm~\ref{alg:generic_mst} executes \(|V|-1\) times, because it finds one of the \(|V|-1\) edges of a a minimum spanning tree in each iteration.
Initially, when \(A=\emptyset\), there are \(|V|\) trees in \(G_A\) and each iteration reduces that number by one, until the forest contains only a single tree, upon which the method terminates.

\nameref{alg:kruskal} and \nameref{alg:prim} both use the following corollary of Theorem~\ref{safe_edge_cut}.
\begin{corollary}\label{cor:safe_edge_connected_component}
Let \(A \subseteq E\) be included in a minimum spanning tree and 
let \(C = (V_C, E_C)\) be a connected component (tree) of \(G_A = (V,A)\).
Then a minimum-weight edge \(e\) connecting \(C\) to some other component in \(G_A\) is a safe edge for \(A\).
\end{corollary}

\nameref{alg:kruskal} and \nameref{alg:prim} use a different, specific rule to determine a safe edge in Line~\ref{alg:generic_mst:safe_edge} of Algorithm~\ref{alg:generic_mst}.
In \nameref{alg:kruskal}'s algorithm, \(A\) is a forest whose vertices are all those of the given graph and the safe edge added is always a lowest-weight edge that connects two distinct components.
In \nameref{alg:prim}'s algorithm, \(A\) is a single tree and the safe edge added is always a lowest-weight edge connecting the tree to a vertex not in the tree.

For \nameref{alg:kruskal}, we need the \emph{disjoint-set data structure}:
\begin{itemize}
  \item \Call{MakeSet}{$x$} creates a set containing only the element \(x\)
  \item \Call{Find}{$x$} returns the representative of the set containing \(x\)
  \item \Call{Union}{$x,y$} joins the sets containing \(x\) and \(y\) into a single set
\end{itemize}

\begin{algorithm}[h]
\caption{Kruskal}\label{alg:kruskal}
\begin{algorithmic}[1]
\Function{Kruskal}{$G,w:E\to\R$}
  \State $A\gets\emptyset$
  \ForAll{$v\in V$}
  \State \Call{MakeSet}{$v$}
  \EndFor
  \State sort $E$ in non-decreasing order by $w$
  \ForAll{$(u,v)\in E$ in non-decreasing $w$-order}
    \If{$\text{\Call{Find}{$u$}} \neq \text{\Call{Find}{$v$}}$}
      \State $A\gets A\cup\{(u,v)\}$
      \State \Call{Union}{$u,v$}
    \EndIf
  \EndFor
  % \State \Return $A$ \Comment{edges of the minimum spanning tree}
\EndFunction
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[h]
\caption{Prim}\label{alg:prim}
\begin{algorithmic}[1]
\Function{Prim}{$G, w:E\to\R, r$} 
  \State $T\gets(\emptyset,\emptyset)$ \Comment{minimal spanning tree}
  \ForAll{$v\in V(G)$}
    \State $\attrib{v}{weight}\gets\infty$ \Comment{best known cost of connecting \(v\) to \(T\)}
    \State $\attrib{v}{\pi}\gets\nil$ \Comment{\(u \in T\) such that \((u,v)\) is the least-cost edge connecting \(v\) to \(T\)}
  \EndFor
  \State $\attrib{r}{weight}\gets 0$ \Comment{\(r\) is the root of the spanning tree}
  \While{$V(T)\neq V(G)$} \Comment{while \(T\) does not span all vertices}
    \State find \(u \notin V(T)\) such that \(\attrib{u}{weight}\) is minimal
    \State $T\gets T\cup\{u\}$ \Comment{add \(u\) to the spanning tree \(T\)}
    \ForAll{$v\in\Gamma(u) \setminus V(T)$}
      \If{$w(u,v)<\attrib{v}{weight}$}
        \State $\attrib{v}{weight} \gets w(u,v)$
        \State $\attrib{v}{\pi} \gets u$
      \EndIf
    \EndFor
  \EndWhile
  % \State \Return $\{(\attrib{v}{\pi},v)\mid v\in V\setminus\{r\}\}$ \Comment{edges of the minimum spanning tree}
\EndFunction
\end{algorithmic}
\end{algorithm}


\subsection{Single-Source Shortest Paths}\label{sec:single_source_shortest_paths}


\nameref{alg:dijkstra}'s algorithm is a greedy, label-setting method for nonnegative edge weights.  
At each step it finalizes the closest unreached vertex and relaxes its outgoing edges, yielding a time complexity of \(O((|V|+|E|)\log |V|)\) with a binary-heap implementation.

\begin{algorithm}[h]
\caption{Dijkstra}\label{alg:dijkstra}
\begin{algorithmic}[1]
\Function{Dijkstra}{$G, w:E\to\R_{\ge0}, s$} 
  \State $N \gets \emptyset$ \Comment{nodes of \(G\) whose least-cost path from \(s\) is definitely known}
  \ForAll{$v\in V(G)$}
    \State $\attrib{v}{\delta}\gets\infty$ \Comment{best known cost from \(s\) to \(v\)}
    \State $\attrib{v}{\pi}\gets\nil$ \Comment{node preceding \(v\) on the least-cost path from \(s\)}
  \EndFor
  \State $\attrib{s}{\delta}\gets 0$ \Comment{\(s\) is the source}
  \While{$N \neq V(G)$} \Comment{while we do not know the least-cost path to all nodes}
    \State find \(u \notin N\) such that \(\attrib{u}{\delta}\) is minimal
    \State $N\gets N\cup\{u\}$ \Comment{add \(u\) to the nodes \(N\)}
    \ForAll{$v\in\Gamma(u) \setminus N$}
      \If{$\attrib{u}{\delta} + w(u,v)<\attrib{v}{\delta}$}
        \State $\attrib{v}{\delta} \gets \attrib{u}{\delta} + w(u,v)$
        \State $\attrib{v}{\pi} \gets u$
      \EndIf
    \EndFor
  \EndWhile
  % \State \Return $(\attrib{v}{\delta})_{v\in V(G)}$ \Comment{least-cost path from \(s\) to each \(v\)}
\EndFunction
\end{algorithmic}
\end{algorithm}






\nameref{alg:bellmanford} algorithm is a dynamic-programming, label-correcting method that supports arbitrary (i.e. including negative) weights and detects negative-weight cycles.  
It relaxes all edges in up to \(|V|-1\) passes for \(O(|V|\times|E|)\) time and uses one extra pass to check for cycles.

The Bellman-Ford equation:
\begin{equation}\label{eq:bellmanford}
\delta(s, v) = \min_{(u,v)\in E} \left(\delta(s, u) + w(u, v)\right)
\end{equation}

\begin{algorithm}[h]
\caption{Bellman-Ford}\label{alg:bellmanford}
\begin{algorithmic}[1]
\Function{BellmanFord}{$G, w:E\to\R, s$}
  \ForAll{$v\in V(G)$}
    \State $\attrib{v}{\delta}\gets\infty$
    \State $\attrib{v}{\pi}\gets\nil$
  \EndFor
  \State $\attrib{s}{\delta}\gets 0$
  \For{$i=1$ to $|V(G)|-1$} 
    \ForAll{$(u,v)\in E(G)$}
      \If{$\attrib{u}{\delta}+w(u,v)<\attrib{v}{\delta}$} 
        \State $\attrib{v}{\delta}\gets \attrib{u}{\delta}+w(u,v)$ \Comment{applying Equation~\eqref{eq:bellmanford}}
        \State $\attrib{v}{\pi}\gets u$
      \EndIf
    \EndFor
  \EndFor
  \ForAll{$(u,v)\in E(G)$}
    \If{$\attrib{u}{\delta}+w(u,v)<\attrib{v}{\delta}$}
      \State \Return \fals \Comment{negative-weight cycle detected}
    \EndIf
  \EndFor
  \State \Return \tru
\EndFunction
\end{algorithmic}
\end{algorithm}






%\clearpage
\section{Design Techniques}\label{sec:design_techniques}

\subsection{Greedy Algorithms}\label{sec:greedy_algorithms}

Greedy algorithms construct a solution by always choosing the option that looks best at the moment.
That is, they make a locally optimal choice in the hope that this choice leads to a globally optimal solution.

At every step, we consider only what is best best in the current problem, not considering the results of the subproblems.


% \begin{definition}\label{def:greedy_properties}
The key ingredients of a problem for a greedy strategy to work:
\begin{enumerate}
  \item \emph{Greedy-Choice}: \label{greedy_choice_property}
  One can always arrive at a globally optimal solution by making a locally optimal choice.
  \item \emph{Optimal Substructure}:  \label{optimal_substructure}
  An optimal solution to the problem contains within it optimal solutions to subproblems.
\end{enumerate}
It is natural to prove property \ref{optimal_substructure} by induction: 
if the solution to the subproblem is optimal, then combining the greedy choice with that solution yields an optimal solution.
% \end{definition}


The proof pattern when designing a greedy algorithm is:
\begin{enumerate}[partopsep=0em, topsep=0em, label=(\roman*)]
  \item \emph{Cast} the problem as one where we make a \emph{greedy choice} and are left with a \emph{subproblem}. \label{greedy_design_step:cast}
  \item \emph{Prove} that (at least) one optimal solution (not necessarily always the same one) contains the greedy choice (Property \ref{greedy_choice_property}).
  \item \emph{Prove} that the remaining subproblem is such that combining the greedy choice with the optimal solution of the subproblem gives an optimal solution to the original problem.% (Property \ref{optimal_substructure}). not sure if it is accurate to say that this is the same as the optimal substructure property
\end{enumerate}

\begin{remark}[Designing Greedy Algorithms]\label{rem:designing_greedy_algorithms}
\leavevmode
\begin{itemize}
  \item Inventing a greedy algorithm is easy (easy to come up with greedy choices)
  \item Proving it optimal may be hard (requires deep understanding of the structure of the problem)
  % \item When correctness proofs become convoluted, the problem likely lacks the greedy-choice property—reconsider dynamic programming.
  \qedhere
\end{itemize}
\end{remark}


\begin{remark}[Greedy vs. Dynamic Programming]\label{rem:greedy_vs_dp}
Greedy algorithms have many similarities to dynamic programming. 
In particular, problems for which dynamic programming works also need to have optimal substructure (Property \ref{optimal_substructure}).

One major difference is that instead of first finding optimal solutions to subproblems and then making an informed choice, greedy algorithms first make a greedy choice (the choice that looks best at the time) and then solve \emph{one} resulting subproblem, without bothering to solve all possible related smaller subproblems.

Dynamic programming is more general since it does not need the greedy-choice property (Property \ref{greedy_choice_property}).
It usually looks at several subproblems and ``dynamically'' chooses one of them to optain a global solution.
\end{remark}


\subsection{Dynamic Programming}\label{sec:dynamic_programming}

Dynamic programming is a method for solving (typically optimization or counting, sometimes also probability) problems by decomposing them into (slightly smaller) subproblems that share subsubproblems.
To make such a recursive approach efficient, the optimal solutions to those subproblems are stored in an array/table of appropriate size and dimension, so that each susubbproblem is solved just once.
% Naïve recursion yields exponential complexity!
Instead of solving the problem top-down (recursively, with memoization) it is also possible (and often slightly more efficient due to the overhead of recursion) to solve the problem bottom-up (iteratively filling the table, starting with the base cases).




Not all problems can be solved with dynamic programming. 
They need to have certain properties.
The key ingredients of a problem for a dynamic programming method to work are:
\begin{enumerate}
  \item \emph{Optimal Substructure}:  \label{optimal_substructure_dp}
  An optimal solution to the problem contains within it optimal solutions to subproblems.
  \item \emph{Overlapping Subproblems}: \label{overlapping_subproblems}
  The subproblems share subsubproblems.
  A recursive solution therefore revisits the same subsubproblem repeatedly.
\end{enumerate}

% Steps to discover the optimal substructure (Property \ref{optimal_substructure_dp}):
% \begin{enumerate}[partopsep=0em, topsep=0em, label=(\roman*)]
% 	\item Show that a solution to the problem consists of making a choice.
%         Making this choice leaves one or more subproblems to be solved.
% 	\item Suppose that for a given problem there exists a choice which leads to an optimal substructure. 
%         Do not concern yourself with how to determine this choice, but assume it has been given to you.
% 	\item Given this choice, determine which subproblems ensue and how to best characterize the resulting space of subproblems.
% 	\item Using a ``cut-and-paste'' technique, show that the solutions to the subproblems used within an optimal solution to the problem must themselves be optimal.
%         You can do so by supposing that each subproblem is not optimal and then deriving a contradiction.
% \end{enumerate}


\begin{example}[Unweighted shortest/longest path]\label{ex:unweighted_shortest_longest_path}
Given $G=(V,E)$,
\begin{itemize}
  \item find the length of the shortest path from $u$ to $v$
    \begin{itemize}
    \item decompose $u\leadsto v$ into $u\leadsto w\leadsto v$
    \item easy to prove that, if $u\leadsto w\leadsto v$ is minimal, then $w\leadsto v$ is also minimal
    \item this is the \emph{optimal substructure property}!
    \end{itemize}
  \item find the length of the longest simple (i.e., no cycles) path from $u$ to $v$
    \begin{itemize}
    \item we can also decompose $u\leadsto v$ into $u\leadsto w\leadsto v$
    \item however, it is not the case that, if $u\leadsto w\leadsto v$ is maximal, then $w\leadsto v$ is also maximal, as can be shown by a counterexample \qedhere
    \end{itemize}
  \end{itemize}
\end{example}

Typical steps to solve a problem with dynamic programming:
\begin{enumerate}
  \item Identify a problem as a dynamic programming problem: It should satisfy the above properties.
  \item State expression\footnote{Dynamic Programming problems are all about the state and its transition. This is the most basic step which must be done very carefully because the state transition depends on the choice of state definition you make.
        A state can be defined as the set of parameters that can uniquely identify a certain position or standing in the given problem. This set of parameters should be as small as possible to reduce state space.}: Choose a set of state variables so that each valid combination represents a distinct subproblem. A good rule of thumb is to try to keep it as simple as possible.
  \item Formulate state and transition relationship: Derive a recurrence (transition) relation that expresses the cost/value of a state in terms of smaller states, i.e., derive the solution to the main problem from (one of) the solutions to the $O(1), O(n), O(n^2), \ldots$ subproblems.
  \item Identify the base case(s): Subproblem(s) that can't be divided into any smaller subproblems and don't depend on any other subproblems for a solution.
  \item Apply tabulation (bottom-up) or memoization (top-down).
\end{enumerate}

\begin{remark}[Dynamic Programming vs. Divide and Conquer]\label{rem:dp_vs_divide_and_conquer} 
Both decompose a problem into subproblems.
But in divide-and-conquer the subproblems are disjoint, that is, they do not share subsubproblems.
The subproblems in divide-and-conquer are also typically much smaller than the original problem, while in dynamic programming the subproblems are typically only slightly smaller than the original problem (e.g. reducing \(L(j)\) to \(L(j-1)\))
\end{remark}



\begin{example}[Pingala]\label{ex:pingala_dp}
Using memoization, we can make the recursive algorithm from Example~\ref{ex:pingala} much more efficient.


\begin{algorithm}[htb] 
    \caption{Pingala, top-down with memoization}
    \label{alg:pingala_dp_topdown}
  \begin{algorithmic}[1]
    \State $\mathtt{memo} \gets [\nil] \times n$ \Comment{memoization array of size $n$}
    \Function{Pingala}{$n$} \Comment{Number of $1,2$-beats in $n$ beats}
      \If{$n \le 2$}
        \State \Return $n$
      \EndIf
      \If{$\mathtt{memo}[n] = \nil$} 
        \State $\mathtt{memo}[n] \gets \Call{Pingala}{n-1} + \Call{Pingala}{n-2}$
      \EndIf
      \State \Return $\mathtt{memo}[n]$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

% And with a couple extra varaibles, we can also build the solution bottom-up, without recursion:
% \begin{algorithm}[htb] 
%     \caption{Pingala, bottom-up}
%     \label{alg:pingala_dp_bottomup}
%   \begin{algorithmic}[1]
%     \State $\mathtt{memo} \gets [\nil] \times n$ \Comment{memoization array of size $n$}
%     \Function{Pingala}{$n$} \Comment{Number of $1,2$-beats in $n$ beats}
%       \If{$n \le 2$}
%         \State \Return $n$
%       \EndIf
%       \If{$\mathtt{memo}[n] = \nil$} 
%         \State $\mathtt{memo}[n] \gets \Call{Pingala}{n-1} + \Call{Pingala}{n-2}$
%       \EndIf
%       \State \Return $\mathtt{memo}[n]$
%     \EndFunction
%   \end{algorithmic}
% \end{algorithm}

Algorithm~\ref{alg:pingala_dp_topdown} runs in $O(n)$ time.
\end{example}






%\clearpage



\section{Complexity Theory}
% Complexity theory is about classifying computational problems by their computational difficulty. 
% \[
% \begin{tikzpicture}[scale=0.8]
%   % Draw the two circles
%   % \filldraw[color=blue!60, fill=blue!20, very thick]
%   \filldraw[color=black, fill=white!20, very thick]
%     (0,0) circle (2.5)  coordinate (C);  % big NP circle
%   \filldraw[color=black, fill=white!20, very thick]
%     (0,1) circle (1.0);                  % small P circle

%   % Labels
%   \node at (0,1)   {P};
%   \node at (0,-0.5){NP};
%   \node at (0,-1.7){NPC};

%   % Dashed NPC boundary
%   \draw[dashed, thick]
%     (-1.65,-1.7) .. controls (-0.5,-1) and (0.5,-1) .. (1.65,-1.7);

%   % Define the line path and the circle path
%   \path[name path=ray] (1.5,1.5) -- (3,2.5);
%   \path[name path=circle] (0,0) circle (2.5);

%   % Compute their intersection
%   \path[name intersections={of=ray and circle, by=I}];

%   % % Draw only up to the intersection point
%   % \draw[] (3,2.5) -- (I);

%   % % Place label at that intersection
%   % \node[above] at (3,2.5) {all computational problems};
% \end{tikzpicture}
% \]
% P contains all problems that can be \emph{solved} in polynomial time.

% NP contains all problems for which one can \emph{verify} a given solution in polynomial time, whereas the solution may not be found in polynomial time.



\begin{definition}[Polynomial-Time Algorithm]
A \emph{polynomial-time algorithm} is one whose worst-case running time $T(n)$, on an input of size $n$ bits, is $O(n^{k})$ for some constant $k$.
\end{definition}


you have $n$ objects:\\[2pt]
\begin{tabular}{p{0.025\linewidth} p{0.3\linewidth} p{0.25\linewidth} p{0.15\linewidth}}
& all pairs                        & polynomial:        & $\Theta(n^2)$\\ 
& all triples                      & polynomial:        & $\Theta(n^3)$\\ 
& all $k$-tuples for a fixed $k$   & polynomial:        & $\Theta(n^k)$\\ 
& all subsets                      & super-polynomial: & $\Theta(2^n)$\\ 
& all permutations                 & super-polynomial: & $\Theta(n!)$\\ 
\end{tabular}

you have a graph over $n$ vertexes:\\[2pt]
\begin{tabular}{p{0.025\linewidth} p{0.3\linewidth} p{0.25\linewidth} p{0.15\linewidth}}
& all edges                        & polynomial:        & $\Theta(n^2)$\\ 
& all trees                        & super-polynomial:  & $\Theta(n^{n-2})$\\ 
& all complete tours               & super-polynomial:  & $\Theta(n!)$\\ 
& all cuts                         & super-polynomial:  & $\Theta(2^n)$ \\ 
\end{tabular}




\subsection{Decision Problems}\label{sec:decision_problems}
\begin{definition}[Problem]
A \emph{problem} $Q$ is a binary relation between a set $I$ of \emph{instances} and a set $S$ of \emph{solutions}.
\end{definition}

\begin{definition}[Concrete Problem]
A \emph{concrete problem} $Q$ is a problem where $I$ and $S$ are the set of binary strings $\{0,1\}^{\ast}$.
\end{definition}
For all practical purposes, instances and solutions can be encoded as binary strings (i.e. mapped into $\{0,1\}^{\ast}$).

\begin{definition}[Decision Problem]
A \emph{decision problem} $Q$ is a problem where the set of solutions is $S=\{0,1\}$.
\end{definition}
% We write $Q(x)=\tru$ when the correct answer is ``yes'' and $Q(x)=\fals$ otherwise.
\begin{example}[Primality Testing]
Input: the binary representation of an integer $n$.
Output: $\tru$ if $n$ is prime, else $\fals$.
\end{example}

% \subsection{Decision vs. Optimization Problems}\label{sec:decision_vs_optimization}

We focus on decision problems only.
\begin{itemize}
\item An optimization problem is \emph{at least as hard} as its corresponding decision problem
\begin{itemize}
\item having a solution to the optimization gives an immediate solution to the decision problem
\end{itemize}
\item An optimization problem is \emph{not much harder} than the corresponding decision problem
\begin{itemize}
\item having a solution to the decision problem does not give an immediate solution to the optimization problem
\item but we can typically use the decision problem as a subroutine in some kind of (binary) search to solve the corresponding optimization problem
\end{itemize}
\end{itemize}



\begin{example}[Shortest Path]\label{ex:shortest_path_decision}
% --- helper macros --------------------------------
% marks a coordinate inside math/paragraph text
\newcommand{\mymark}[1]{\tikz[remember picture,baseline]\coordinate (#1);}
% draws the blue “U-bracket” with the yellow label
\newcommand{\bluebracket}[3]{%
  \begin{tikzpicture}[remember picture,overlay]
    \coordinate (mid) at ($(#1)!0.5!(#2)$);
    \draw[blue] 
      (#1) -- ++(0,-0.25) -- ($(#2)+(0,-0.25)$) -- (#2);
    \node[fill=yellow!20,draw=yellow!40!gray,
          inner sep=2pt] at ($(mid)+(0,-0.55)$) {#3};
  \end{tikzpicture}%
}
% ---------------------------------------------------
\leavevmode
\begin{itemize}
  % -------------------------------------------------
  \item as an optimization problem:
  \[
    \mymark{Il}G=(V,E),v_{\text{source}},v_{\text{destination}}\mymark{Ir}
    \longrightarrow
    \mymark{Sl}(v_{\text{source}}, \ldots, v_{\text{destination}})\mymark{Sr}
  \]
  \bluebracket{Il}{Ir}{instance}
  \bluebracket{Sl}{Sr}{solution}
  \begin{itemize}[nosep]
    \item Input: a graph $G$, a source vertex, and a destination vertex
    \item Output: a sequence of vertexes
  \end{itemize}
  % -------------------------------------------------
  \item as a decision problem:
  \[
    \mymark{IDl}G=(V,E),v_{\text{start}},v_{\text{end}},\delta\mymark{IDr}
    \longrightarrow
    \mymark{SDl}1/0\mymark{SDr}
  \]
  \bluebracket{IDl}{IDr}{instance}
  \bluebracket{SDl}{SDr}{solution}
  \begin{itemize}[nosep]
    \item Input: a graph $G$, a start vertex, an end vertex, and a path length ($\delta$)
    \item Output: $1$ if there is a path of (at most) the given length, else $0$ \qedhere
  \end{itemize}
\end{itemize}
\end{example}


%––––––––––––––––––––––––––––––––––
\subsection{Complexity Classes}
\begin{definition}[Class P]
A concrete decision problem $Q$ is \emph{polynomial-time solvable} if there is a polynomial-time algorithm $A$ that solves it.
The complexity class P is the set of all concrete decision problems that are polynomial-time solvable:
\[
Q\in\text{P} \iff \exists \text{$A$ that solves $Q$ in polynomial time (worst case)} \qedhere
\]
\end{definition}
\begin{example}
Primality Testing (2002, by Agrawal and Bachelor students Kayal and Saxena), Context-free Language Parsing, Shortest-Path (decision variant).
\end{example}

\begin{definition}[Class NP]
A concrete decision problem $Q$ is \emph{polynomial-time verifiable} if there is a polynomial-time algorithm $A$ such that for every instance $x \in I$ that has a $\tru$ solution ($Q(x)=1$), there is a certificate $y$ of polynomial size $|y| = O(|x|^c)$ for some constant $c$ such that $A(x,y)=1$.
$A(x,y)$ verifies in  polynomial time that $y$ proves that $Q(x)=1$.
The  complexity class NP is the set of all concrete decision problems that are polynomial-time verifiable:
\[
Q\in\text{NP} \iff \exists \text{$A$ that verifies $Q$ in polynomial time (worst case)} \qedhere
\]
\end{definition}
\begin{example}
Longest-Path (decision variant), Vertex Cover, Satisfiability, Knapsack (decision variant).
\end{example}


\begin{theorem}\label{thm:PsubsetNP}
$\text{P}\subseteq\text{NP}$
\end{theorem}
\begin{proof}
Given a polynomial-time solver for a problem, construct a verifier that ignores the certificate and simply runs the solver.
\end{proof}

$\text{polynomial-time solvable} \implies \text{polynomial-time verifiable}$

$\text{polynomial-time verifiable} \overset{\text{?}}{\implies} \text{polynomial-time solvable}$

\begin{conjecture}[P vs. NP]\label{conj:PvNP}
$\text{P}=\text{NP}$? The prevailing opinion is $\text{P}\neq\text{NP}$.
\end{conjecture}


%––––––––––––––––––––––––––––––––––
\subsection{Polynomial-Time Reductions}
\begin{definition}[Polynomial-Time Reduction]
A problem $Q$ is \emph{polynomial-time reducible} to another problem $Q'$ if there is a polynomial-time reduction from $Q$ to $Q'$.
That means there is a polynomial-time algorithm that transforms every instance $q$ of $Q$ into an instance $q'$ of $Q'$ such that the solution to $q$ is $\tru$ if and only if the solution to $q'$ is $\tru$.
\end{definition}
\[
\begin{tikzpicture}[>=Stealth, thick]
    % top row ---------------------------------------------------------
    \node (qinst) at (-4,0) {{instance of $Q$}};
    \node[draw, rectangle, minimum width=2.6cm, minimum height=1cm] (qbox) at (0,0) {\Huge ?};
    \node (qsol) at (4,0) {{solution}};
    \draw[->, blue] (qinst) -- (qbox);
    \draw[->, blue, dotted] (qbox) -- (qsol);
    % bottom row ------------------------------------------------------
    \node (qpin)  at (-4,-3) {{instance of $Q'$}};
    \node[draw, rectangle, minimum width=2.6cm, minimum height=1cm] (qpbox) at (0,-3) {$A$};
    \node (qpsol) at (4,-3) {{solution}};
    \draw[->, blue] (qpin) -- (qpbox);
    \draw[->, blue] (qpbox) -- (qpsol);
    % reduction algorithm --------------------------------------------
    \node[draw, rectangle, align=center, minimum width=3cm, minimum height=1cm] (alg) at (-4,-1.5) {polynomial-time\\algorithm};
    \draw[->, blue] (qinst.south) -- (alg.north);
    \draw[->, blue] (alg.south) -- (qpin.north);
    % equivalence of solutions ---------------------------------------
    \draw[<->, blue] (qsol.south) -- (qpsol.north) node[midway,right, black] {$=$};
  \end{tikzpicture}
\]
\begin{example}[2-CNF-SAT]
When every clause of $\varphi$ has at most two literals, satisfiability can be tested via an implication graph and two runs of \nameref{alg:dfs}.  
A formula is no satisfiable if and only if $x_i \leadsto \neg x_i \leadsto x_i$ for some $i$.
Therefore $\text{2-CNF-SAT} \in \text{P}$.
\end{example}
% \[
%   \begin{tikzpicture}[>=Stealth, thick,scale=0.8]
%     % bounding box  ---------------------------------------------------
%     \draw[black, line width=1.5pt] (-4.5,-4) rectangle (2,0);
%     \node at (1,-1.6) {$A_Q$};
%     % instance, algorithms, solution ---------------------------------
%     \node (qin)  at (-2,1)  {{instance of $Q$}};
%     \node[draw, rectangle, align=center, minimum width=3cm, minimum height=1cm] (redalg) at (-2,-1.3) {polynomial-time\\algorithm};
%     \node[draw, rectangle, minimum width=2.6cm, minimum height=1cm] (abox)   at (0,-3) {$A$};
%     \node (sol)  at (3.5,-3) {{solution}};
%     % arrows ----------------------------------------------------------
%     \draw[->, blue] (qin) -- (redalg.north);
%     \draw[->, blue] (redalg.south) |- (abox.west);
%     \draw[->, blue] (abox.east) -- (sol);
%   \end{tikzpicture}
% \]

%––––––––––––––––––––––––––––––––––
\subsection{NP-Hardness and NP-Completeness}
\begin{definition}[NP-Hard]
A problem $Q'$ is NP-hard if all problems $Q \in \text{NP}$ are polynomial-time reducible to $Q'$.
\end{definition}

\begin{definition}[NP-Complete]
A problem $Q'$ is NP-complete if $Q' \in \text{NP}$ and $Q'$ is NP-hard.
\end{definition}
\begin{example}[Circuit Satisfiability (SAT)]
was the first problem proven to be NP-hard and, since $\text{SAT} \in \text{NP}$, it is also NP-complete.
Many problems were then proved to be NP-complete through polynomial reductions from SAT, for example Vertex Cover.
\end{example}

\begin{theorem}
If $Q'$ is NP-hard and polynomial-time reducible to $Q''$, then $Q''$ is NP-hard.
\end{theorem}
\begin{theorem}
If $Q'$ is NP-hard and polynomial-time solvable, then $\text{P}=\text{NP}$.
Most people believe there is no such $Q'$.
\end{theorem}





% A subset of NP are the NP-complete problems (NPC): Problems in NPC are as hard as any other problem in NP. 
% That means that any problem in NP can be reduced to any problem in NPC in polynomial time. 
% Consequently, any problem in NPC can be reduced to another problem in NPC in polynomial time. 
% This means that NPC problems can be used as indicators of simplicity for all NP problems: 
% If you can solve one NPC problem in polynomial time, then you can solve all of them. 
% (In fact, you have then shown $\mathrm{P}=\mathrm{NP}=\mathrm{NPC}$, which would be an overly surprising result that makes you very famous; 
% most people believe that the three classes P, NP, and NPC are all different from each other - no one has proven that so far, however.)


% I will ask if it is a problem is in a class p or np. 
% Answer is ALWAYS YES, and proof your answer. 
% (Otherwise you wouldn’t be able to prove it)

% Example: is the problem in p?, prove it. 
% Answer: yes.  
% So we have to show that there is an algorithm that solves the problem in polynomial time. 
% ALWAYS the proof is to write and Algorithm.

% If it is an np, means: poly-time verifiable. 
% We need problem instance + Solution “witness” for a yes answer $\to$ then, check it. 
% So, you dont need to show how so solve it!

% Example: exist a subset in A that sums s? 
% \Call{SubsetSum}{$A, s$}, $A= [3, 7, -1, 5, 2, 8]$, $s =12$. ? Prove it.

% Answer: Yes. So, WRITE An algorithm that verifies a solution, it can have another parameter as the \texttt{solution} 
% \Call{SubsetSumVerify}{$A, s, \texttt{solution}$}. we have to proof that that \texttt{solution} is a solution for the problem. 

% In this case:
% \begin{algorithm}[h]
% \caption{Subset-Sum Verification}\label{alg:subset_sum_verify}
% \begin{algorithmic}[1]
% \Function{SubsetSumVerify}{$A, s, \texttt{solution}$}
%   \If{$\sum(\texttt{solution})\neq s$}
%     \State \Return \fals
%   \EndIf
%   \If{not ($\texttt{solution}$ is a subset of $A$)}  
%     \State \Return \fals  \Comment{can be done in polynomial time}
%   \EndIf
%   \State \Return \tru
% \EndFunction
% \end{algorithmic}
% \end{algorithm}


















% \section{Reduction}
% In complexity theory we often show that a problem $Q'$ is \emph{just as hard} as another problem~$Q$ by providing a \emph{polynomial--time reduction} from $Q$ to~$Q'$.

% \begin{figure}[ht]
%   \centering
%   \begin{tikzpicture}[>=Stealth, thick]
%     % top row ---------------------------------------------------------
%     \node (qinst) at (-4,0) {\emph{instance of $Q$}};
%     \node[draw, rectangle, minimum width=2.6cm, minimum height=1cm] (qbox) at (0,0) {\Huge ?};
%     \node (qsol) at (4,0) {\emph{solution}};
%     \draw[->, blue] (qinst) -- (qbox);
%     \draw[->, blue, dotted] (qbox) -- (qsol);
%     % bottom row ------------------------------------------------------
%     \node (qpin)  at (-4,-3) {\emph{instance of $Q'$}};
%     \node[draw, rectangle, minimum width=2.6cm, minimum height=1cm] (qpbox) at (0,-3) {$A$};
%     \node (qpsol) at (4,-3) {\emph{solution}};
%     \draw[->, blue] (qpin) -- (qpbox);
%     \draw[->, blue] (qpbox) -- (qpsol);
%     % reduction algorithm --------------------------------------------
%     \node[draw, rectangle, align=center, minimum width=3cm, minimum height=1cm] (alg) at (-2,-1.5) {polynomial--time\\algorithm};
%     \draw[->, blue] (qbox.west |- alg.north) -- (alg.north);
%     \draw[->, blue] (alg.south) -- (qpin.east |- alg.south);
%     % equivalence of solutions ---------------------------------------
%     \draw[<->, blue] (qsol.south) -- (qpsol.north) node[midway,right] {$=$};
%   \end{tikzpicture}
%   \caption{Polynomial--time reduction from $Q$ to $Q'$}
%   \label{fig:reduction-basic}
% \end{figure}

% An instance $q$ of $Q$ is mapped to an instance $q'$ of $Q'$ by a polynomial--time algorithm, and $q$ is accepted \emph{iff} $q'$ is accepted.

% \section{Reduction by Solving a Simpler Problem}
% Suppose we already have a polynomial--time algorithm $A$ for~$Q'$.  Chaining the reduction with~$A$ yields a direct algorithm $A_Q$ for~$Q$.

% \begin{figure}[ht]
%   \centering
%   \begin{tikzpicture}[>=Stealth, thick]
%     % bounding box  ---------------------------------------------------
%     \draw[blue!70!black, line width=2pt] (-4,-4) rectangle (3,0.5);
%     \node at (1,-1.6) {$A_Q$};
%     % instance, algorithms, solution ---------------------------------
%     \node (qin)  at (-3.5,0)  {\emph{instance of $Q$}};
%     \node[draw, rectangle, align=center, minimum width=3cm, minimum height=1cm] (redalg) at (-2,-1.3) {polynomial--time\\algorithm};
%     \node[draw, rectangle, minimum width=2.6cm, minimum height=1cm] (abox)   at (0,-3) {$A$};
%     \node (sol)  at (3.2,-3) {\emph{solution}};
%     % arrows ----------------------------------------------------------
%     \draw[->, blue] (qin) -- ($(qin)+(1.2,0)$) |- (redalg.north);
%     \draw[->, blue] (redalg.south) |- (abox.west);
%     \draw[->, blue] (abox.east) -- (sol);
%   \end{tikzpicture}
%   \caption{Composing a reduction with a known algorithm~$A$}
%   \label{fig:reduction-compose}
% \end{figure}

% If $A$ runs in polynomial time, so does $A_Q$.  Hence, whenever $Q'\in\textsf P$ we also have $Q\in\textsf P$.

% \section{The 2--CNF--SAT Problem}
% \subsection*{Definition}
% \begin{itemize}
%   \item \textbf{Input:} a Boolean formula $f$ on variables $x_1,\dots,x_n$ in \emph{conjunctive normal form}
%         $f=C_1 \land C_2 \land \dots \land C_k$, where each clause $C_i$ has exactly two literals.
%   \item \textbf{Output:} $1$ \,(\emph{yes}) iff $f$ is satisfiable.
% \end{itemize}
% The formula
% \[
%   (x_1\lor\lnot x_3)\land(\lnot x_2\lor x_3)\land(\lnot x_1\lor\lnot x_3)\land(x_1\lor x_2)
% \]
% is an illustrative example.

% \section{From 2--CNF to Implicative Form}
% Every clause $(a\lor b)$ is equivalent to $(\lnot a\Rightarrow b)$ and $(\lnot b\Rightarrow a)$.  Rewriting each clause yields an \emph{implicative normal form}.

% \begin{align*}
%   & (x_1\lor\lnot x_3)\land(\lnot x_2\lor x_3)
%   \\
%   \equiv{} & (\lnot x_1\Rightarrow\lnot x_3)\land(x_3\Rightarrow x_1)\land(x_2\Rightarrow x_3)\land(\lnot x_3\Rightarrow\lnot x_2).
% \end{align*}

% \section{From Implications to Graph Reachability}
% Build a directed graph whose vertices are the literals $x_i$ and $\lnot x_i$.  For every implication $u\Rightarrow v$ add an arc $u\to v$.

% \begin{figure}[ht]
%   \centering
%   \begin{tikzpicture}[>=Stealth, node distance=2cm, thick]
%     % nodes -----------------------------------------------------------
%     \node[circle, draw] (x1)  {$x_1$};
%     \node[circle, draw, above right=of x1] (x2)  {$x_2$};
%     \node[circle, draw, right=3cm of x1]   (x3)  {$x_3$};
%     \node[circle, draw, below=3cm of x1] (nx1) {$\lnot x_1$};
%     \node[circle, draw, below right=of nx1] (nx2) {$\lnot x_2$};
%     \node[circle, draw, right=3cm of nx1] (nx3) {$\lnot x_3$};
%     % edges -----------------------------------------------------------
%     \foreach \u/\v in {nx1/nx3,x3/x1,x2/x3,nx3/nx2,x1/nx3,x3/nx1,nx1/x2,nx2/x1}
%       \draw[->] (\u) -- (\v);
%     % callout with condition -----------------------------------------
%     \node[rectangle callout, callout relative pointer={(1,0)}, draw, align=left, right=3.2cm of x3] (cond)
%       {Formula is \emph{unsatisfiable}\\iff\\$x_i\leadsto\lnot x_i\leadsto x_i$ for some~$i$};
%     \node[rectangle, draw, align=center, below=0.8cm of cond] (dfs) {depth--first search};
%     \draw[->, blue] (cond.south) -- (dfs.north);
%   \end{tikzpicture}
%   \caption{Implication graph for the running example}
%   \label{fig:implication-graph}
% \end{figure}

% Testing each variable for reachability to its negation (and back) can be done with a single DFS, so \textsc{2--CNF--Sat} is in~$\textsf P$.

% \section{Putting It Together: a Reduction to Reachability}
% \begin{figure}[ht]
%   \centering
%   \begin{tikzpicture}[>=Stealth, thick]
%     % top row ---------------------------------------------------------
%     \node (inst) at (-4,0)  {instance of 2--CNF--SAT};
%     \node[draw, rectangle, minimum width=2.6cm, minimum height=1cm] (unknown) at (0,0) {\Huge ?};
%     \node (sol)  at (4,0)  {\emph{solution}};
%     \draw[->, blue] (inst) -- (unknown);
%     \draw[->, blue, dotted] (unknown) -- (sol);
%     % reduction algo --------------------------------------------------
%     \node[draw, rectangle, align=center, minimum width=3cm, minimum height=1cm] (reducer) at (-2,-1.5) {polynomial--time\\algorithm};
%     \draw[->, blue] (unknown.west |- reducer.north) -- (reducer.north);
%     % bottom row ------------------------------------------------------
%     \node (ginst) at (-4,-3) {instance of reachability};
%     \node[draw, rectangle, minimum width=2.6cm, minimum height=1cm] (dfs) at (0,-3) {DFS};
%     \node (gsol) at (4,-3) {\emph{solution}};
%     \draw[->, blue] (reducer.south) -- (ginst);
%     \draw[->, blue] (ginst) -- (dfs);
%     \draw[->, blue] (dfs) -- (gsol);
%     % equal solutions -------------------------------------------------
%     \draw[<->, blue] (sol.south) -- (gsol.north) node[midway,right] {$=$};
%   \end{tikzpicture}
%   \caption{Reduction of \textsc{2--CNF--Sat} to graph reachability}
%   \label{fig:2cnf-reduction}
% \end{figure}

% Consequently $\textsc{2--CNF--Sat}\in\textsf P$.

% \section{NP--Completeness}
% A problem $Q$ is \emph{polynomial--time reducible} to $Q'$ if a polynomial--time algorithm transforms every instance $q$ of~$Q$ into an instance $q'$ of~$Q'$, preserving the answer.

% A problem $Q'$ is \emph{NP--hard} if every $Q\in\textsf{NP}$ reduces to~$Q'$.  If additionally $Q'\in\textsf{NP}$, then $Q'$ is \emph{NP--complete}.

% If some NP--hard problem were found to be in~$\textsf P$, we would have $\textsf P=\textsf{NP}$, which most researchers doubt.

% \section{The First NP--Complete Problem}
% \begin{figure}[ht]
%   \centering
%   \begin{tikzpicture}[>=Stealth, thick]
%     \node (Q)   at (-3,0) {\emph{any problem $Q\in\textsf{NP}$}};
%     \node[draw, rectangle, align=center] (red) at (0,0) {polynomial--time\\reduction};
%     \node (SAT) at (3,0) {\textsc{Sat}};
%     \draw[->, blue] (Q) -- (red);
%     \draw[->, blue] (red) -- (SAT);
%   \end{tikzpicture}
%   \caption{Cook's reduction showing that \textsc{Sat} is NP--hard}
%   \label{fig:Cook}
% \end{figure}

% Stephen Cook proved in 1971 that \textsc{Circuit Sat} (and soon after, \textsc{Sat}) is NP--complete.  Since then, countless other problems have been shown NP--complete by reductions, e.g.~from \textsc{Sat} to \textsc{Vertex Cover}.

% If you encounter an NP--hard problem, it is widely accepted that no efficient exact algorithm exists unless $\textsf P=\textsf{NP}$.  In practice one resorts to heuristics, approximation algorithms, or fixed--parameter tractability.








% \section*{Exam Exercises}

% \subsection*{294}
% \begin{algorithm}[h]
%   \caption*{minimal number of edges to make an undirected graph connected}%
%   \label{alg:min_edges_connected}
%   \begin{algorithmic}[1]
%     \Function{MinimalAdditionalEdges}{$G$}
%     \State $X \gets [\fals] * |V(G)|$ \Comment{visited vertices}
%       \State $n_C \gets 0$ \Comment{number of connected components}
      
%       \For{$i \gets 1 \ldots |V(G)|$}
%         \If{$X[i] = \fals$}
%           \State $n_C \gets n_C + 1$
%           \State initialize an empty queue $Q$
%           \State \Call{Enqueue}{$Q, i$}
%           \State $X[i] \gets \tru$          \Comment{mark source vertex as visited}
          
%           \While{$Q \neq \emptyset$}
%             \State $u \gets$ \Call{Dequeue}{$Q$}
%             \ForAll{$v \in \Gamma(u)$}
%               \If{$X[v] = \fals$} 
%                 \State \Call{Enqueue}{$Q, v$} \Comment{mark on enqueue to avoid duplicates}
%                 \State $X[v] \gets \tru$
%               \EndIf
%             \EndFor
%           \EndWhile
%         \EndIf
%       \EndFor
      
%       \State \Return $n_C - 1$ 
%     \EndFunction
%   \end{algorithmic}
% \end{algorithm}

% \subsection*{298}
% \begin{algorithm}[h]
%   \caption*{Checking for 3 distinct values in an array}\label{alg:distinct_values}
%   \begin{algorithmic}[1]
%     \Function{DistinctValues}{$A$}
%       \State $a_1, a_2, a_3 \gets \nil$ \Comment{the three distinct values}
%       \ForAll{$a\in A$}
%         \If{$a = a_1 \lor a_1 = \nil$}
%           \If{$a_1 = \nil$}
%             \State $a_1 \gets a$
%           \EndIf
%           \State \textbf{continue}
%         \EndIf
%         \If{$a = a_2 \lor a_2 = \nil$}
%           \If{$a_2 = \nil$}
%             \State $a_2 \gets a$
%           \EndIf
%           \State \textbf{continue}
%         \EndIf
%         \If{$a = a_3 \lor a_3 = \nil$}
%           \If{$a_3 = \nil$}
%             \State $a_3 \gets a$
%           \EndIf
%           \State \textbf{continue}
%         \EndIf
%         \State \Return \texttt{False} 
%       \EndFor
%       \State \Return \texttt{True} 
%     \EndFunction
%   \end{algorithmic}
% \end{algorithm}



% 

\end{document}
