% author: Fabian Bosshard % © CC BY 4.0

\documentclass[9pt, headings=standardclasses, parskip=half]{scrartcl}
\usepackage{ifthen}
\usepackage{iftex}
\usepackage{csquotes}

\usepackage[T1]{fontenc}     % handle accented glyphs properly
\usepackage[english]{babel}  % load the hyphenation patterns you need



\usepackage[automark]{scrlayer-scrpage}
% \clearpairofpagestyles
% \ofoot{\pagemark} % für ein einseitiges Dokument: \ofoot platziert den Inhalt in der äußeren Fußzeile (unten rechts)
\pagestyle{scrheadings}

% \renewcommand{\familydefault}{\sfdefault} % sans serif font for text (math font is still serif)

\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{soulutf8}
\usepackage{xparse}

\ExplSyntaxOn
\tl_new:N \__l_SOUL_argument_tl
\cs_set_eq:Nc \SOUL_start:n { SOUL@start }
\cs_generate_variant:Nn \SOUL_start:n { V }
\cs_set_protected:cpn {SOUL@start} #1
 {
  \tl_set:Nn \__l_SOUL_argument_tl { #1 }
  \regex_replace_all:nnN
   { \c{\(} (.*?) \c{\)} } % look for \(...\) (lazily)
   { \cM\$ \1 \cM\$ }      % replace with $...$
   \__l_SOUL_argument_tl
  \SOUL_start:V \__l_SOUL_argument_tl % do the usual
 }
\ExplSyntaxOff

% save soul's \hl under a private name
\let\SOULhl\hl
\renewcommand{\hl}[1]{\SOULhl{#1}} % keep plain \hl working if you want

% punchy highlighter colors
\definecolor{HLgreen}{HTML}{77DD77}
\definecolor{HLyellow}{HTML}{FFFF66}
\definecolor{HLorange}{HTML}{FFB347}
\definecolor{HLred}{HTML}{FF6961}
\definecolor{HLpink}{HTML}{FFB6C1}
\definecolor{HLturquoise}{HTML}{40E0D0}



% master highlight macro: \hl[level]{text}, default = green
\RenewDocumentCommand{\hl}{O{1} m}{%
  \begingroup
  \IfEqCase{#1}{%
    {1}{\sethlcolor{HLgreen}\SOULhl{#2}}%
    {2}{\sethlcolor{HLyellow}\SOULhl{#2}}%
    {3}{\sethlcolor{HLorange}\SOULhl{#2}}%
    {4}{\sethlcolor{HLred}\SOULhl{#2}}%
    {5}{\sethlcolor{red}\SOULhl{#2}}%
    {6}{\sethlcolor{HLturquoise}\SOULhl{#2}}%
  }[\PackageError{hl}{Undefined highlight level: #1}{}]%
  \endgroup
}








% \usepackage[left=20mm, right=20mm, top=20mm, bottom=30mm]{geometry}
% \usepackage[left=25mm, right=25mm, top=20mm, bottom=30mm]{geometry}
% \usepackage[left=30mm, right=30mm, top=20mm, bottom=30mm]{geometry}
% \usepackage[left=20mm, right=60mm, top=20mm, bottom=30mm]{geometry}
\usepackage[left=38mm, right=38mm, top=20mm, bottom=30mm]{geometry}


\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{mathdots} % without this package, only \vdots and \ddots are taken from the text font (not the math font), which looks bad if the text font is different from the math font
\usepackage{accents}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Kern}{kern}
\DeclareMathOperator{\Trace}{trace}
\DeclareMathOperator{\Rank}{rank}

\usepackage{enumitem}
\renewcommand{\labelitemi}{\textbullet}
\renewcommand{\labelitemii}{\raisebox{0.1ex}{\scalebox{0.8}{\textbullet}}}
\renewcommand{\labelitemiii}{\raisebox{0.2ex}{\scalebox{0.6}{\textbullet}}}
\renewcommand{\labelitemiv}{\raisebox{0.3ex}{\scalebox{0.4}{\textbullet}}}

\usepackage{caption, subcaption}

\usepackage[backend=biber,style=numeric]{biblatex}
\addbibresource{\jobname.bib}

\usepackage{pifont}


\usepackage{tikz}
\usetikzlibrary{arrows, arrows.meta, shapes, positioning, calc, fit, patterns, intersections, math, 3d, tikzmark, decorations.pathreplacing, decorations.markings}
\usepackage{tikz-dependency, tikz-qtree, tikz-qtree-compat}
\usepackage{tikz-3dplot}
\usepackage{tikzpagenodes}
% \usetikzlibrary{external}
% \tikzexternalize[prefix=tikz/]
\usepackage{pgfplots}


% define colors
\definecolor{funblue}{rgb}{0.10, 0.35, 0.66}
\definecolor{alizarincrimsonred}{rgb}{0.85, 0.17, 0.11}
\definecolor{amethyst}{rgb}{0.6, 0.4, 0.8}
\definecolor{mypurple}{rgb}{128, 0, 128} 
\definecolor{highlightpurple}{rgb}{0.6, 0.0, 0.6}
% \renewcommand{\emph}[1]{\textcolor{highlightpurple}{#1}}
% \renewcommand{\emph}[1]{\textsl{#1}}
\renewcommand{\emph}[1]{\textcolor{highlightpurple}{\textsl{#1}}}

\usepackage{thmtools}

\newlength{\thmspace}
\setlength{\thmspace}{3pt plus 1pt minus 1pt}

% ===== main assertion-style family (Theorem, Lemma, etc.) =====

\declaretheoremstyle[
  headfont=\bfseries,
  bodyfont=\normalfont,
  spaceabove=\thmspace,
  spacebelow=\thmspace,
  qed=\ensuremath{\vartriangleleft},
  postheadspace=1em
]{assertionstyle}

\declaretheorem[
  style=assertionstyle,
  name=Theorem,
  numberwithin=section   % <-- resets in each section
]{theorem}

\declaretheorem[style=assertionstyle, name=Lemma,       sibling=theorem]{lemma}
\declaretheorem[style=assertionstyle, name=Corollary,   sibling=theorem]{corollary}
\declaretheorem[style=assertionstyle, name=Proposition, sibling=theorem]{proposition}
\declaretheorem[style=assertionstyle, name=Conjecture,  sibling=theorem]{conjecture}
\declaretheorem[style=assertionstyle, name=Claim,       sibling=theorem]{claim}
\declaretheorem[style=assertionstyle, name=Fact,        sibling=theorem]{fact}


% ===== definitions =====

\declaretheoremstyle[
  headfont=\bfseries,
  bodyfont=\normalfont,
  spaceabove=\thmspace,
  spacebelow=\thmspace,
  qed=\ensuremath{\blacktriangleleft},
  postheadspace=1em
]{definitionstyle}

\declaretheorem[style=definitionstyle, name=Definition, numberwithin=section]{definition}
\declaretheorem[style=definitionstyle, name=Problem,    sibling=definition]{problem}

% ===== exercises, solutions =====
\declaretheoremstyle[
  headfont=\bfseries,
  bodyfont=\normalfont,
  spaceabove=\thmspace,
  spacebelow=\thmspace,
  qed=\ding{45},
  postheadspace=1em
]{exercisestyle}
\declaretheorem[style=exercisestyle, name=Exercise, numberwithin=section]{exercise}
\declaretheoremstyle[
  headfont=\bfseries\color{red},
  bodyfont=\normalfont,
  spaceabove=\thmspace,
  spacebelow=\thmspace,
  qed=\ensuremath{\color{red}\blacktriangleleft},
  postheadspace=1em
]{solutionstyle}
\declaretheorem[style=solutionstyle, name=Solution, numbered=no]{solution}

% ===== proofs =====

\declaretheoremstyle[
  headfont=\bfseries,
  bodyfont=\normalfont,
  spaceabove=6pt,
  spacebelow=6pt,
  qed=\ensuremath{\square},
  postheadspace=1em
]{proofstyle}

\let\proof\relax
\let\endproof\relax
\declaretheorem[style=proofstyle, name=Proof,    numbered=no]{proof}


% ===== remarks, cautions, examples =====

\declaretheoremstyle[
  headfont=\bfseries,
  bodyfont=\normalfont\normalsize,
  spaceabove=\thmspace,
  spacebelow=\thmspace,
  qed=\ensuremath{\blacktriangleleft},
  postheadspace=1em
]{remarkstyle}

\declaretheorem[style=remarkstyle, name=Remark,   numberwithin=section]{remark}

\declaretheoremstyle[
  headfont=\bfseries\color{funblue},
  bodyfont=\normalfont\normalsize,
  spaceabove=\thmspace,
  spacebelow=\thmspace,
  qed=\ensuremath{\color{funblue}\blacktriangleleft},
  postheadspace=1em
]{examplestyle}

\declaretheorem[style=examplestyle, name=Example, sibling=remark]{example}


\declaretheoremstyle[
  headfont=\color{alizarincrimsonred}\bfseries,
  bodyfont=\normalfont\normalsize,
  spaceabove=\thmspace,
  spacebelow=\thmspace,
  qed=\ensuremath{\color{alizarincrimsonred}\blacktriangleleft},
  postheadspace=1em
]{cautionstyle}

\declaretheorem[style=cautionstyle, name=Caution, sibling=remark]{caution}

\declaretheoremstyle[
  headfont=\bfseries,
  bodyfont=\normalfont\footnotesize,
  spaceabove=\thmspace,
  spacebelow=\thmspace,
  postheadspace=1em
]{smallremarkstyle}

\declaretheorem[style=smallremarkstyle, name=Remark, sibling=remark]{smallremark}


\declaretheoremstyle[
  headfont=\bfseries\color{amethyst},
  bodyfont=\normalfont,
  spaceabove=6pt,
  spacebelow=6pt,
  qed=\ensuremath{\color{amethyst}\blacktriangleleft},
  postheadspace=1em
]{digressionstyle}

\declaretheorem[style=digressionstyle, name=Digression, sibling=remark]{digression}


\numberwithin{equation}{section} % equations numbered within sections






\newenvironment{verticalhack}
  {\begin{array}[b]{@{}c@{}}\displaystyle}
  {\\\noalign{\hrule height0pt}\end{array}} % to make qed symbol aligned


\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[italicComments=false]{algpseudocodex} % macht probleme mit TikZ externalization

\newcommand*{\algorithmautorefname}{Algorithm}

% commands for functions etc.
\newcommand{\im}{\operatorname{Im}}

% commands for vectors and matrices
\newcommand*\matrspace{0.8mu}
\newcommand{\matr}[1]{%
  \mspace{\matrspace}%
  \underline{%
    \mspace{-\matrspace}%
    \smash[b]{\boldsymbol{#1}}%
    \mspace{-\matrspace}%
  }%
  \mspace{\matrspace}%
}

\newcommand{\vect}[1]{{\boldsymbol{#1}}}

% commands for derivatives
\newcommand{\dif}{\mathrm{d}}

% commands for number sets
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\F}{\mathbb{F}}

% commands for probability
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Exp}{\operatorname{E}}
\newcommand{\Prob}{\operatorname{P}}
\newcommand{\numof}{\ensuremath{\# \,}} % number of elements in a set

% algorithm helpers
\algnewcommand{\TO}{, \ldots ,}
\algnewcommand{\DOWNTO}{, \ldots ,}
\algnewcommand{\OR}{\vee}
\algnewcommand{\AND}{\wedge}
\algnewcommand{\NOT}{\neg}
\algnewcommand{\LEN}{\operatorname{len}}
\algnewcommand{\tru}{\ensuremath{\mathrm{\texttt{true}}}}
\algnewcommand{\fals}{\ensuremath{\mathrm{\texttt{false}}}}
\algnewcommand{\append}{\circ}

\algnewcommand{\nil}{\ensuremath{\mathrm{\textsc{nil}}}}

\newcommand{\attrib}[2]{\ensuremath{#1\mathtt{.}\mathtt{#2}}} % object.field with field in math typewriter (like code)
\newcommand{\attribnormal}[2]{\ensuremath{#1\mathtt{.}#2}} 

\title{Natural Language Processing}
\author{Fabian Bosshard}
\date{\today}

\usepackage[
  linktoc=none,
  pdfauthor={Fabian Bosshard},
  pdftitle={USI - Natural Language Processing - summary},
  pdfkeywords={USI, natural language processing, nlp, course notes, informatics},
  colorlinks=false,  
  pdfborder={0.0 0.0 0.0},      
  linkbordercolor={0 0.6 1},      % internal links
  urlbordercolor={0 0.6 1},       % URLs
  citebordercolor={0 0.6 1}       % citations
]{hyperref}

\DeclareTOCStyleEntry[linefill=\dotfill]{tocline}{section}
\DeclareTOCStyleEntry[linefill=\dotfill]{tocline}{subsection}
\DeclareTOCStyleEntry[linefill=\dotfill]{tocline}{subsubsection}
\makeatletter
\newlength\FB@toclinkht
\newlength\FB@toclinkdp
\setlength\FB@toclinkht{.80\ht\strutbox}
\setlength\FB@toclinkdp{.40\dp\strutbox}

\let\FB@orig@contentsline\contentsline
\renewcommand*\contentsline[4]{%
  \begingroup
    \Hy@safe@activestrue
    \edef\Hy@tocdestname{#4}%
    \FB@orig@contentsline{#1}{%
      \Hy@raisedlink{\hyper@anchorstart{toc:\Hy@tocdestname}\hyper@anchorend}%
      \leavevmode
      \rlap{%
        \hyper@linkstart{link}{\Hy@tocdestname}%
          \raisebox{0pt}[\FB@toclinkht][\FB@toclinkdp]{%
            \hbox to \dimexpr\hsize-\parindent\relax{\hfil}%
          }%
        \hyper@linkend
      }%
      #2%
    }{#3}{#4}%
  \endgroup
}

\let\FB@orig@sectionlinesformat\sectionlinesformat
\renewcommand{\sectionlinesformat}[4]{%
  \ifx\@currentHref\@empty
    \FB@orig@sectionlinesformat{#1}{#2}{#3}{#4}%
  \else
    \leavevmode
    \hyper@linkstart{link}{toc:\@currentHref}%
      \@hangfrom{\hskip #2\relax #3}{#4}%
    \hyper@linkend
    \par
  \fi
}
\makeatother

\usepackage[
  type     = {CC},
  modifier = {by},
  version  = {4.0},
]{doclicense}
\usepackage{cleveref}

\makeatletter
\renewcommand{\theHequation}{\thesection.\arabic{equation}}
\renewcommand{\theHtheorem}{\thesection.\arabic{theorem}}
\renewcommand{\theHlemma}{\theHtheorem}
\renewcommand{\theHcorollary}{\theHtheorem}
\renewcommand{\theHproposition}{\theHtheorem}
\renewcommand{\theHconjecture}{\theHtheorem}
\renewcommand{\theHclaim}{\theHtheorem}
\renewcommand{\theHfact}{\theHtheorem}

\renewcommand{\theHdefinition}{\thesection.\arabic{definition}}
\renewcommand{\theHproblem}{\theHdefinition}

\renewcommand{\theHexercise}{\thesection.\arabic{exercise}}

\renewcommand{\theHremark}{\thesection.\arabic{remark}}
\renewcommand{\theHexample}{\theHremark}
\renewcommand{\theHcaution}{\theHremark}
\renewcommand{\theHsmallremark}{\theHremark}
\renewcommand{\theHdigression}{\theHremark}
\makeatother


\begin{document}

\maketitle

\tableofcontents

\clearpage











% --- Slide 1 ---------------------------------------------------------------
\section{Useful formulae} % Slide 1

\textbf{Conditional probability:}
Let $(\Omega, \mathcal{A}, \Prob)$ be a probability space. Recall that the conditional probability of an $A \in \mathcal{A}$ given $B \in \mathcal{A}$ is defined as
\begin{equation}\label{eq:conditional-probability}
\Prob(A \mid B)
:= 
\begin{cases}
\frac{\Prob(A \cap B)}{\Prob(B)} & \Prob(B)>0 \\ 
0 & \Prob(B)=0
\end{cases}
\end{equation}


\begin{theorem}[Chain rule of probability]
For any events \(A_1, \ldots, A_n\): %whose intersection has not probability zero
\begin{equation}\label{eq:chain-rule-probability}
\Prob\left(\bigcap_{i=1}^n A_i\right)
=
\prod_{k=1}^n \Prob\left(A_k \mid \bigcap_{j=1}^{k-1} A_j\right)
\end{equation}
e.g. with 4 variables:
\(
  \Prob(A,B,C,D)
  = \Prob(A)\,\Prob(B\mid A)\,\Prob(C\mid A,B)\,\Prob(D\mid A,B,C)
\).
\end{theorem}
\begin{proof}
Apply \autoref{eq:conditional-probability} repeatedly.
% \[
% \begin{aligned}
% \Prob\left(A_1 \cap A_2 \cap \ldots \cap A_n\right) & =\Prob\left(A_n \mid A_1 \cap \ldots \cap A_{n-1}\right) \Prob\left(A_1 \cap \ldots \cap A_{n-1}\right) \\
% & =\Prob\left(A_n \mid A_1 \cap \ldots \cap A_{n-1}\right) \Prob\left(A_{n-1} \mid A_1 \cap \ldots \cap A_{n-2}\right) \Prob\left(A_1 \cap \ldots \cap A_{n-2}\right) \\
% & =\Prob\left(A_n \mid A_1 \cap \ldots \cap A_{n-1}\right) \Prob\left(A_{n-1} \mid A_1 \cap \ldots \cap A_{n-2}\right) \cdot \ldots \cdot \Prob\left(A_3 \mid A_1 \cap A_2\right) \Prob\left(A_2 \mid A_1\right) \Prob\left(A_1\right) \\
% & =\Prob\left(A_1\right) \Prob\left(A_2 \mid A_1\right) \Prob\left(A_3 \mid A_1 \cap A_2\right) \cdot \ldots \cdot \Prob\left(A_n \mid A_1 \cap \cdots \cap A_{n-1}\right) \\
% & =\prod_{k=1}^n \Prob\left(A_k \mid A_1 \cap \cdots \cap A_{k-1}\right) \\
% & =\prod_{k=1}^n \Prob\left(A_k \mid \bigcap_{j=1}^{k-1} A_j\right) 
% \end{aligned}
% \]
\end{proof}

% \begin{figure}[tbp]
%   \centering
% %   \includegraphics[width=\linewidth]{Screenshot 2026-01-17 alle 09.53.08.png}
%   \caption{Overview of the models analysed up to transformers.}
%   \label{fig:models-overview}
% \end{figure}


Cosine similarity between two vectors $\vect{A}$ and $\vect{B}$:
\begin{equation}\label{eq:cosine-similarity}
\text{similarity} = \cos(\angle(\vect{A}, \vect{B})) = \frac{\vect{A} \cdot \vect{B}}{\|\vect{A}\| \|\vect{B}\|} = \frac{\sum_{i=1}^n A_i B_i}{\sqrt{\sum_{i=1}^n A_i^2} \sqrt{\sum_{i=1}^n B_i^2}}
\end{equation}






\clearpage
% --- Slide 2 ---------------------------------------------------------------
\section{Text Classification} % Slide 2

\subsection{Text preprocessing}

Prior to tokenization:
\begin{itemize}
  \item remove markup (non-content information), e.g.\ \texttt{<HTML>} tags
  \item lowercase text to reduce vocabulary size (can lose information: e.g. "WHO" vs "who")
  \item remove punctuation
\end{itemize}

\medskip

After tokenization:
\begin{itemize}
  \item remove stopwords\footnote{high-frequency terms in language, e.g. "the", "is", "and"} (convey little information about the topic of a text)
  \item remove low-frequency words (since insufficient information present to determine correlation between word presence and class)
\end{itemize}

\medskip
less common activities:
\begin{itemize}
  \item stemming or lemmatization to reduce vocabulary size
  \item spelling correction
\end{itemize}

\medskip

ASCII encoding keyboard $\to [1,128]$

UTF-8 encoding very vast (160 languages, 149k Unicode characters)

\begin{definition}[Morpheme]
Smallest linguistic unit that has semantic meaning.
\end{definition}

\subsection{Bag of words}
Vocabulary of document provides most important signal; \# occurrences of words provides further information.

Bag-of-words model:
\begin{itemize}
  \item represent documents as vectors of word counts
  \item sparse representation (long vector with many zeros)
  \item completely ignores word order
  \item extension to include $n$-grams can increase performance: but greatly increases number of dimensions, so more data is then needed
\end{itemize}

usually have far fewer documents than vocabulary terms, which means fewer examples than features
\begin{itemize}
  \item means multiple settings of parameters can explain observed class labels
  \item strong regularization needed to prevent overfitting
\end{itemize}

\subsubsection{Heaps' law}
Vocabulary grows with approximately the square root of document/collection length:
\[
\boxed{
  V(l) \propto l^{\beta}, \quad \beta \approx 0.5
}
\]

\subsubsection{Zipf's law}
Define as the collection term frequency \(\operatorname{ctf}_t\) of a token \(t\) (``token frequency'') as the number of times it occurs in the entire corpus:
\[
  \operatorname{ctf}_t := \sum_{d \in \mathcal{D}} \operatorname{count}(t,d)
\]

Token frequency approximately proportional to the inverse of its rank:
\[
\boxed{
  \operatorname{ctf}_t \propto \frac{1}{\operatorname{rank}(t)^s}, \quad s \approx 1
}
\]
Most common word occurs approximately twice as often as the next common one, three times as often as the third most common, and so on.

\subsection{Evaluating text classifiers}

\subsubsection{Evaluating a Binary Text Classifier}

First thing to investigate is the confusion matrix:
\definecolor{myblue}{RGB}{0,90,135}
\definecolor{mygreen}{RGB}{205,232,170}
\definecolor{myred}{RGB}{245,135,135}
\definecolor{framegray}{gray}{0.65}
\[
\begin{tikzpicture}[font=\footnotesize, line cap=round, line join=round, scale=0.35]

% ---- sizes ----
\def\cw{6.2} % cell width
\def\ch{3.4} % cell height

% ---- 2x2 confusion matrix (bottom-left at (0,0)) ----
% fills
\fill[myred]   (0,0) rectangle (\cw,\ch);               % FP
\fill[mygreen] (\cw,0) rectangle (2*\cw,\ch);           % TN
\fill[mygreen] (0,\ch) rectangle (\cw,2*\ch);           % TP
\fill[myred]   (\cw,\ch) rectangle (2*\cw,2*\ch);       % FN

% border + inner lines
\draw[line width=1pt] (0,0) rectangle (2*\cw,2*\ch);
\draw[line width=1pt] (\cw,0) -- (\cw,2*\ch);
\draw[line width=1pt] (0,\ch) -- (2*\cw,\ch);

% cell text
\node[text=myblue, align=center] at (0.5*\cw,1.5*\ch) {TP: true\\positives};
\node[text=myblue, align=center] at (1.5*\cw,1.5*\ch) {FN: false\\negatives};
\node[text=myblue, align=center] at (0.5*\cw,0.5*\ch) {FP: false\\positives};
\node[text=myblue, align=center] at (1.5*\cw,0.5*\ch) {TN: true\\negatives};

% ---- headings ----
\node[text=myblue] at (\cw,2*\ch+2.2) {PREDICTED CLASS};
\node[text=myblue] at (0.5*\cw,2*\ch+0.95) {Pos};
\node[text=myblue] at (1.5*\cw,2*\ch+0.95) {Neg};

% ---- side labels ----
\node[text=myblue, align=center] at (-3.6,\ch) {TRUE\\CLASS};
\node[text=myblue, align=center] at (-1.05,1.5*\ch) {Pos};
\node[text=myblue, align=center] at (-1.05,0.5*\ch) {Neg};

% ---- outer frame ----
\draw[framegray, line width=1.1pt]
  (-6.0,-1.0) rectangle (2*\cw+1.0,2*\ch+3.0);

\end{tikzpicture}
\]

\begin{itemize}
  \item Accuracy = \% of correct predictions:
  \[
  \text{Accuracy}
  =
    \frac{\text{TN}+\text{TP}}{\text{TN}+\text{TP}+\text{FN}+\text{FP}}
  \]
  \item Precision = \% of positive predictions that were correct:
  \[
  \text{Precision}
  =
    \frac{\text{TP}}{\text{TP}+\text{FP}}
  \]
  \item Recall = \% of positive instances that were found by model:
  \[
    \text{Recall}
    =
    \frac{\text{TP}}{\text{TP}+\text{FN}}
  \]
  \item F-measure = harmonic mean of precision and recall:
  \[
    F_{1}
    =
    \frac{2}{\frac{1}{\text{Precision}}+\frac{1}{\text{Recall}}}
    % =
    % \frac{2\cdot \text{Precision}\cdot \text{Recall}}{\text{Precision}+\text{Recall}}
  \]
  \item AUC = area under the ROC curve is often used as a single measure that doesn't depend on the confidence threshold used to make predictions with the model
\end{itemize}


\subsubsection{Evaluating multi-class classifiers}
if there are \(n\) classes,
\begin{itemize}
  \item confusion matrix will be $n\times n$
  \item by considering it positive class in a one-vs-all setting
  \item combine each class’s precision and recall values into a single measure:
  \begin{itemize}
    \item \textbf{macro-average}: average over classes weighting each class the same
    \item \textbf{micro-average}: average over classes weighting each class by the number of datapoints in it
  \end{itemize}
\end{itemize}


\clearpage
% --- Slides 3 ---------------------------------------------------------------
\section{Text search and clustering} 

\subsection{tf-idf: term frequency - inverse document frequency}
\begin{definition}[inverse document frequency]\label{def:idf}
\begin{equation}\label{eq:inverse-document-frequency}
\operatorname{idf}_t
=
\log\frac{N}{ \operatorname{df}_{t}}
\end{equation}
where $\operatorname{df}_{t}$ is the number of documents in the corpus which contains $t$ (document frequency), 
while $N$ is the number of documents in total (corpus size). 
\end{definition}
high idf $\Rightarrow$ few documents matching;  $\operatorname{idf}_t=0$ \(\Rightarrow\) \(t\) is in every document ($\log 1 = 0$).

\begin{definition}[term frequency]
\begin{equation}\label{eq:term-frequency}
\operatorname{tf}_{t,d}
=
\begin{cases}
  1+\log_{10}(\operatorname{count}(t,d))  &  \operatorname{count}(t,d) > 0 \\
    0                                       & \text{otherwise}
\end{cases}
\end{equation}
\end{definition}
a term \(t\) which occur 0 times in a document \(d\) has \(\operatorname{tf}_{t,d} = 0\);
if it occurs once, \(\operatorname{tf}_{t,d} = 1 + \log_{10}(1) = 1\);
if it occurs 10 times, \(\operatorname{tf}_{t,d} = 1 + \log_{10}(10) = 2\); and so on.


\begin{definition}[tf-idf]
\begin{equation}\label{eq:tf-idf}
\operatorname{tf-idf}(t,d) = \operatorname{tf}_{t,d} \cdot \operatorname{idf}_t
\end{equation}
\end{definition}

\begin{definition}[BM25]
goto method for term-based text retrieval...
\[
  \sum_{t\in q} \operatorname{idf}_t\;
  \frac{\operatorname{tf}_{t,d}}{\operatorname{tf}_{t,d}+k\Bigl(1-b+b\frac{\lvert d\rvert}{d_{\operatorname{avg}}}\Bigr)}
\]
\begin{itemize}
  \item $t$ terms in $q$ query
  \item $k$ $\to$ adjusts balance between term frequency and IDF
  \item $b$ $\to$ controls the importance of document length normalization
%   \item typical $k_1=1.2$ and $b=0.75$
  \item $|d|$ is length of the document; $d_{\operatorname{avg}}$ is the average on the document collection
\qedhere
\end{itemize}
\end{definition}


\subsubsection{Scoring documents}
% Vector interpretation: $\vect v=\{w_{1},\dots,w_{n}\}$ where $w_{i}$ is importance of term $i$ (BM25).
% Then you do a query vector $\vect q$ which most likely has $1$ for terms it has and $0$ for terms it doesn't have (BM25 of each term).
% Then compute cosine:
Vector space model treates \(\operatorname{tf-idf}\) values for all terms in document as a vector representation:
\[
\vect{d} = \bigl[\operatorname{tf-idf}(t_1,d), \operatorname{tf-idf}(t_2,d), \ldots, \operatorname{tf-idf}(t_n,d)\bigr]
\]

similarity between query vector \(\vect{q}\) and document vector \(\vect{d}\) computed using cosine similarity:
\[
\operatorname{score}(q, d)
=
  \cos(\vect q,\vect d)
  = 
  \frac{\vect q}{\lVert \vect q\rVert}\cdot \frac{\vect d}{\lVert \vect d\rVert}
\]

% \subsection{Crawlers}
% A crawler starts from one or more initial web pages, often called seed URLs. 
% It retrieves the content of those pages, analyses them to find hyperlinks inside, then visits those linked pages, and then the links inside them, and so on. 
% In this way it explores the web graph. It's used to compute search engine scores etc.

\subsection{Evaluating search engines}
\begin{itemize}
  \item precision at depth $k$: $P@k = (\#\text{ relevant docs in top \(k\)})/k$
  \item recall at depth $k$: $R@k = (\#\text{ relevant docs in top \(k\)})/(\#\text{ relevant docs in total})$
  \item F-measure at depth $k$:
  \[
    F_{1}@k
    =
    \frac{1}{\bigl((1/P@k+1/R@k)/2\bigr)}
  \]
  \item AveP = MAP: mean average precision (estimate area under precision/recall curve)
  \[
    \operatorname{AveP}
    =
    \frac{\sum_{k=1}^{n} P@k\,\operatorname{rel}(k)}{\#\text{ relevant docs}}
  \]
  \begin{itemize}
    \item bool $\operatorname{rel}(k)=1$ if document at position $k$ is relevant
  \end{itemize}
\end{itemize}



% --- Slide 4 ---------------------------------------------------------------
\section{Language models and word embeddings} % Slide 4


\subsection{Markov model}
\[
  \Prob(\text{croquet}\mid \text{play})
  = \frac{N(\text{play croquet})}{N(\text{play})},
  \qquad
  \Prob(\text{croquet}\mid \text{to play})
  = \frac{N(\text{to play croquet})}{N(\text{to play})}
  \]
  
Longer \(n\)-grams give better prediction but also less chance of finding the exact same sequence.

Smoothing: add a small constant to all counts before estimating probabilities, e.g. Laplace (with bigram assumption):
\[
\Prob_\text{Laplace}(w_n \mid w_{n-1}) 
=
\frac{\operatorname{count}(w_{n-1} \, w_n) + 1}
     {\sum_w (\operatorname{count}(w_{n-1} \, w) + 1)}
=
\frac{\operatorname{count}(w_{n-1} \, w_n) + 1}
     {\operatorname{count}(w_{n-1}) + V} 
\]


Backoff: if an $n$-gram is unknown, use $(n-1)$-gram

Interpolate between $n$-grams, e.g. trigram-bigram-unigram:
\[
\Prob(w_n\mid w_{n-1} \, w_{n-2})
=
  \lambda_{1}\Prob(w_n\mid w_{n-1} \, w_{n-2})
  +
  \lambda_{2}\Prob(w_n\mid w_{n-1})
  +
  \lambda_{3}\Prob(w_n),
  \qquad
  \sum_{i}\lambda_{i}=1
\]
choose lambdas that maximize probability on held-out \textbf{development} data

\subsection{Evaluating}

\textbf{extrinsic}: use model in a downstream task (e.g. as a spelling corrector) and evaluate performance on that task 

\textbf{intrinsic}:
\begin{itemize}
\item train parameters of model on training set and test model performance on held -out dataset\tikzmark{arrowstart}
\item use likelihood that model would produce the new data to evaluate how well it is doing\tikzmark{arrowend}
\end{itemize}

\subsubsection{Perplexity} 
of language model quantifies level of surprise/confusion at seeing new text
\begin{itemize}\item measures how unlikely the observed data is under the model \end{itemize}
Calculating perplexity:
\begin{tikzpicture}[remember picture, overlay]
  \draw[->, thick, myblue, >=stealth] 
    ($(pic cs:arrowstart)+(0.05cm,0.1cm)$) 
    % The 'controls' command defines the curve.
    % +(5cm,0) means: go 5cm to the right of the start...
    % and +(5cm,0) means: ...approach the end from 5cm to the right.
    .. controls +(1cm,0.2cm) and +(1cm,0) .. 
    ($(pic cs:arrowend)+(0.05cm,0.1cm)$);
\end{tikzpicture}
\begin{enumerate}
  \item \emph{compute probability} of observed sequence $\Prob(w_1,w_2,\dots,w_n)$ under model
  \begin{itemize}
    \item 
  e.g.\ for a bigram m:
  \(
    \Prob(w_1,\dots,w_n)
    = \Prob(w_1)\Prob(w_2\mid w_1)\Prob(w_3\mid w_2)\cdots \Prob(w_n\mid w_{n-1})
  \)
   \end{itemize}
  \item \emph{normalize probability} for length of text sequence
  \begin{itemize}
    \item i.e. compute ``average'' per-word probability: $\sqrt[n]{\Prob(w_1,w_2,\dots,w_n)}$
  \end{itemize}
  \item \emph{invert probability} to compute uncertainty:
  \[
    \operatorname{Perplexity}(w)
    =
    \bigl(1/\Prob(w_1,w_2,\dots,w_n)\bigr)^{1/n}
    =
    \sqrt[n]{\frac{1}{\Prob(w_1,w_2,\dots,w_n)}}
  \]
\end{enumerate}



\subsection{Embeddings}
To overcome the problem of data sparsity in $n$-gram language models it would be good to have a similar representation for similar words.
\medskip

\subsubsection{WordNet}
\textbf{WordNet} organizes lexical information in terms of word meanings (senses), rather than word forms (actual words). 
In that respect, WordNet resembles a thesaurus more than a dictionary.
\begin{itemize}
\item
attempt to model human lexical memory
\item
is essentially an ontology (a carving up of the world's meanings)
\item
no longer SOTA, since difficult to build, language-specific, incomplete, still difficult to quantify similarity
\end{itemize}
\medskip

\subsubsection{Distributional hypothesis of Harris (1968)}
\textcolor{red}{words that occur in the same contexts tend to have similar meanings.}
\begin{itemize}
\item
\textbf{Practical implication}:\\
We can find similar words by comparing their distribution over contexts.
\item Count how often words appear in several contexts; 
contexts become dimensions and target words $w,v$ become vectors $\vect w,\vect v$, then compute $\cos(\vect w,\vect v)$.
gives a value in $[0,1]$ of similarity.
\item Then apply dimensionality reduction (SVD / Latent Semantic Analysis): 
reducing millions of contexts to hundreds of dimensions, while keeping as much of the info as possible (now it's \textbf{dense} embedding).
\end{itemize}

While tf-idf vectors are very long and full of $0$s (sparse), learnt vectors are shorter and with full of non-$0$ (dense).


\subsubsection{Sparse versus dense vectors}


Count-based (or tf-idf) vectors are
\begin{itemize}%[nosep,topsep=0pt, before=\vspace{-\parskip}]
\item \textbf{long} (length $|V|\tikzmark{cb} = 20{,}000$ to $50{,}000$)%
% --- bubble + pointer (overlay) ---%
\begin{tikzpicture}[remember picture,overlay]%
  \node[
    fill=yellow!40,
    draw=black,
    thick,
    % line width=1pt,
    rounded corners=10pt,
    inner xsep=20pt,
    inner ysep=4pt,
    align=center,
    font=\normalsize
  ] (bubble) at ($(pic cs:cb)+(8.0cm,1cm)$)
  {Vocabulary, all the words\\ your model covers};

  \draw[black,thick]
    (bubble) -- ($(pic cs:cb)+(0,0.2cm)$);
\end{tikzpicture}%
\item \textbf{sparse} (most elements are zero)
\end{itemize}

Alternative: learn vectors which are
\begin{itemize}%[nosep,topsep=0pt, before=\vspace{-\parskip}]
\item \textbf{short} (length =  50 to 1000)
\item \textbf{dense} (most elements are non-zero)
\end{itemize}
\medskip

Why dense vectors? 
\begin{itemize}
  \item short vectors may be easier to use as \emph{features} in machine learning (fewer weights to learn)
  \item may \emph{generalize} better than explicit counts
  \item emprically, work better
\end{itemize}



\subsection{Word2vec}
Train a classifier on a binary prediction task: “Is word $w$ likely to show up in context $c$?”

We don't care about the task, but we take learned classifier weights as word embeddings.

big idea: \textbf{\textcolor{blue}{self-supervision}}

% Sigmoid maps $\R \to [0,1]$:
% \[
%   s(z)=\frac{1}{1+e^{-z}}
% \]

Different versions, we focus on \textbf{skip-gram} with negative sampling (SGNS).

Skip-gram: given the central word, predict context.

One context word \(c\):
$$
P(+\mid w, c)=\sigma(\vect c \cdot \vect w) =\frac{1}{1+\exp (-\vect c \cdot \vect w)}
$$

We have lots of context words. We'll assume independence and just multiply them:
$$
P\left(+\mid w, c_{1: L}\right)  =\prod_{i=1}^L \sigma\left(\vect c_i \cdot \vect w\right)
$$
$$
\log P\left(+\mid w, c_{1: L}\right) =\sum_{i=1}^L \log \sigma\left(\vect c_i \cdot \vect w\right)
$$


Train via SGD!

Word weights will be adjusted such that 
\begin{itemize}
  \item positive pairs will be more likely
  \item negative pairs will be less likely
\end{itemize}  
over the entire training set.

\bigskip


SGNS learns 2 sets of embeddings:\tikzmark{w2v}%
\begin{tikzpicture}[scale=0.6, font=\footnotesize, remember picture, overlay]%
\begin{scope}[shift={($(pic cs:cb)+(12cm,8cm)$)}]
        
  % geometry
  \def\xL{0}
  \def\xR{0.8}
  \def\yTop{10}
  \def\yMid{5}
  \def\yBot{0}

  % main stacked matrix block
  \draw[rounded corners=1pt, line width=.8pt] (\xL,\yBot) rectangle (\xR,\yTop);
  \fill[green!35] (\xL,\yMid) rectangle (\xR,\yTop);
  \fill[blue!25]  (\xL,\yBot) rectangle (\xR,\yMid);
  \draw[line width=.8pt] (\xL,\yMid) -- (\xR,\yMid);

  % top label
  \node[gray, anchor=south] at ({(\xL+\xR)/2},\yTop) {$1..d$};

  % slim horizontal "vector" bar with 3 red dots on top
  \newcommand{\vecdots}[1]{%
    % bar
    \fill[white] (\xL,#1-0.16) rectangle (\xR,#1+0.16);
    \draw[black, line width=.35pt] (\xL,#1-0.16) rectangle (\xR,#1+0.16);
    % dots
    \foreach \dx in {0.2,0.4,0.6}{
      \fill[red!80!black] (\xL+\dx,#1) circle (1.4pt);
      \draw[black, line width=.3pt] (\xL+\dx,#1) circle (1.4pt);
    }%
  }

  % row y-positions (tighter aardvark/apricot; corrected bottom dot rows)
  \def\yA{9.35}
  \def\yB{8.35}  % apricot closer to aardvark
  \def\yZt{5.55}
  \def\yAb{4.45}
  \def\yBb{3.45}
  \def\yZb{0.65}

  \vecdots{\yA}
  \vecdots{\yB}
  \vecdots{\yZt}
  \vecdots{\yAb}
  \vecdots{\yBb}
  \vecdots{\yZb}

  % internal ellipses (gray)
  \node[gray] at ({(\xL+\xR)/2},6.65) {$\cdots$};
  \node[gray] at ({(\xL+\xR)/2},2.10) {$\cdots$};

  % left word lists
  \node[anchor=east, text=green!50!black] at (-.25,\yA) {aardvark};
  \node[anchor=east, text=green!50!black] at (-.25,\yB) {apricot};
  \node[anchor=east, text=green!50!black] at (-.25,6.65) {$\cdots$};
  \node[anchor=east, text=green!50!black] at (-.25,\yZt) {zebra};

  \node[anchor=east, text=blue!70!black] at (-.25,\yAb) {aardvark};
  \node[anchor=east, text=blue!70!black] at (-.25,\yBb) {apricot};
  \node[anchor=east, text=blue!70!black] at (-.25,2.10) {$\cdots$};
  \node[anchor=east, text=blue!70!black] at (-.25,\yZb) {zebra};

  % right index labels
  \node[anchor=west, gray] at (\xR,\yA) {$1$};
  \node[anchor=west, gray] at (\xR,\yZt) {$|V|$};
  \node[anchor=west, gray] at (\xR,\yAb) {$|V|{+}1$};
  \node[anchor=west, gray] at (\xR,\yZb) {$2|V|$};

  % braces + annotations
  \draw[decorate, decoration={brace, amplitude=7pt}, line width=.8pt]
    (\xR+1.5,\yTop) -- (\xR+1.5,\yMid)
    node[midway, xshift=18pt] { $\matr{W}$};

  \draw[decorate, decoration={brace, amplitude=7pt}, line width=.8pt]
    (\xR+1.5,\yMid) -- (\xR+1.5,\yBot)
    node[midway, xshift=18pt] { $\matr{C}$};
\end{scope}
\end{tikzpicture}%
\begin{itemize}
  \item target embeddings matrix $\matr{W}$
  \item context embeddings matrix $\matr{C}$
\end{itemize}
it is common to just add them together, \\
representing word $i$ as $\vect{w}_i + \vect{c}_i$.


\bigskip

Fasttext
\begin{itemize}
  \item addresses problem of unknown words
  \item imagine language with a lot of morphology
  \item uses subword segmentation to deal with this
\end{itemize}

\subsection{Properties of Word Embeddings}

many interesting and somewhat surprising properties


semantic clustering:
\begin{itemize}
  \item neighbors in embedding space are semantically related to one another
\end{itemize}

provide dense distributed representation
\begin{itemize}
  \item where individual dimensions aren't usually interpretable
\end{itemize}

but translation in the space is meaningful
\begin{itemize}
  \item certain directions in the space have meaning
  \item which means that semantics is often additive
  \item and analogies are encoded in the embedding: e.g. \texttt{man} is to \texttt{woman} as \texttt{king} is to \texttt{queen}
\end{itemize}



% \textbf{Causal models}:
% \begin{itemize}
%     \item restrict context to before the missing word
%     \item language modelling: can be used for predicting next word in a sequence
%     \item with longer dependencies than possible with $n$-grams
% \end{itemize}

% \textbf{Non-causal}: 
% \begin{itemize}
%     \item can be used as additional feature vector for representing words
%     \item improve performance on most tasks
%     \item such as training classifier to detect sentiment
%     \item or translating one language to another
%     \item because they make use of additional domain knowledge (semantics of words)
% \end{itemize}


\clearpage
% --- Slide 5 ---------------------------------------------------------------
\section{Sequence classification and labelling} % Slide 5

\textbf{CRF}: conditional random fields.

\textbf{Part of speech (POS)}: 
assigning to each word a word class (noun, verb etc), so a map from words to classes. 


\subsection{Named entity recognition (NER)}
Similar to POS but instead of noun etc classes like \enquote{person}, \enquote{organisation}, etc.

NER can be hard because of:
\begin{itemize}
  \item \emph{segmentation}: in POS tagging each word gets one tag, while in NER entities can be phrases
  \item \emph{type ambiguity}: same word/phrase can have many types depending on context
\end{itemize}


\clearpage
% --- Slide 6 ---------------------------------------------------------------
\section{Sequence-to-sequence model} % Slide 6


Soft-Attention produces weighted average over input embeddings:
\[
  w_{i,j}=\Prob(j\mid i)=\operatorname{softmax}\bigl(\operatorname{similarity}(\vect{h}_{i-1}, \vect{e}_{j})\bigr),
  \qquad
  \vect{z}_{i}=\sum_{j} w_{i,j}\, \vect{e}_{j}
\]


Calculating similarity:
\begin{itemize}
  \item \textbf{additive} attention: concatenate decoder state and encoder embedding in a feedforward network:
  \[
    \operatorname{similarity}(\vect{h}_{i-1}, \vect{e}_{j})
    =
    \operatorname{FFNN}(\vect{h}_{i-1} \mathbin{\|} \vect{e}_{j})
  \]
  \item \textbf{multiplicative} attention: dot product of encoder vector and decoder previous state,
  \[
    \operatorname{similarity}(\vect{h}_{i-1}, \vect{e}_{j})
    =
    \frac{\vect{h}_{i-1} \cdot \vect{e}_{j}}{\sqrt{d}}
  \]
  where \(d\) is the embedding size. 
  \begin{itemize}
    \item we divide by \(\sqrt{d}\) so that the standard deviation remains \(1\):
    \[
    \begin{aligned}
    & \text { if } a_i, b_i \sim \mathcal{N}(0,1) \Rightarrow \Var [a_i b_i]=1 \\
    & \Rightarrow \Var[ \vect{a} \cdot \vect{b}]=\sum\nolimits_{i=1}^d \Var[a_i b_i] = d
    \end{aligned}
    \]
  \end{itemize}
\end{itemize}


Generalizing the notation with queries (what is being looked up), keys (index of what to find), values (stored information at key):
\[
  \vect{z}_i=\sum_{j}\operatorname{softmax}\Bigl(\frac{\vect{q}_i\cdot \vect{k}_j}{\sqrt{d}}\Bigr)\,\vect{v}_j
\]
where the queries, keys, and values are usually obtained by a linear transformation:
\[
  \vect{q}_i=\matr W_q \vect{h}_{i-1},\qquad
  \vect{k}_j=\matr W_k \vect{e}_j,\qquad
  \vect{v}_j=\matr W_v \vect{e}_j,
  \qquad
  \matr W_{q,k,v}\in \R^{d\times d}
\]


\[
\begin{tikzpicture}[
    scale=0.6,
  font=\footnotesize,
  >=Stealth,
  node distance=14mm,
  cell/.style={draw, fill=yellow!60, minimum width=6mm, minimum height=6mm},
  weight/.style={draw, solid,fill=orange!75, minimum width=6.5mm, minimum height=6.5mm},
  attn/.style={draw, rounded corners=3mm, fill=gray!25, minimum width=35mm, minimum height=10mm, align=center},
  redbox/.style={draw=red, fill=white, very thick, inner sep=2pt},
  dot/.style={circle, fill=black, inner sep=1.3pt},
  arr/.style={->, thick},
  darr/.style={->, thick, dashed}
]

% --- Encoder (bottom row) ----------------------------------------------------
\node[cell] (enc1) at (0,-6) {};
\node[cell] (enc2) at (3,-6) {};
\node[cell] (enc3) at (6,-6) {};
\node[cell] (enc4) at (9,-6) {};

\draw[arr] (enc1) -- (enc2);
\draw[arr] (enc2) -- (enc3);
\draw[arr] (enc3) -- (enc4);

% small up/down arrows on encoder cells
\foreach \n in {enc1,enc2,enc3,enc4}{
  \draw[arr, <-] (\n.south) -- ++(0,-6mm);
}
\draw[arr, <-] (enc1.west) -- ++(-6mm,0);

\node[anchor=east] at ($(enc1.west)+(-8mm,0)$) {Encoder};

% --- Decoder (top row) --------------------------------------------------------
\node[cell] (dec1) at (0,7) {};
\node[cell] (dec2) at (3,7) {};
\node[cell] (dec3) at (6,7) {};

\draw[arr] (dec1) -- (dec2);
\draw[arr] (dec2) -- (dec3);

% small up/down arrows on decoder cells
\draw[arr, ->] (dec1.north) -- ++(0,6mm);
\draw[arr, ->] (dec2.north) -- ++(0,6mm);
\draw[arr, ->] (dec3.north) -- ++(0,6mm);
\draw[arr, <-] (dec1.south) -- ++(0,-6mm);
\draw[arr, <-] (dec3.south) -- ++(0,-6mm);

\node[anchor=east] at ($(dec1.west)+(-8mm,0)$) {Decoder};

% black dot on decoder state and label h_{i-1}
\coordinate (mid12) at ($(dec1.east)!0.45!(dec2.west)$);
\node[dot] (hpt) at (mid12) {};
\node[above=0pt of hpt] {$\vect{h}_{i-1}$};

% --- Attention block (middle) -------------------------------------------------
\node[attn] (att) at ($(dec2)!0.5!(enc2)$) {\textbf{Attention}\\ $(w_{i1},\ldots,w_{in})$};

% Multiplication circle and z_i arrow
\node[outer sep=0pt, inner sep=0pt] (mul) at ($(att.north)!0.3!(dec2.south)$) {\huge$\otimes$};
\draw[arr] (mul) -- node[pos=0.3, right] {$\vect{z}_i$} (dec2.south);
\draw[arr] (att.north) -- (mul);

% Values list (right, red boxed)
\node[redbox, anchor=west] (vals) at ($(att.east)+ (0.6,0)$) {$(\vect{v}_1,\ldots,\vect{v}_j,\ldots,\vect{v}_n)$};

% Dashed right-angle arrow from values to mul
\draw[darr] (vals.north) |- (mul.east);

% Inputs from encoder up into attention
\draw[arr] (enc1.north) -- (att);
\draw[arr] (enc2.north) -- (att);
\draw[arr] (enc4.north) -- (att);






\node[outer sep = 0pt] (ej) at ($(enc3.north)+(0,2.5mm)$) {$\vect{e}_j$};

\draw[darr] (ej) -- node[pos=0.5, weight] {$\matr{W}_v$} (vals);


\draw[arr] (ej) -- 
node[pos=0.4, weight] {$\matr{W}_k$}
node[pos=0.66, redbox] (kj) {$\vect{k}_j$}
(att);



\draw[arr] (hpt) to[out=-130,in=180] 
node[pos=0.5, weight] {$\matr{W}_q$} 
node[pos=0.76, redbox] (qi) {$\vect{q}_i$}
(att.west);






% little red "..." dots at encoder end
\node[draw, circle, fill=red!60, minimum size=2mm, inner sep=0pt, outer sep=3pt] (r1) at ($(enc4.east)+(10mm,0)$) {};
\node[draw, circle, fill=red!60, minimum size=2mm, inner sep=0pt, right=-3pt of r1, outer sep=3pt] (r2) {};


\draw[arr] (enc4) -- (r1.west);
\draw[arr] (r2.east) -- ++(0.5,0) -- ++(0,11) -- ++(-12.8,0) |- (dec1.west);


\end{tikzpicture}
\]



Interpreting queries, keys, values with a hypothetical example:
\begin{itemize}
  \item \textbf{query}: what is being looked up\\
  \textcolor{red}{need adjective that describes a person}
  \item \textbf{key: }index of what to find\\
  \textcolor{red}{have adjective that describes people \& animals}
  \item \textbf{value}: stored information at key\\
  \textcolor{red}{word is ``friendly'', which in Italian could be translated to ``simpatico''}
\end{itemize}




\end{document}
