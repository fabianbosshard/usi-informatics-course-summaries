% author: Fabian Bosshard
% © CC BY 4.0

% compile with:
% pdflatex -> biber -> pdflatex -> pdflatex

\begin{filecontents}[overwrite]{\jobname.bib}
@book{boschini2022,
  author    = {Cecilia Boschini and Arne Hansen and Stefan Wolf},
  title     = {Discrete Mathematics},
  year      = {2022},
  publisher = {vdf Hochschulverlag AG an der ETH Zürich},
  isbn      = {9783728141101},
  doi       = {10.3218/4110-1},
  url       = {https://vdf.ch/discrete-mathematics-e-book.html},
}
\end{filecontents}


% \documentclass[twocolumn, a3paper, fontsize=9pt, headings=standardclasses, parskip=half]{scrartcl}
% \documentclass[a4paper, fontsize=9pt, headings=standardclasses, parskip=half,twoside]{scrartcl} % for printing (twosided)
\documentclass[a4paper, fontsize=9pt, headings=standardclasses, parskip=half]{scrartcl}
\usepackage{csquotes}


% \usepackage[automark]{scrlayer-scrpage}
% \clearpairofpagestyles
% \ofoot{\pagemark} % für ein einseitiges Dokument: \ofoot platziert den Inhalt in der äußeren Fußzeile (unten rechts)
% % \pagestyle{scrheadings}

\usepackage{scrlayer-scrpage}
\clearpairofpagestyles
\ofoot{\pagemark} % für ein einseitiges Dokument: \ofoot platziert den Inhalt in der äußeren Fußzeile (unten rechts)
% \pagestyle{scrheadings}


% \renewcommand{\familydefault}{\sfdefault} % sans serif font for text (math font is still serif)

\usepackage{graphicx}
% \usepackage[dvipsnames, table]{xcolor} % causes problem with the red \ddots in the cantor's diagonal argument
\usepackage[dvipsnames]{xcolor}


\usepackage[left=43mm, right=43mm, top=20mm, bottom=30mm]{geometry} % for A4
% \usepackage[left=25mm, right=25mm, top=25mm, bottom=40mm]{geometry} % for A3
% \usepackage{showframe}







% float management ------------------------------------------------
\usepackage{float}
\usepackage{placeins} 
\usepackage{etoolbox}

% \makeatletter
%   % single-column floats
%   \def\fps@figure {htb} 
%   \def\fps@table  {htb}

%   % double-column floats
%   \def\fps@figure*{htb} 
%   \def\fps@table* {htb}
% \makeatother

\preto\subsection{\FloatBarrier}
\preto\subsubsection{\FloatBarrier} 

\setcounter{topnumber}    {8}
\setcounter{bottomnumber} {8}
\setcounter{totalnumber}  {20}
\renewcommand{\textfraction}{0.0}
\renewcommand{\topfraction}{1.0}
\renewcommand{\floatpagefraction}    {1.0}
\renewcommand{\dblfloatpagefraction} {1.0}

% On a float‐only page, kill the default “centered” glue
\makeatletter
  \setlength{\@fptop}{0pt} % no extra space above
  \setlength{\@fpsep}{\floatsep} % between floats: same as \floatsep
  \setlength{\@fpbot}{0pt plus 1fil} % infinite stretch below, so floats are pushed up
\makeatother
% ----------------------------------------------------------------


% \newlength\tindent
% \setlength{\tindent}{\parindent}
% \setlength{\parindent}{0pt}
% \renewcommand{\indent}{\hspace*{\tindent}}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{mathdots} % without this package, only \vdots and \ddots are taken from the text font (not the math font), which looks bad if the text font is different from the math font
\allowdisplaybreaks[4] 


\usepackage{enumitem}
\renewcommand{\labelitemi}{\textbullet}
\renewcommand{\labelitemii}{\raisebox{0.1ex}{\scalebox{0.8}{\textbullet}}}
\renewcommand{\labelitemiii}{\raisebox{0.2ex}{\scalebox{0.6}{\textbullet}}}
\renewcommand{\labelitemiv}{\raisebox{0.3ex}{\scalebox{0.4}{\textbullet}}}




\usepackage{pifont}
\usepackage{marvosym}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{ragged2e} 
\usepackage{hhline}
\newcommand{\tightaligned}[1]{%
  % local group so the change doesn't leak out
  {\setlength{\jot}{0.0ex}%  ← choose any small value you like
  \begin{aligned}
     #1
   \end{aligned}}%
}


\usepackage{xparse}
\usepackage{calc}   % for \heightof
\usepackage{tikz}
\usepackage{tikz}
\usetikzlibrary{arrows, arrows.meta, shapes, positioning, calc, fit, patterns, intersections, math, 3d, tikzmark, decorations.pathreplacing, backgrounds, chains, svg.path, decorations.markings}
\usepackage{forest}
\usepackage{tikz-3dplot}
\usepackage{tikz-cd}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\tikzset{restore nodes from file,} % Load in the saved node information from previous runs.


% define colors
\definecolor{funblue}{rgb}{0.10, 0.35, 0.66}
\definecolor{alizarincrimsonred}{rgb}{0.85, 0.17, 0.11}
\definecolor{amethyst}{rgb}{0.6, 0.4, 0.8}
\definecolor{mypurple}{rgb}{128, 0, 128} 
\renewcommand{\emph}[1]{\textcolor{mypurple}{#1}}

\usepackage{thmtools}
\declaretheoremstyle[headfont=\bfseries,bodyfont=\normalfont,spaceabove=6pt,spacebelow=6pt,qed=\ensuremath{\vartriangleleft},postheadspace=1em]{assertionstyle}
\declaretheorem[style=assertionstyle,name=Theorem]{theorem}
\declaretheorem[style=assertionstyle,name=Lemma,sibling=theorem]{lemma}
\declaretheorem[style=assertionstyle,name=Corollary,sibling=theorem]{corollary}
\declaretheorem[style=assertionstyle,name=Proposition,sibling=theorem]{proposition}
\declaretheorem[style=assertionstyle,name=Conjecture,sibling=theorem]{conjecture}
\declaretheorem[style=assertionstyle,name=Claim,sibling=theorem]{claim}
\declaretheoremstyle[headfont=\bfseries,bodyfont=\normalfont,spaceabove=6pt,spacebelow=6pt,qed=\ding{45},postheadspace=1em]{definitionstyle}
\declaretheorem[style=definitionstyle,name=Definition]{definition}
\declaretheorem[style=definitionstyle,name=Axiom,sibling=definition]{axiom}
\declaretheoremstyle[headfont=\bfseries\color{funblue},bodyfont=\normalfont\normalsize,spaceabove=6pt,spacebelow=6pt,qed=\ensuremath{\color{funblue}\blacktriangleleft},postheadspace=1em]{examplestyle}
\declaretheorem[style=examplestyle,name=Example]{example}
\declaretheoremstyle[headfont=\bfseries,bodyfont=\normalfont\normalsize,spaceabove=6pt,spacebelow=6pt,qed=\ensuremath{\blacktriangleleft},postheadspace=1em]{remarkstyle}
\declaretheorem[style=remarkstyle,name=Remark]{remark}
\declaretheoremstyle[headfont=\color{alizarincrimsonred}\bfseries,bodyfont=\normalfont\normalsize,spaceabove=6pt,spacebelow=6pt,qed=\ensuremath{\color{alizarincrimsonred}\blacktriangleleft},postheadspace=1em]{cautionstyle}
\declaretheorem[style=cautionstyle,name=Caution,sibling=remark]{caution}
\declaretheoremstyle[headfont=\bfseries,bodyfont=\normalfont\footnotesize,spaceabove=6pt,spacebelow=6pt,postheadspace=1em]{smallremarkstyle}
\declaretheorem[style=smallremarkstyle,name=Remark,sibling=remark]{smallremark}
\declaretheoremstyle[headfont=\bfseries\color{amethyst},bodyfont=\normalfont,spaceabove=6pt,spacebelow=6pt,qed=\ensuremath{\color{amethyst}\blacktriangleleft},postheadspace=1em]{digressionstyle}
\declaretheorem[style=digressionstyle,name=Digression]{digression}
\let\proof\relax
\let\endproof\relax
\declaretheoremstyle[
  headfont=\bfseries,
  bodyfont=\normalfont,
  spaceabove=6pt,
  spacebelow=6pt,
  qed=\ensuremath{\square},
  postheadspace=1em
]{proofstyle}
\declaretheorem[
  style=proofstyle,
  name=Proof,
  numbered=no
]{proof}
\newenvironment{verticalhack}
  {\begin{array}[b]{@{}c@{}}\displaystyle}
  {\\\noalign{\hrule height0pt}\end{array}} % to make qed symbol aligned

\usepackage{algorithm}
\usepackage[italicComments=false]{algpseudocodex}




% notation makros ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% \newcommand{\contradiction}{\; \: \text{\textcolor{red}{\Lightning}}}

% commands for functions etc.
\newcommand{\im}{\operatorname{Im}}

% commands for vectors and matrices
\newcommand{\matr}[1]{\underline{\boldsymbol{#1}}}
\newcommand{\vect}[1]{\vec{\boldsymbol{#1}}}
% \newcommand{\vect}[1]{\vec{{#1}}\,}

% commands for derivatives
\newcommand{\dif}{\mathrm{d}}

% commands for number sets
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}

% commands for probability
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Exp}{\operatorname{E}}
% \newcommand{\P}{\operatorname{P}} % this is already defined in amsmath/amsopn
\newcommand{\Prob}{\operatorname{P}}
\newcommand{\numof}{\ensuremath{\# \,}} % number of elements in a set
\newcommand{\blackheight}{\operatorname{bh}}

% \algnewcommand{\LeftComment}[1]{\(\triangleright\) #1}
\algnewcommand{\TO}{, \ldots ,}
\algnewcommand{\DOWNTO}{, \ldots ,}
\algnewcommand{\OR}{\vee}
\algnewcommand{\AND}{\wedge}
\algnewcommand{\NOT}{\neg}
\algnewcommand{\LEN}{\operatorname{len}}
\algnewcommand{\tru}{\ensuremath{\mathrm{\texttt{true}}}}
\algnewcommand{\fals}{\ensuremath{\mathrm{\texttt{false}}}}
\algnewcommand{\append}{\circ}

\algnewcommand{\nil}{\ensuremath{\mathrm{\textsc{nil}}}}
\algnewcommand{\red}{\ensuremath{\mathrm{\textsc{red}}}}
\algnewcommand{\black}{\ensuremath{\mathrm{\textsc{black}}}}
\algnewcommand{\gray}{\ensuremath{\mathrm{\textsc{gray}}}}
\algnewcommand{\white}{\ensuremath{\mathrm{\textsc{white}}}}

\newcommand{\attribute}[1]{\ensuremath{\mathtt{#1}}}
\newcommand{\attrib}[2]{\ensuremath{#1\mathtt{.}\mathtt{#2}}} % object.field with field in math typewriter (like code)


\newcommand{\BC}{\textbf{Base case.}}
\newcommand{\IH}{\textbf{Induction hypothesis.}}
\newcommand{\IS}{\textbf{Induction step.}}


% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\usepackage{caption, subcaption}

\usepackage[backend=biber,style=numeric]{biblatex}
\addbibresource{\jobname.bib}


% \usepackage[colorlinks=true, linkcolor=black, urlcolor=black, citecolor=black]{hyperref}
\usepackage[
  pdfauthor={Fabian Bosshard},
  pdftitle={USI - Discrete Structures - Course Summary},
  pdfkeywords={USI, discrete structures, course summary, informatics},
  colorlinks=false,        % don't wrap links in a colour
  pdfborder={0 0 0}        % no border around links
]{hyperref}
\usepackage[
  type     = {CC},
  modifier = {by},
  version  = {4.0},
]{doclicense}
\usepackage{cleveref}







% Title information 
\title{Discrete Structures - Summary}
\author{Fabian Bosshard}
\date{\today}




\begin{document}

\pagenumbering{roman}
\pagestyle{plain}


\maketitle


% \begingroup
%   \renewcommand*{\contentsname}{\underline{Discrete Structures}\vspace{0.5em}}
%   \setkomafont{section}{\Huge\bfseries}
%   \tableofcontents
% \endgroup
\tableofcontents

% % \clearpage
% \pagenumbering{arabic}    
% \setcounter{page}{2}      % 2) internally treat this as page 2 (even)
% \makeatletter
% \renewcommand*{\thepage}{\number\numexpr\value{page}-1\relax}
% \makeatother              % 3) but print counter minus one → “1”






\section*{Preface}
\addcontentsline{toc}{section}{Preface}

% https://search.usi.ch/courses/35270737/discrete-structures
This document is an unofficial student-made summary of the course 
\href{https://search.usi.ch/courses/35270737/discrete-structures}{Discrete Structures} taught by \href{https://search.usi.ch/people/eefbe656c9dfacf0e1a1e15bf8893bcb/wolf-stefan}{Stefan Wolf} in Spring 2025 at the 
\href{https://www.usi.ch/it}{Università della Svizzera italiana}.
It is mainly based on ~\cite{boschini2022}.
It is not complete (e.g. Chapter 1 and 6 from \cite{boschini2022} are missing) and could contain errors.
If you spot one, please report it to \href{mailto:fabianlucasbosshard@gmail.com}{fabianlucasbosshard@gmail.com} or open an issue at \url{https://github.com/fabianbosshard/usi-informatics-course-summaries}.
The \LaTeX{} source is also available there.

\doclicenseThis


\renewcommand{\emph}[1]{\textcolor{black}{#1}}
\printbibliography[heading=bibintoc,title={References}]
\renewcommand{\emph}[1]{\textcolor{mypurple}{#1}}




\clearpage
\section{Introduction}
\pagenumbering{arabic}
\pagestyle{scrheadings}

\begin{figure}[ht]
\centering
\begin{subfigure}[t]{0.4\textwidth}
\centering
\begin{tikzpicture}[scale=0.94]
    
    % pillars
    \node[draw=black, fill=gray!40, line width=2pt, rounded corners=0pt, inner sep=0, 
          fit={(-1.1,-0.9) (-0.5,1.2)}] (logicPillar) {};
    \node[draw=black, fill=gray!40, line width=2pt, rounded corners=0pt, inner sep=0, 
          fit={(0.5,-0.9) (1.1,1.2)}] (setPillar) {};
    
    % foundation
    \draw[fill=gray!40, draw=black, line width=2pt, rounded corners=3pt] 
         (-1.5,-1.4) rectangle (1.5,-0.9);
    
    % roof
    \draw[fill=gray!40, draw=black, line width=2pt, rounded corners=3pt] 
         (-1.5,1.23) -- (0,2.4) -- (1.5,1.23) -- cycle;
    
    % labels of pillars: referencing the center of each pillar node
    \node at (0,1.5) {\footnotesize Mathematics};
    \node at (0,-1.175) {\footnotesize ``Philosophy''};
    \node[rotate=90] at (logicPillar.center) {\footnotesize Logic};
    \node[rotate=90] at (setPillar.center) {\footnotesize Set Theory};
\end{tikzpicture}
\caption*{Pillars of mathematics}
\end{subfigure}
\begin{subfigure}[t]{0.4\textwidth}
\centering
\begin{tikzpicture}[scale=0.5]

	% coordinate axes
    \draw[thick,->] (-3.5,0) -- (3.5,0);% node[right] {\footnotesize\(\Z\)};
    \draw[thick,->] (0,-3.5) -- (0,3.5);% node[above] {\footnotesize\(\Z\)};
    
    % dotted red arrow
    \draw[red!60!black, line width=1.4pt, dotted, -{>[length=2.8mm, width=4mm]}]
        (0,0) -- (1,0) -- (1,1) -- (0,1) -- (-1,1) 
        -- (-1,0) -- (-1,-1) -- (0,-1) -- (1,-1) -- (2,-1) -- (2,0) -- (2,1)
        -- (2,2) -- (1,2) -- (0,2) -- (-1,2) -- (-2,2) -- (-2,1) 
        -- (-2,0) -- (-2,-1) -- (-2,-2) -- (-1,-2) -- (0,-2) -- (1,-2)
        ;
%     \node at (3.5,3.5) {\footnotesize \(\Q := \left\{ \frac{a}{b} \in \mathbb{R} \mid a \in \mathbb{Z}, b \in \mathbb{Z} \setminus \{0\} \right\}\)};
        
    % grid points
    \foreach \x in {-3,-2,-1,0,1,2,3}
        \foreach \y in {-3,-2,-1,0,1,2,3}
            \fill (\x,\y) circle (3.0pt);

\end{tikzpicture}
\caption*{Matching \(\Q\) and \(\N\)}
\end{subfigure}
\end{figure}
One can single out two pillars of today's mathematics.
Logic determines how to reason within mathematics, i.e., what is considered a valid proof.
Set theory describes the objects we deal with.

\begin{definition}[Countable set]\label{def:countable}
  A set \(S\) is called \emph{countable} if there exists a injective function \(f: S \to \N\).
  If \(f\) is also surjective, then \(S\) is called \emph{countably infinite}.
\end{definition}

The rational numbers are countable, as we can see if we arrange them in a grid:
\[
\Q := \left\{ \frac{a}{b} \in \mathbb{R} \mid a \in \mathbb{Z}, b \in \mathbb{Z} \setminus \{0\} \right\}
\]

\begin{theorem}[Cantor's diagonal argument]
\label{thm:diagonal}
The set of real numbers in the interval \([0,1]\) is uncountable. 
Thus the size of the set of real numbers is strictly greater than the size of the set of natural numbers (in the sense that there is no bijective map between the two):
\[
  \lvert [0,1] \rvert = \lvert \R \rvert > \lvert \N \rvert = \lvert \Q \rvert. \qedhere
\] 
\end{theorem}
\begin{proof} (Contradiction)
Assume there exists a bijection
\[
f: \mathbb{N} \to [0,1].
\]
Then every real number in \([0,1]\) can be expressed in its binary expansion, allowing us to write:
\[
\begin{array}{rcl}
f(1) &=& 0.\textcolor{red}{b_{11}}b_{12}b_{13}b_{14}\dots \\
f(2) &=& 0.b_{21}\textcolor{red}{b_{22}}b_{23}b_{24}\dots \\
f(3) &=& 0.b_{31}b_{32}\textcolor{red}{b_{33}}b_{34}\dots \\
f(4) &=& 0.b_{41}b_{42}b_{43}\textcolor{red}{b_{44}}\dots \\
&\vdots& \hfill \textcolor{red}{\ddots} \\
\end{array}
\]
where each \(b_{ij}\) is either 0 or 1.

Using Cantor's diagonal method, we now construct a new number \(x\) in \([0,1]\) by flipping the \(i\)-th bit of the binary expansion of \(f(i)\). Define \(x\) as
\[
x = 0.c_1c_2c_3c_4\dots
\quad
\text{
with
}
\quad
c_i = \begin{cases}
1, & \text{if } b_{ii} = 0 \\
0, & \text{if } b_{ii} = 1
\end{cases}\text{.}
\]
This guarantees that for every \(i \in \mathbb{N}\), the \(i\)-th digit of \(x\) is different from the \(i\)-th digit of \(f(i)\).

Since \(x\) differs from each \(f(i)\) in at least the \(i\)-th digit, it follows that \(x\) cannot equal any of the numbers in the list generated by \(f\). This contradicts the original assumption that \(f\) is a bijection since \(x\) is a member of \([0,1]\) that is not included in the enumeration.

Thus, there is no bijection between \(\mathbb{N}\) and \([0,1]\), which implies that \([0,1]\) (and hence \(\mathbb{R}\)) is uncountable.
\end{proof}
%\clearpage
%\section{Motivation}
%\subsection{Swapping knights}
%\subsection{Combinatorics}
%\subsection{Connections without crossings}
%\subsection{Coloring maps}
%\subsection{Exercises}
%\clearpage






\section{Propositional Logic}
% \subsection{Definitions}
\begin{definition}[Proposition, Atom, Connective]
  A \emph{proposition} is a sentence, expression, or formula which is either true or false, i.e., truth-definite.
  An \emph{atom} or atomic proposition is a basic proposition that is not composed of other propositions.
  A \emph{connective} links (generally) two propositions to a new proposition.
\end{definition}
% \subsubsection{Connectives as truth functions}
Connectives are full characterized by a truth table.
As truth can take one of two values, just like bits in a computer, there is a close relation between connectives and logical gates.
\subsection{Syntax of propositional logic}
In \emph{syntax} (of logic as well as of programming languages), we specify what a correct string (formula, program) is, independently of its respective ``meaning'' of ``function''.
\begin{definition}[Syntax]\label{def:syntax_propositional_logic}
  Syntactically correct formulas \(\mathcal{E}_{\mathcal{D}}\) with a set of atoms \(\mathcal{D} \coloneqq \{A, B, C, \dots\}\) are
  \begin{itemize}
    \item atomic formulas in \(\mathcal{D}\);
    \item if \(f\) and \(g\) are syntactically correct formulas, then also \((\neg f)\), \((f \land g)\) and \((f \lor g)\) are syntactically correct.
  \end{itemize}
  These are all the syntactically correct formulas.
\end{definition}

\subsection{Semantics of propositional logic}
\begin{definition}[Assignment]\label{def:assignment}
  A \emph{truth assignment} is a mapping from the set of atoms \(\mathcal{D}\) to the set of truth values \(\{0,1\}\).
  The set of all truth assignments is denoted by \(\mathcal{A}\).
 A \emph{truth assignment} \(\mathcal{A} \colon \mathcal{D} \to \{0,1\}\) is a function that assigns a truth value to every atom.
 This truth function is extended to all syntactically correct formulas by simply evaluating the formula, based on the truth values for its atoms and the connectives used.
\end{definition}
\begin{definition}[Semantic Behavior / Truth Vector]\label{def:truthvector}
  The \emph{semantic behavior} or \emph{truth vector} of a proposition is the list of truth values for all possible assignments.
\end{definition}
This behavior or vector can be expressed completely in a truth table.
\begin{definition}[Semantic Equivalence]\label{def:semanticequivalence}
  Two formulas are \emph{semantically equivalent} if they have the same truth value for all assigments of their atomic formulas.
  We write \(F \equiv G\) or \(F \Leftrightarrow G\).
\end{definition}
We introduce the ``truth values'' \(0\) and \(1\) as syntactically correct formulas, abbreviating (the shortest) unsatisfiable formula and tautology, respectively:
\[
  \begin{aligned}
    0 & :\equiv (A \land (\neg A)) \\
    1 & :\equiv (A \lor (\neg A))
  \end{aligned}
\]
\begin{definition}[Tautology, Unsatisfiable Formula]\label{def:tautology_unsatisfiable}
If a formula \(F\) is semantically equivalent to \(0\), i.e. \(F \equiv 0\), then \(F\) is called \emph{unsatisfiable}.
If a formula \(F\) is semantically equivalent to \(1\), i.e. \(F \equiv 1\), then \(F\) is called a \emph{tautology}.
\end{definition}

Similarly, we introduce additional connectives as abbreviations:
\[
\begin{aligned}
  A \oplus B: \equiv((A \wedge(\neg B)) \vee((\neg A) \wedge B)) \\
  A \leftrightarrow B: \equiv(A \wedge B) \vee((\neg A) \wedge(\neg B))
\end{aligned}
\]

Semantic equivalence is an \emph{equivalence relation}: It partitions the set of all formulas into disjoint subsets, groups of semantically equivalent formulas, the \emph{equivalence classes}.
It structures the set of all syntactically correct formulas \(\mathcal{E}_\mathcal{D}\) as visualized in the following diagram:
\begin{center}
\begin{tikzpicture}[scale=0.5]

  % The outer green region (filling only)
  \colorlet{intermediate}{white!70!green}
  \fill[black!20!intermediate]
    plot[smooth cycle] coordinates{
      (-3,-2) (0.1,-2.8) (2.5,-2) (3,2.5) (1,4) (-2,2)
    };

  % Red partition lines (before the boundary)
  \draw [line width=1pt, red!50!black] (-3,-2) -- (-1.5,0) -- (0,1.5) -- (1,4); % Left boundary --> top boundary
  \draw [line width=1pt, red!50!black] (0,-1) -- (1.5,0.5) -- (3,2.5); % Bottom boundary --> right boundary
  \draw [line width=1pt, red!50!black] (-2,2) -- (0,1.5) -- (1.5,0.5) -- (2.5,-2); % Left boundary (near top) --> bottom boundary
  \draw [line width=1pt, red!50!black] (-1.5,0) -- (0,-1) -- (0.1,-2.8); % Another cut connecting left to bottom

  % The outer green boundary (draw after red lines to overlay them)
  \draw[name path=outer, line width=2pt, green!50!black]
    plot[smooth cycle] coordinates{
      (-3,-2) (0.1,-2.8) (2.5,-2) (3,2.5) (1,4) (-2,2)
    };

  % Label the region as E_D, with an arrow to the boundary
  \node at (-2.3,3.9) (Elabel) {\footnotesize $\mathcal{E}_{\mathcal{D}}$};

  % Choose a direction for the arrow
  \coordinate (centroid) at (0.1,0.28);
  
  % Define a ray from the label toward the centroid, extended enough to hit the boundary
  \path[name path=ray] (Elabel) -- ($(Elabel)!1.1!(centroid)$);
  
  % Calculate the intersection point between the ray and the green boundary
  \path [name intersections={of=outer and ray, by=Eintersection}];
  
  % Draw the arrow from the label to the calculated intersection point
  \draw [black!100!white,-] (Elabel) -- (Eintersection);

  % Annotations: tautologies / unsatisfiable
  \node at (4.5,4.5) {\footnotesize contains all tautologies};
  \draw [black!100!white,-] (3,4.2) -- (2.2,3.3);

  \node at (-4.5,-3.5) {\footnotesize contains all unsatisfiable formulas};
  \draw [black!100!white,-] (-3.3,-3.2) -- (-2.2,-2.2);

  % The points 1 and 0 
  \fill[black] (2,2.3) circle (0.6mm);
  \node at (2,2.7) {$1$};

  \fill[black] (-1,-1.7) circle (0.6mm);
  \node at (-1,-1.3) {$0$};

\end{tikzpicture}
\end{center}


\paragraph{Number of Equivalence Classes}
The set of all syntactically correct formulas \(\mathcal{E}_\mathcal{D}\) is infinite.
% For instance, it contains the sequence \((A, (\neg A), (\neg (\neg A)), \ldots)\).
The number of equivalence classes, however, is finite (if the set of atoms, \(\mathcal{D}\), is).
Let \(n\) be the number of atoms in \(\mathcal{D}\), i.e. \(\lvert \mathcal{D} \rvert = n\).
Then there are \(2^n\) different input configurations, i.e. the truth table has \(2^n\) rows.
As each row specifies an entry (\(0\) or \(1\)) of the truth vector, there are 
\[
\fcolorbox{red}{white}{$\displaystyle
2^{2^n}
$}
\]
different semantic behaviors, i.e. equivalence classes (Section~\ref{subsubsec:equivalence-relations}).

We express the semantic equivalence of \emph{two} formulas through a property of \emph{one single} formula:
\begin{theorem}[Relationship between semantic equivalence and tautology]\label{thm:semantic_equivalence_tautology}
  Two formulas \(F\) and \(G\) are semantically equivalent (Def.~\ref{def:semanticequivalence}), i.e. \(F \Leftrightarrow G\), if and only if the formula \(F \leftrightarrow G\) is a tautology (Def.~\ref{def:tautology_unsatisfiable}).
\end{theorem}
\begin{proof}
  The formula \(F \leftrightarrow G \equiv (F \land G) \lor ((\neg F) \land (\neg G))\) is a tautology if and only if \(F\) and \(G\) have the same truth values for all assignments.
  Thus, it is a tautology if and only if \(F\) and \(G\) are semantically equivalent.
\end{proof}
Note that \(\leftrightarrow\) connects \(F\) and \(G\) syntactically, whereas \(\Leftrightarrow\) connects the two formulas semantically.

\subsection{Logical Laws}\label{subsec:logical_laws}

So far, we have been sticking closely to the syntax permitted by Def~\ref{def:syntax_propositional_logic}.
We introduce some simplifications of notation, motivated by equivalences that can be derived from the logical laws:
\begin{itemize}
\item We allow the connectives $(\oplus, \rightarrow, \leftrightarrow)$, as they are equivalent to formulas with basic connectives.
\item If brackets do not change the truth behavior, they can be dropped. We write
$$
\bigwedge_{i=1}^n A_i \equiv A_1 \wedge \ldots \wedge A_n, \quad \bigvee_{i=1}^n A_i \equiv A_1 \vee \ldots \vee A_n
$$
\item We introduce the priority rules (operator precedence):
\[
\neg, (\wedge, \vee), (\oplus, \leftarrow, \rightarrow, \leftrightarrow) 
\]
or if we want to be more extreme:
\[
\neg, \wedge, \vee, \oplus, (\leftarrow, \rightarrow), \leftrightarrow
\]
\end{itemize}

\begin{table}[ht]
  \begin{tabular}{|l|l|}
  \hline
  % \multicolumn{2}{|c|}{Logical Laws} \\ \hline
  \textbf{Equivalence} & \textbf{Name} \\ 
  \hhline{|=|=|} 
  $\displaystyle
  \tightaligned{
  & p \wedge \tru \equiv p \\
  & p \vee \fals \equiv p
  }
  $ & Identity laws \\ \hline
  $\displaystyle
  \tightaligned{
  & p \vee \tru \equiv \tru \\
  & p \wedge \fals \equiv \fals
  }
  $ & Domination laws \\ \hline
  $\displaystyle
  \tightaligned{
  & p \vee p \equiv p \\
  & p \wedge p \equiv p
  }
  $ & Idempotent laws \\ \hline
  $\displaystyle
  \tightaligned{
  & \neg(\neg p) \equiv p
  }
  $ & Double negation law \\ \hline
  $\displaystyle
  \tightaligned{
  & p \vee q \equiv q \vee p \\
  & p \wedge q \equiv q \wedge p
  }
  $ & Commutative laws \\ \hline
  $\displaystyle
  \tightaligned{
  & (p \vee q) \vee r \equiv p \vee (q \vee r) \\
  & (p \wedge q) \wedge r \equiv p \wedge (q \wedge r)
  }
  $ & Associative laws \\ \hline
  $\displaystyle
  \tightaligned{ 
  & p \vee (q \wedge r) \equiv (p \vee q) \wedge (p \vee r) \\
  & p \wedge (q \vee r) \equiv (p \wedge q) \vee (p \wedge r) \\
  & p \wedge (q \oplus r) \equiv (p \wedge q) \oplus (p \wedge r)
  }
  $ & Distributive laws \\ \hline
  $\displaystyle
  \tightaligned{
  & \neg(p \wedge q) \equiv \neg p \vee \neg q \\
  & \neg(p \vee q) \equiv \neg p \wedge \neg q
  }
  $ & De Morgan's laws \\ \hline
  $\displaystyle
  \tightaligned{
  & p \vee (p \wedge q) \equiv p \\
  & p \wedge (p \vee q) \equiv p
  }
  $ & Absorption laws \\ \hline
  $\displaystyle
  \tightaligned{
  & p \vee \neg p \equiv \tru \\
  & p \wedge \neg p \equiv \fals
  }
  $ & Negation laws \\ \hline
  $\displaystyle
  \tightaligned{
  & p \rightarrow q \equiv \neg p \vee q \\
  & p \rightarrow q \equiv \neg q \rightarrow \neg p \\
  & p \vee q \equiv \neg p \rightarrow q \\
  & p \wedge q \equiv \neg(p \rightarrow \neg q) \\
  & \neg(p \rightarrow q) \equiv p \wedge \neg q \\
  & (p \rightarrow q)\wedge(p \rightarrow r) \equiv p \rightarrow (q \wedge r) \\
  & (p \rightarrow r)\wedge(q \rightarrow r) \equiv (p \vee q) \rightarrow r \\
  & (p \rightarrow q)\vee(p \rightarrow r) \equiv p \rightarrow (q \vee r) \\
  & (p \rightarrow r)\vee(q \rightarrow r) \equiv (p \wedge q) \rightarrow r
  }
  $ & Implication laws \\ \hline
  $\displaystyle
  \tightaligned{
  & p \leftrightarrow q \equiv (p \rightarrow q)\wedge (q \rightarrow p) \\
  & p \leftrightarrow q \equiv \neg p \leftrightarrow \neg q \\
  & p \leftrightarrow q \equiv (p \wedge q)\vee (\neg p \wedge \neg q) \\
  & \neg(p \leftrightarrow q) \equiv p \leftrightarrow \neg q
  }
  $ & Biconditional laws \\ \hline
  \end{tabular}
\end{table}

Note that the distributivity for AND and XOR does not hold if the two connectives are swapped (just as in arithmetic, there is a distributive law: $a(b+c)=a b+a c$, but not $a+b c=$ $(a+b)(a+c)$ in general). 
In fact, XOR and AND can be sees as addition and multiplication of logic.

\subsection{Normal forms}
\begin{definition}[Literal]\label{def:literal}
If \(A \in \mathcal{D}\) is an atom, then \(A\) and \(\neg A\) are called literals:
A literal is an atom or a negated atom.
\end{definition}
\begin{definition}[Disjunctive Normal Form]\label{def:DNF}
A formula \(F\) is in \emph{disjunctive normal form} (DNF) if there exist literals \(L_{i,j}\) such that
\[
  F=  \bigvee_{i=1}^n\left(\bigwedge_{j=1}^{m_i} L_{i, j}\right) =  \left(L_{1,1} \wedge \ldots \wedge L_{1, m_1}\right) \vee \ldots  \vee\left(L_{n, 1} \wedge \ldots \wedge L_{n, m_n}\right) \qedhere
\]
\end{definition}
\begin{definition}[Conjunctive Normal Form]\label{def:CNF}
A formula \(F\) is in \emph{conjunctive normal form} (CNF) if there exist literals \(L_{i,j}\) such that
\[
  F=  \bigwedge_{i=1}^n\left(\bigvee_{j=1}^{m_i} L_{i, j}\right) =  \left(L_{1,1} \vee \ldots \vee L_{1, m_1}\right) \wedge \ldots  \wedge\left(L_{n, 1} \vee \ldots \vee L_{n, m_n}\right) \qedhere
\]
\end{definition}
Note that, in these definitions, we put equalities $(=)$, not merely equivalences $(\equiv)$: The normal forms are also syntactic notions.

Given a syntactically correct formula, one obtains its semantic behavior by writing down the truth table.
It is also possible to construct a syntactically correct formula that reproduces a given truth table by considering either all the true rows (using Def~\ref{def:DNF}) or all the false rows (using Def~\ref{def:CNF}).
The DNF and CNF obtained for a given formula \(F\) are syntactically different but, due to construction - we started out from the same truth vector - semantically equivalent.


\subsection{Models and semantic conclusion}
\begin{definition}[Model]\label{def:model}
  Let \(F\) be a formula and \(\mathcal{A}\) an assigment (Def~\ref{def:assignment}) which renders \(F\) true.
  Then \(\mathcal{A}\) is a \emph{model} of \(F\).
  We write \(\mathcal{A} \models F\).
\end{definition}

Some of the properties of models are the following:
\begin{itemize}
  \item Two formulas \(F\) and \(G\) are semantically equivalent, i.e. \(F \equiv G\), if and only if they have the same models: \[\mathcal{A} \models F  \iff  \mathcal{A} \models G\]
  \item A formula \(F\) is a tautology if and only if every assignment \(\mathcal{A}\) is a model for \(F\) (\(F\) is true in any ``thinkable'' world).
  \item A formula \(F\) is unsatisfiable if and only if it has no model.
\end{itemize}

Based on the notions of models, we define the one-sided variant of semantic equivalence.

\begin{definition}[Semantic Conclusion]\label{def:semantic_conclusion}
$G$ is a semantic conclusion of $F_1, \ldots$, $F_n$ if every model $\mathcal{A}$ of all the $F_i$ is also a model of $G$. We say that $F_1, \ldots, F_n$ semantically implies $G$. We write $\left\{F_1, \ldots, F_n\right\} \vDash G$ or $\left\{F_1, \ldots, F_n\right\} \Rightarrow G$.
\end{definition}

Semantic conclusion is the one-sided variant of semantic equivalence.
They relate just like \(\rightarrow\) and \(\leftrightarrow\) on the syntactic level.
In particular, two formulas $F$ and $G$ are semantically equivalent if and only if $F$ is the semantic conclusion of $G$, and vice versa.

Note that the symbol $\models$ has two meanings: It indicates semantic conclusion as well as models.

\begin{theorem}[Relationship between semantic implication and tautology]\label{def:semantic_implication_tautology}
A set of formulas $\{F_1, \ldots, F_n\}$ semantically implies a formula $G$, i.e. $\{F_1, \ldots, F_n\} \models G$, if and only if the formula $\bigwedge_{i=1}^n F_i \rightarrow G$ is a tautology.
\end{theorem}

We carefully distinguish between the syntactical and the semantic levels: The expression $\left\{F_1, \ldots, F_n\right\} \vDash G$ relates the set of formulas $\left\{F_1, \ldots, F_n\right\}$ semantically with the formula $G$. 
The expression $\bigwedge_{i=1}^n F_i \rightarrow G$ connects the two syntactically and yields another syntactically correct formula. 
Only by demanding this formula to be a tautology does a semantic criterion emerge.

\begin{proof}
The implication $A \rightarrow B$ is equivalent to $\neg A \vee B$. Therefore,
$$
\begin{aligned}
\bigwedge_{i=1}^n F_i \rightarrow G & \equiv \neg\left(F_1 \wedge \ldots \wedge F_n\right) \vee G \\
& \equiv \neg F_1 \vee \ldots \vee \neg F_n \vee G
\end{aligned}
$$
This formula being a tautology means: If an assignment renders all $F_i$ true - $\mathcal{A}\left(F_i\right)=1 \; \forall i$ - then the assignment must also make $G$ true. 
But that is exactly the definition of a semantic conclusion.
\end{proof}

\begin{remark}
A formula $F$ is a tautology if and only if $F$ is a conclusion of 1, i.e., $1 \vDash F: F$ is a tautology if and only if the implication $1 \rightarrow F$ is a tautology.

A formula $F$ is unsatisfiable if and only if 0 is a conclusion of $F$, i.e., $F \vDash 0$ : $F$ is unsatisfiable if the implication $F \rightarrow 0$ is a tautology.
\end{remark}


\subsection{Proof theory of propositional logic}
The goal of a proof theory is to decide semantic questions such as
\begin{itemize}
  \item Is a formula $F$ a tautology?
  \item Is a formula $F$ unsatisfiable?
  \item Does $\{F_1, \ldots, F_n\} \models G$ hold?
\end{itemize}
In the end, this is always the same type of questions, that boils down to the question of whether a certain formula is a tautology or not.

\begin{example}[Tautology Problem of CNF]
Given a formula $F$ in CNF (Def~\ref{def:CNF}), how can we decide wether it is a tautology?
Since $F$ is a conjunction of subformulas $F_i$, which are themselves disjunctions of literals:
$$
F = \underbrace{(L_{1,1} \vee \ldots \vee L_{1, m_1})}_{=: F_1} \wedge \ldots \wedge \underbrace{(L_{n, 1} \vee \ldots \vee L_{n, m_n})}_{=: F_n} = F_1 \wedge \ldots \wedge F_n
$$
The formula $F$ is a tautology if and only if all $F_i$ are.
The $F_i$ are tautologies if at least one atom appears twice therein, once negated and once not negated. (Otherise, a violating assignment can always be found.)
\end{example}

\begin{example}[Tautology Problem of DNF]
Can we derive a similarly simple criterion for a formula $F$ in DNF (Def~\ref{def:DNF})?
$$
F = \underbrace{(L_{1,1} \wedge \ldots \wedge L_{1, m_1})}_{=: F_1} \vee \ldots \vee \underbrace{(L_{n, 1} \wedge \ldots \wedge L_{n, m_n})}_{=: F_n} = F_1 \vee \ldots \vee F_n
$$
Unfortunately, the problem does not "localize" (reduce to similar questions independently for the subformulas) in the same sense: 
The relation between subformulas is important here. 
Clearly, what can always be done is a truth table. 
However, the size of that grows exponentially with the number of different atoms in the formula.
\end{example}  

\paragraph{Relating DNF and CNF}
Is it easy to change from one normal form to the other, for a formula $F$? 
No, but what we can do is to obtain the negation of $F$ in the other normal form in which $F$ itself is given, using de Morgan's law:
$$
\neg F = \neg \bigvee_{i=1}^n\left(\bigwedge_{j=1}^{m_i} L_{i, j}\right) \equiv \bigwedge_{i=1}^n\left(\neg \bigwedge_{j=1}^{m_i} L_{i, j}\right) \equiv \bigwedge_{i=1}^n\left(\bigvee_{j=1}^{m_i} \neg L_{i, j}\right)
$$
As $F$ being a tautology is equivalent to $\neg F$ being unsatisfiable, the satisfiability problem for a DNF can be solved analogously to the tautology problem of a CNF: 
They are both simple.
The other problems (satisfiability for CNF, tautology for DNF, change from CNF to DNF and vice versa for some $F$ ) seem hard. 
Let us look at this is some more detail.

\paragraph{Comparison of computational hardness}
Let us compare the hardness of the tautology problems for CNF and DNF, respectively:
If $F$ is given in CNF, one merely has to check the double occurrence of an atom (once positive, once negated) in each of the subformulas. 
This can be done in essentially linear time, i.e., the number of steps being upper-bounded by a linear function in the length of the formula, simply going through the formula. 
On the other hand, in the case of $F$ being a DNF, one has to write down the entire truth table, containing $2^l$ rows. 
The number of computational steps is thus exponential in the length $l$, growing much faster. In a sense. the two problems are opposite extremal cases of hardness. 
Let us look at that in some more depth.



\subsubsection{Short excursion into complexity theory}
\begin{center}
% \begin{tikzpicture}[scale=0.8]
% \filldraw[color=blue!60, fill=blue!20, very thick](0,0) circle (2.5);
% \filldraw[color=red!60, fill=red!20, very thick](0,1) circle (1.0);
%     \node[align=center] at (0,1) {P};
%     \node[align=center] at (0,-0.5) {NP};
%     \node[align=center] at (0,-1.7) {NPC};
%     \draw (1.5,1.5) -- (3,2.5); % please make this line intersect with the circle (stop at the point of intersection)
% \node[above] at (3,2.5) {all computational problems};
% \draw[dashed, thick] (-1.65,-1.7).. controls (-0.5,-1) and (0.5,-1).. (1.65,-1.7);
% \end{tikzpicture}
\begin{tikzpicture}[scale=0.8]
  % Draw the two circles
  % \filldraw[color=blue!60, fill=blue!20, very thick]
  \filldraw[color=black, fill=green!20, very thick]
    (0,0) circle (2.5)  coordinate (C);  % big NP circle
  \filldraw[color=black, fill=green!20, very thick]
    (0,1) circle (1.0);                  % small P circle

  % Labels
  \node at (0,1)   {P};
  \node at (0,-0.5){NP};
  \node at (0,-1.7){NPC};

  % Dashed NPC boundary
  \draw[dashed, thick]
    (-1.65,-1.7) .. controls (-0.5,-1) and (0.5,-1) .. (1.65,-1.7);

  % Define the line path and the circle path
  \path[name path=ray] (1.5,1.5) -- (3,2.5);
  \path[name path=circle] (0,0) circle (2.5);

  % Compute their intersection
  \path[name intersections={of=ray and circle, by=I}];

  % Draw only up to the intersection point
  \draw[] (3,2.5) -- (I);

  % Place label at that intersection
  \node[above] at (3,2.5) {all computational problems};
\end{tikzpicture}
\end{center}

Complexity theory is about classifying computational problems by their computational difficulty. The set P contains all problems that can be solved in polynomial time, i.e., the number of computational steps being at most a polynomial function of the input size. P is a subset of NP, containing all problems for which one can verify a given solution in polynomial time, whereas the solution may not be found in polynomial time.

A subset of NP are the NP-complete problems (NPC): Problems in NPC are as hard as any other problem in NP. That means that any problem in NP can be reduced to any problem in NPC in polynomial time. Consequently, any problem in NPC can be reduced to another problem in NPC in polynomial time. This means that NPC problems can be used as indicators of simplicity for all NP problems: If you can solve one NPC problem in polynomial time, then you can solve all of them. (In fact, you have then shown $\mathrm{P}=\mathrm{NP}=\mathrm{NPC}$, which would be an overly surprising result that makes you very famous; most people believe that the three classes P, NP, and NPC are all different from each other - no one has proven that so far, however.)


\begin{example}[Elements of P] 
P contains
\begin{itemize}
\item the tautology problem for CNF;
\item the (un)satisfiability problem for DNF.
\end{itemize}
Even more, these decision problems can be solved not only in polynomial but even in linear time; they are among the simplest even among P problems. 
\end{example}


\begin{example}[Elements in NPC]
NPC contains
\begin{itemize}
\item the tautology problem for DNF;
\item the satisfiability problem for CNF
\item the problem to find a semantically equivalent DNF for a given CNF, and vice versa.
\end{itemize}
The last follows from the first two: If we could find a $G$ in CNF for any $F$ in DNF with $F \equiv G$ in polynomial time, we could solve the tautology problem for a DNF in polynomial time by first turning it into a CNF and then applying the criteria above.
\end{example}

\paragraph{Real-life problems?}
Are problems in real life hard? Yes, they are, as one can see in the following example.
\begin{example}[Sudoku]
Sudoku is an NPC problem, as it boils down to the satisfiability of a CNF: As one has to find a solution satisfying all conditions on the rows, columns, and subboxes, it is a problem of the form
\begin{equation}\label{eq:real_life_problem}
(\text{condition 1}) \wedge (\text{condition 2}) \wedge \ldots
\end{equation}
Each of these conditions can be satisfied in different ways:
$$
\text { condition } i =\text { way } 1 \vee \text { way } 2 \vee \text { way } 3 \vee \ldots
$$

This is a CNF; to find a solution is the same as proving satisfiability. 
The satisfiability problem for a CNF is in NPC; so is Sudoku.
\end{example} 

Actually, this is a common structure of real-life problems. 
Usually there is a number of necessary conditions yielding a formula of the form \eqref{eq:real_life_problem}.
Each condition can be met in various ways, so the subformulas are disjunctions. 
Thus, one is usually looking for an assignment satisfying a CNF.




\subsection{The resolution calculus}
Although there is no hope to solve satisfiability (SAT) for CNF formulas efficiently \emph{always}, we discuss here a calculus that can do it \emph{often}.
\begin{definition}[Resolution Step]
Let $C_1, C_2$, and $R$ be clauses. 
$R$ is the resolvent of $C_1$ and $C_2$ if there exists a literal $L$ such that $L$ is in $C_1$ and $\neg L$ is in $C_2$ and $R$ contains all literals in $C_1$ and $C_2$ except of $L$ and $\neg L$.
In set notation:
\[
R=C_1 \backslash\{L\} \cup C_2 \backslash\{\neg L\} .
\]
The rationale is that $C_1$ and $C_2$ semantically imply $R$
\[
\left\{C_1, C_2\right\} \vDash R . \qedhere
\]
\end{definition}

Resolution does not merely show unsatisfiability but also yields an assignment if the formula is satisfiable.

That is, if a formula is not unsatisfiable, then the resolution calculus leads us to a satisfying assignment.
A functional programming language directly based on resolution calculus is PROLOG.




% \section{Set Theory}\label{sec:set-theory}
% Let us turn to the second pillar of mathematics, namely, set theory. As mentioned in the Introduction, all objects in mathematics are sets. For instance, the natural numbers can be defined inductively, starting from the empty set, through never‐ending formation of new sets:
% \begin{align}\label{eq:natural-numbers}
% 0 &:= \emptyset,\\
% 1 &:= \{0\} = \{\emptyset\},\nonumber\\
% 2 &:= \{0,1\} = \{\emptyset,\{\emptyset\}\},\nonumber\\
% \vdots\quad &\nonumber\\
% n &:= \{0,1,\dots,n-1\}.\nonumber
% \end{align}

% \subsection{Basic notions}\label{subsec:basic-notions}
% A set is a collection of objects—where all these objects are sets themselves. Thus, set theory is about a relation, called the \emph{element relation}, among sets.

% \begin{definition}[Element Relation]\label{def:element-relation}
% The relation “is an element of” relates a set $x$ with a set $A$. We write
% \[
% x\in A,
% \]
% and if $x$ is not in $A$ we write
% \[
% x\notin A.
% \]
% \end{definition}

% \subsubsection{Cantor's Paradise}\label{subsubsec:cantor-paradise}
% Georg Cantor's naive definition:

% \begin{definition}[Cantor's naive approach]\label{def:cantor-naive}
% Any collection of well‐distinguished objects is a set.
% \end{definition}

% This allows in particular that a set might contain itself. Russell's paradox shows this leads to inconsistency: consider
% \[
% M := \{\,B \mid B\notin B\}.
% \]
% Asking whether $M\in M$ yields a contradiction. Naive set theory thus must be replaced by an axiomatic theory.

% \subsubsection{Zermelo-Fraenkel-Choice (ZFC) set theory}\label{subsubsec:zfc}
% ZFC is a collection of axioms prescribing how sets can be formed.

% \begin{definition}[Quantifiers]\label{def:quantifiers}
% The universal quantifier $\forall$ and existential quantifier $\exists$ from predicate logic are used as usual.
% \end{definition}

% \begin{axiom}[Extensionality]\label{ax:extensionality}
% Two sets are equal iff they have the same elements:
% \[
% \forall A,B\bigl(\forall x\,(x\in A\Leftrightarrow x\in B)\Rightarrow A=B\bigr).
% \]
% \end{axiom}

% \begin{definition}[Predicate]\label{def:predicate}
% A predicate on $A$ is a function $P\colon A\to\{\text{false},\text{true}\}$. We write
% \[
% \{x\in A\mid P(x)\}
% \]
% for the subset of $A$ where $P$ is true.
% \end{definition}

% \begin{axiom}[Separation / Subsets]\label{ax:separation}
% For any set $A$ and predicate $P$ there is a set
% \[
% \{x\in A \mid P(x)\}.
% \]
% \end{axiom}

% \begin{definition}[Subset]\label{def:subset}
% $A\subseteq B$ iff $\forall x\,(x\in A\!\Rightarrow\!x\in B)$.
% \end{definition}

% \begin{theorem}\label{thm:subset-extensionality}
% If $A\subseteq B$ and $B\subseteq A$ then $A=B$.
% \end{theorem}
% \begin{proof}
% Immediate from Axiom~\ref{ax:extensionality} and the definition of $\subseteq$.
% \end{proof}

% One obtains the empty set by separation on any nonempty $A$ with the predicate $x\neq x$.

% \begin{definition}[Intersection]\label{def:intersection}
% \[
% A\cap B := \{\,x\mid x\in A\wedge x\in B\}.
% \]
% \end{definition}

% \begin{axiom}[Union]\label{ax:union}
% \[
% A\cup B := \{\,x\mid x\in A\vee x\in B\}
% \]
% is a set.
% \end{axiom}

% \begin{definition}[Difference and Symmetric Difference]\label{def:differences}
% \[
% A\setminus B := \{\,x\mid x\in A\wedge x\notin B\}, 
% \quad
% A\triangle B := (A\setminus B)\cup(B\setminus A).
% \]
% \end{definition}

% \begin{definition}[Power Set]\label{def:powerset}
% \[
% \mathbb{P}(A) := \{\,X\mid X\subseteq A\}.
% \]
% \end{definition}

% \begin{definition}[Complement]\label{def:complement}
% If $A\subseteq\mathcal U$ then 
% \[
% \bar A := \mathcal U\setminus A.
% \]
% \end{definition}

% For families $\{A_i\}_{i\in I}$ one defines
% \[
% \bigcup_{i\in I}A_i,\quad
% \bigcap_{i\in I}A_i
% \]
% in the obvious way by $\exists i$ or $\forall i$.

% \begin{axiom}[Choice]\label{ax:choice}
% Given a family of nonempty sets, there is a function selecting one element from each.
% \end{axiom}

% \subsubsection{Laws derived from logic}\label{subsubsec:laws-logic}
% Replacing $\wedge,\vee,\neg$ by $\cap,\cup,{}^c$ in logical identities yields:
% \[
% A\cap A=A,\quad A\cup A=A,\quad A\cap(A\cup B)=A,\quad (A\cap B)\cup C=(A\cup C)\cap(B\cup C),
% \]
% \[
% (A\cup B)^c = A^c\cap B^c,\quad (A\cap B)^c = A^c\cup B^c.
% \]

% \begin{example}\label{ex:absorption}
% Prove $A\cap(A\cup B)=A$ by mutual inclusion; see standard textbook proof.
% \end{example}

% \subsubsection{The Cartesian product}\label{subsubsec:cartesian}
% Ordered pairs are defined by Kuratowski:
% \[
% (x,y):=\{\{x\},\{x,y\}\}.
% \]
% Then
% \[
% A\times B := \{(a,b)\mid a\in A,\;b\in B\}.
% \]

% The definitions of ordered pairs extends naturally to ordered lists of more than two numbers, so called tuples. Given a finite index set, $I=\{1, \ldots, k\}$ we define the Cartesian product of $k$ sets as
% $$
% \underset{i \in I}{X} A_i:=\left\{\left(a_1, \ldots, a_k\right) \mid \forall i \in I\left(a_i \in A_i\right)\right\}
% $$


% \begin{example}\label{ex:order-relation}
% The relation $\{(x,y)\in\R^2\mid x\le y\}$ is the usual order on~$\R$.
% \end{example}

% \subsection{Relations}\label{sec:relations}
% \begin{definition}[Binary Relation]\label{def:binary-relation}
% A relation $R$ from $A$ to $B$ is a subset $R\subseteq A\times B$. We write $aRb$ for $(a,b)\in R$.
% \end{definition}

% Set‐operations on relations and their connection with logical connectives follow directly.

% \subsubsection{Representation of relations}\label{subsubsec:repr-relations}
% Finite relations $R\subseteq A\times B$ can be shown as a $0$--$1$ matrix or as a bipartite graph.

% \subsubsection{Properties of relations}\label{subsubsec:props-relations}
% For $R\subseteq A\times A$:
% \begin{itemize}
%   \item \emph{Reflexive:} $\forall a\,(a,a)\in R$.
%   \item \emph{Irreflexive:} $\forall a\,(a,a)\notin R$.
%   \item \emph{Symmetric:} $(a,b)\in R\Leftrightarrow(b,a)\in R$.
%   \item \emph{Antisymmetric:} $(a,b)\in R\wedge(b,a)\in R\Rightarrow a=b$.
%   \item \emph{Transitive:} $(a,b)\in R\wedge(b,c)\in R\Rightarrow(a,c)\in R$.
% \end{itemize}

% \subsubsection{Equivalence relations}\label{subsubsec:equivalence}
% \begin{definition}[Equivalence Relation]\label{def:equivalence}
% $R$ on $A$ is an equivalence if it is reflexive, symmetric, and transitive.
% \end{definition}

% Equivalence relations partition $A$ into \emph{equivalence classes} $[a]=\{x\mid x\sim a\}$.

% \begin{theorem}\label{thm:equiv-partition}
% Equivalence relations on $A$ are in bijection with partitions of $A$.
% \end{theorem}
% \begin{proof}
% Standard mutual‐construction proof using reflexivity, symmetry, transitivity.
% \end{proof}

% \subsubsection{Order relations}\label{subsubsec:orders}
% \begin{definition}[Partial Order]\label{def:partial-order}
% A relation $\le$ on $A$ is a partial order if it is reflexive, antisymmetric, and transitive.
% \end{definition}

% If in addition $\forall x,y\;(x\le y\vee y\le x)$ it is a \emph{total} (linear) order.

% Maximal and greatest elements of a poset are defined in the usual way.

% \subsection{Functions}\label{sec:functions}
% \begin{definition}[Function]\label{def:function}
% A function $f\colon A\to B$ is a relation such that
% \[
% \forall a\in A\,\exists!\,b\in B:\;(a,b)\in f.
% \]
% We write $f(a)=b$.
% \end{definition}

% \begin{definition}[Injective / Surjective / Bijective]\label{def:inj-surj-bij}
% \begin{itemize}
%   \item $f$ \emph{injective} iff $f(a)=f(a')\Rightarrow a=a'$.
%   \item $f$ \emph{surjective} iff $\forall b\in B\,\exists a\in A:f(a)=b$.
%   \item $f$ \emph{bijective} iff it is both injective and surjective.
% \end{itemize}
% \end{definition}

% Cardinality is compared via injective and bijective maps:

% \begin{definition}[Cardinality]\label{def:cardinality}
% $A\preccurlyeq B$ iff there is an injective $f\colon A\to B$.  
% $A\approx B$ iff there is a bijection $f\colon A\to B$.
% \end{definition}

% \begin{theorem}[Cantor-Schröder-Bernstein]\label{thm:csb}
% If $A\preccurlyeq B$ and $B\preccurlyeq A$ then $A\approx B$.
% \end{theorem}

% \begin{theorem}[Cantor]\label{thm:cantor}
% For any set $A$, $A\prec\mathbb P(A)$, i.e.\ no bijection $A\to\mathbb P(A)$ exists.
% \end{theorem}
% \begin{proof}
% Diagonal‐set argument: $B=\{a\in A\mid a\notin f(a)\}$ leads to a contradiction.
% \end{proof}

% This completes the chapter on set theory.









\clearpage
\section{Set Theory}\label{sec:set-theory}

Let us turn to the second pillar of mathematics, namely, set theory. As mentioned in the Introduction, all objects in mathematics are sets. For instance, the natural numbers can be defined inductively, starting from the empty set, through never-ending formation of new sets:
\[
\begin{aligned}
0 &:=\emptyset\\
1 &:=\{0\}=\{\emptyset\}\\
2 &:=\{0,1\}=\{\emptyset,\{\emptyset\}\}\\
&\;\vdots\\
n &:=\{0,1,\dots ,n-1\}
\end{aligned}
\]

%------------------------------------------------
\subsection{Basic notions}\label{subsec:basic-notions}


A \textit{set} is a collection of objects - where all these objects are sets themselves (remember: \emph{all} objects are). 
Thus, set theory is about a relation, called the \emph{element relation}, among sets.

\begin{definition}[Element Relation]\label{def:element-relation}
  The relation ``is an element of'' relates an object (set) \(x\)\footnote{It is common to label sets with capital letters and their elements with small letters - although, again, also the lowercase-letter objects are sets.}
  with a set \(A\).  One says ``\(x\) is an element of \(A\)'' or ``\(x\) is in \(A\)'' or ``\(A\) contains \(x\)'' and writes
  \[
  x \in A .
  \]
  If \(x\) is not in \(A\), one writes
  \[
  x \notin A \quad\text{or}\quad \neg (x\in A). \qedhere
  \]
\end{definition}


%------------------------------
\subsubsection{Cantor's Paradise}\label{subsubsec:cantor}
    
Georg Cantor is the founder of set theory.  
His definition of what a set is, and what objects it can contain, was very liberal in the sense that a set was a collection of any kind of objects with the only condition that these objects be distinguishable from each other.

\begin{definition}[Cantor's ``naïve'' approach]\label{def:cantor}
Any collection of well-distinguished objects is a set.
\end{definition}

In this \emph{liberal} definition, it is, in particular, not excluded that a set contains itself as a member. 
Unfortunately, this possibility leads to a serious problem:  
Consider the set
\[
M:=\{B \mid B \notin B\}\; .
\]
Does \(M\) contain itself?  If it does, it does not, according to the definition—a classical antinomy:  
Naïve set theory crashes.
This antinomy, which is due to the mathematician and philosopher Bertrand Russell, was, by Russell himself, put as follows: 
A barber is a man who shaves every man who does not shave himself. 
Does the barber shave himself? 
Again, he does exactly when he does not.

That was the end of Cantor's Paradise. 
The way out is a set of stricter rules about what can be a set. 
It will still be the case that sets contain sets (there is nothing else, after all), but with the new rules, there is no more such a thing as ``the set of all sets''. 
And in particular, it never occurs that a set contains itself. 
The most famous set of such rules goes back to Ernst Zermelo and Abraham Fränkel (ZF), extended by the somewhat mysterious ``axiom of choice'' (ZFC).

%------------------------------
\subsubsection{Zermelo/Fraenkel/Choice (ZFC) set theory}\label{subsubsec:zfc}
is a set of axioms describing how sets can be formed.
First, we introduce symbols from predicate logic which we use as linguistic abbreviations.

\begin{definition}[Quantifiers]\label{def:quantifiers}
The universal quantifier \(\forall\) is used to express that a statement holds \emph{for all} objects of a certain kind.
The existential quantifier \(\exists\) expresses that a statement holds \emph{for at least one} object.
\end{definition}

% \begin{example}\label{ex:quantifiers}
% ``All natural numbers are non-negative'' is written
% \[
% \forall x\in\N \,(x\ge 0),
% \]
% whereas ``there exists a natural number greater than \(100\)'' becomes
% \[
% \exists x\in\N\,(x>100)\; .
% \]
% \end{example}


%--- Extensionality --------------------------------
\begin{axiom}[Extensionality]\label{ax:extensionality}
Two sets are equal if they have the same elements:
\[
\forall A \, \forall B \, ( \forall x \, (x\in A \Leftrightarrow x\in B) \Rightarrow A=B). \qedhere
\]
\end{axiom}
While \autoref{ax:extensionality} states a sufficient condition, the necessity is logical:
The logic of equality states that if two objects are equal, they have the same properties.

\begin{definition}[Predicate]\label{def:predicate}
A \emph{predicate} on a set \(A\) is a function \(P\colon A\to\{\fals, \tru\}\).
\(P\) can also be seen as a property that all \(x \in A\) have for which \(P(x)=\tru\).
\end{definition}
\begin{axiom}[Subsets from Predicates]\label{ax:subsets-from-predicates}
Given a set \(A\) and a predicate \(P\colon A\to\{\fals, \tru \}\), the collection
\[
\{x\in A \mid P(x)\} := \{x\in A \mid P(x)=\tru\}
\]
is another set.
\end{axiom}

\begin{definition}[Subset]\label{def:subset}
\(
A\subseteq B :\Longleftrightarrow \forall x \, (x\in A\Rightarrow x\in B) 
\)
\end{definition}

\begin{theorem}\label{thm:subset-equality}
If \(A\subseteq B\) and \(B\subseteq A\), then \(A=B\).
\end{theorem}
\begin{proof}
Consequence of \autoref{def:subset} and \autoref{ax:extensionality}.
\end{proof}


The existence of an empty set is usually postulated.  
One may also obtain it from any non-empty set \(A\) using \autoref{ax:subsets-from-predicates} with predicate \(x\neq x\):
\[
\emptyset:=\{x\in A \mid x\neq x\}
\]
Out of this semen alone, the whole rich zoo of mathematical objects blossoms.


\begin{definition}[Intersection]\label{def:intersection}
\(
x\in A\cap B :\Longleftrightarrow x\in A\wedge x\in B
\)
\end{definition}

\begin{axiom}[Union]\label{ax:union}
\(
x \in A\cup B :\Longleftrightarrow x\in A\vee x\in B
\)
\end{axiom}



\begin{definition}[Difference]\label{def:difference}
\(x\in A\setminus B :\Longleftrightarrow x\in A\wedge x\notin B\).
\end{definition}

\begin{definition}[Symmetric difference]\label{def:symmetric-diff}
\(x\in A\triangle B :\Longleftrightarrow (x\in A)\oplus(x\in B)\).
\end{definition}
\(A\triangle B\) can also be written as \((A\setminus B)\cup(B\setminus A)\) or \((A\cup B)\setminus(A\cap B)\).


The definitions above show the close relation between set theory and logic: Using the logical connectives, we can define corresponding set operations.

The construction of the power set goes beyond that, and it is the most ``powerful'' of the ZFC set-forming axioms, allowing for constructing in particular very large sets from smaller ones. 
% The power set of a set $A$ is the set of all subsets of $A$.
\begin{axiom}[Power Set]\label{ax:power-set}
For every set \(A\) the power set
\[
\mathbb{P}(A):=\{X\mid X\subseteq A\}
\]
is a set.
In particular, it contains the empty set \(\emptyset\) and \(A\) itself.
\end{axiom}
The power set is also denoted $2^A$ because the cardinality (i.e. the number of elements) of the power set is
$$
|\mathbb{P}(A)|=2^{|A|}
$$
if $A$ is finite. 
This is since for each of the $|A|$ elements of $A$, we can decide whether we choose it or not for the subset: We have $|A|$ binary choices, multiplying up to $2^{|A|}$ total choices, i.e., different subsets.

\begin{definition}[Complement]\label{def:complement}
Given a universe \(\mathcal{U}\), the complement of \(A\subseteq\mathcal{U}\) is 
\[
\overline{A}:=\mathcal{U}\setminus A \qedhere
\]
\end{definition}



For a family \(\{A_i\}_{i\in I}\) we define
\[
x\in\bigcup_{i\in I}A_i :\Longleftrightarrow \exists i\in I\,(x\in A_i),\qquad
x\in\bigcap_{i\in I}A_i :\Longleftrightarrow \forall i\in I\,(x\in A_i)
\]


Let us finally look at this mysterious ``axiom of choice''. Intuitively, it asks for a quite unsurprising fact: 
If you have a family of all non-empty sets, then it is possible to choose exactly one element out of each of the (non-empty) sets in the family. 
(Another way of putting it is: The Cartesian product of a family of non-empty sets is non-empty.) 
What is so mysterious about that? Although it is intuitive - why would the claimed not be possible? 
- it has counterintuitive consequences, such as the Banach/Tarski paradox: 
A unit ball can be cut into five peaces (subsets) that can be rearranged using only rotations and translations, for obtaining two unit balls of the same size. 
This suggests that our intuition is somehow inconsistent. In this sense, Banach/Tarski is only a paradox, and not an antinomy: 
It appears weird to us, but it is not an intrinsic logical problem of the theory. Which does not mean that it does not have consequences: 
For instance, it is not possible to define a universal volume function. 
The reason why the axiom of choice appears so innocent to us is that we apply it to finite families, whereas the strange consequences come when you apply it beyond this, to infinite families. 
Major parts of mathematics depend on that axiom, such as most of functional analysis, which is the basis for quantum mechanics.
\begin{axiom}[Choice]\label{ax:choice}
Given a family of sets, each containing at least one element, it is possible to make a selection of exactly one object from each set.
\end{axiom}

%------------------------------
\subsubsection{Laws derived from logic}\label{subsubsec:logical-laws}

We can derive many laws from their logical counterparts by replacing the connectives by their set-theoretic counterparts in a logical law from \Cref{subsec:logical_laws}:
$\land\mapsto\cap$, $\lor\mapsto\cup$, $\neg \mapsto \overline{\cdot}$, $\tru \mapsto \mathcal{U}$, $\fals \mapsto \emptyset$, $\oplus\mapsto\triangle$, $\equiv \, \mapsto \, =$

%------------------------------
\subsubsection{The Cartesian product}\label{subsubsec:cartesian-product}

In the $17^{\text{th}}$ century, the early modern philosopher René Descartes introduced the Cartesian product to describe the location of points by their coordinates. 
Only later Cartesian products were formalized in set theory employing ordered pairs. 
% Consider, for example, points in the two-dimensional plane. 
% The points $(x, y)$ and $(y, x)$ are generally not the same. The order of the coordinates does matter: 
% Differently from unordered pairs, two ordered pairs ( $a, b$ ) and ( $c, d$ ) are equal if and only if $a=c$ and $b=d$. 
% How can we define such an ordered pair in set theory?

\begin{definition}[Ordered pair]\label{def:ordered-pair}
\((x,y):=\bigl\{\{x\},\{x,y\}\bigr\}\)
\end{definition}

\begin{definition}[Cartesian Product]\label{def:cartesian-product}
\(A \times B:=\{\, (a,b)\mid a\in A \wedge b\in B\,\}\)
\end{definition}

The definitions of ordered pairs extends naturally to ordered lists of more than two numbers, so called tuples. Given a finite index set, $I=\{1, \ldots, k\}$ we define the Cartesian product of $k$ sets as
\[
\bigtimes_{i\in I}A_i:=\bigl\{\,(a_1,\dots,a_k)\mid \forall i\in I,\;a_i\in A_i\,\bigr\}
\]

%------------------------------------------------
\subsection{Relations}\label{subsec:relations}


\begin{definition}[Binary relation]\label{def:binary-relation}
A (binary) relation \(R\) from \(A\) to \(B\) is a subset of the cartesian product of \(A\) and \(B\), i.e. \(R\subseteq A\times B\).  
We write \(a\;R\;b\) for \((a,b)\in R\).
\end{definition}


\newcommand{\highlightnode}[3][red!20]{% #1 = colour   #2 = node name   #3 = math symbol
  \tikzmarknode{#2}{#3}% zero-width anchor in the text
  \begin{tikzpicture}[remember picture,overlay,baseline]
    \begin{scope}[shift={(#2)}]     % paint highlight (behind the symbol), local origin = anchor position
      \node[inner sep=0pt,outer sep=0.5pt,opacity=0] (tmp) {$#3$}; % typeset the symbol once to know its size
      \fill[#1, rounded corners=0.5pt] ($(tmp.south west)$) rectangle ($(tmp.north east)$); % fill a slightly larger rectangle
      \node at (tmp) {$#3$};  % typeset the symbol *again* so it sits on top of the fill
    \end{scope}
  \end{tikzpicture}%
}


Since they are sets, we can apply set calculus on relations.
\begin{example}\label{ex:equality-relation}
For the relations 
$\highlightnode{leq1}{\leq}$, 
$\highlightnode{geq1}{\geq}$, 
$\highlightnode{le1}{<}$, 
$\highlightnode{ge1}{>}$, 
$\highlightnode{eq1}{=}$, 
$\highlightnode{neq1}{\neq}$
we have
\[
\highlightnode{leq2}{\leq} \cap \highlightnode{geq2}{\geq} = \highlightnode{eq2}{=}, 
\quad 
\highlightnode{leq3}{\leq} \mathrel{\triangle} \highlightnode{geq3}{\geq} = \highlightnode{neq3}{\neq}, 
\quad 
\highlightnode{le4}{<} \subseteq \highlightnode{leq4}{\leq},
\quad
\highlightnode{ge4}{>} \subseteq \highlightnode{geq4}{\geq},
\quad
\overline{\highlightnode{leq5}{\leq}} = \highlightnode{ge5}{>},
\quad
\overline{\highlightnode{geq5}{\geq}} = \highlightnode{le5}{<}
\qedhere
\]
\end{example}

\begin{example}[Divisibility, Modulo]\label{ex:divisibility-modulo}
An integer \(b\) is said to be divisible by another integer \(a\) if there exists an integer \(c \in \Z\) such that \(a \cdot c = b\), i.e.
\[
a \mid b \quad :\Longleftrightarrow \quad \exists c\in\Z\,:\, a\cdot c = b
\]
It is a binary relation on \(\Z\), i.e.
\(
\highlightnode{divides1}{\mid} \subseteq \highlightnode{Zsquare1}{\Z\times\Z}
\).
A binary relation on \(\R\) can be reduced to a relation on \(\Z\) by interesection:
\[
\highlightnode{RZ1}{R_\Z} = \highlightnode{RR1}{R_\R} \cap \highlightnode{Zsquare2}{\Z^2}
\]
Congruence modulo \(m\) is defined as
\[
a \equiv b \pmod m \quad :\Longleftrightarrow \quad m \mid (a-b)
\]
This is the same as saying that the integer division of each \(a\) and \(b\) by \(m\) yields the same remainder. 
Each natural number \(m\) defines a congruence relation \(\highlightnode{congruence1}{\equiv_m}\) with the modulo \(m\).
The intersection of two such congruence relations yields another one with the least common multiple%
\footnote{The least common multiple of two integers $a$ and $b$ is the smallest number $c \in \mathbb{Z}$ such that there exist integers $m, n \in \mathbb{Z}$ with $a \cdot m=c=b \cdot n$. For relatively prime numbers $a$ and $b$, the least common multiple is simply their product, $c=a \cdot b$.}
being the modulus:
\[
\highlightnode{congruence2}{\equiv_{m_1}} \cap \highlightnode{congruence3}{\equiv_{m_2}} = \highlightnode{congruence4}{\equiv_{\operatorname{lcm}(m_1,m_2)}} \qedhere
\]
\end{example}



%------------------------------
\subsubsection{Representation of relations}\label{subsubsec:relation-representation}

Relations on finite sets \(A\) and \(B\) can be represented by either binary matrices or bipartite graphs.
In matrix representation, each row corresponds to an element in $A$ and each column to an element in $B$. The entry is then 1 if and only if the corresponding pair $(a, b)$ is in $R$.
In the bipartite graph, the nodes on the left correspond to elements in $A$, the ones on the right to elements in $B$. The nodes are linked if and only if $(a, b) \in R$.

In the special case of relations on a set $A$, i.e., where $A=B$ holds, representations can be made so that every element $a$ of $A$ is drawn only once, and an arrow connects $a_1$ and $a_2$ if and only if the pair $(a_1, a_2)$ is in the relation. 
What results is a graph. 
It is, however, not a simple graph since it can have loops, i.e., a node connected by itself via an arrow. 
In some way, a graph is in fact nothing but (the representation of) a relation on its vertex set.





%------------------------------
\subsubsection{Properties of relations}\label{subsubsec:relation-properties}

For a relation \(R\subseteq A\times A\):
\begin{itemize}[leftmargin=2em]
\item \emph{Reflexive}: \(\forall a: \;(a,a)\in R\).
In matrix representation, the corresponding matrix has all \(1\) on the diagonal.
The representing graph has a loop for each of its nodes.
\item \emph{Anti-reflexive}: \(\forall a: \;(a,a)\notin R\).
The diagonal of the corresponding matrix is \(0\), and there are no loops in the associated graph.
\item \emph{Symmetric}: \(\forall a,b: \;(a,b)\in R\Rightarrow(b,a)\in R\).
For ﬁnite sets the corresponding binary matrix is symmetric. 
In the graph representation, it means that whenever there is an arrow in one direction, there is also one in the inverse direction, i.e. the graph is undirected.
\item \emph{Anti-symmetric}: \(\forall a, b: \;(a,b)\in R\wedge(b,a)\in R\Rightarrow a=b\).
In the matrix representation, whenever a non-diagonal position $(i, j)$ holds a $1$ , then the mirrored position $(j, i)$ must hold a $0$. 
A $0$ in both positions is possible.
\item \emph{Transitive}: \(\forall a,b,c: \;(a,b)\in R\wedge(b,c)\in R\Rightarrow(a,c)\in R\).
This is the central property shared by equivalence and order relations.
\end{itemize}

%------------------------------
\subsubsection{Equivalence relations}\label{subsubsec:equivalence-relations}

\begin{definition}[Equivalence relation]\label{def:equivalence-relation}
A relation \(\highlightnode{eqrel1}{\sim}\) on a set \(A\) is called an equivalence relation if it is \emph{reflexive}, \emph{symmetric}, \emph{transitive}. See \ref{subsubsec:relation-properties}.
\end{definition}

\begin{definition}[Partition]\label{def:partition}
A partition of a set \(A\) is a family of sets \((A_i)_{i\in I}\) such that
\[
\bigcup_{i\in I} A_i = A \quad \quad \text{and} \quad \quad A_i\cap A_j = \emptyset \quad \forall i,j\in I,\; i\neq j \qedhere
\]
\end{definition}
\begin{theorem}
Any equivalence relation yields a partition, and vice versa. 
More precisely, if $\highlightnode{eqrel2}{\sim}$ is an equivalence relation on $A$, then the equivalence classes
$$
[a]:=\{x \in A \mid x \sim a\} \subseteq A
$$
are a partition of $A$.
Conversely, if $\left(A_i\right)_{i \in I}$ is a partition of $A$, then the relation
$$
x \sim y \quad: \Leftrightarrow \quad \exists i \in I: x \in A_i \wedge y \in A_i
$$
is an equivalence relation.
\end{theorem}

% \begin{theorem}\label{thm:eq-partition}
% Let \(\sim\) be an equivalence relation on \(A\).  
% Then the family of equivalence classes
% \(
% \{[a]\mid a\in A\}\),
% where \([a]:=\{x\in A\mid x\sim a\}\),
% forms a partition of \(A\).  
% Conversely, every partition of \(A\) defines an equivalence relation.
% \end{theorem}

\begin{example}[Congruence modulo \(m\)]\label{ex:congruence}
The relation \(\highlightnode{congruence3}{\equiv_m}\) from Example~\ref{ex:divisibility-modulo} is an equivalence relation on \(\Z\) for any \(m\in\Z\) with \(m\) equivalence classes.  
The set of equivalence classes is denoted by \(\Z_m:=\Z/\equiv_m=\{[0],\dots,[m-1]\}\).
On \(\Z_m\), we can define
\[
[a]+[b]:=[a+b],\qquad
[a]\cdot[b]:=[a\cdot b],
\]
yielding \((\Z_m,+,\cdot)\), an algebraic structure called a ring.
\end{example}

%------------------------------
\subsubsection{Order relations}\label{subsubsec:order-relations}

% \begin{definition}[Partial order]\label{def:partial-order}
% A relation \(\preccurlyeq\) on \(A\) is a \emph{partial order} if it is reflexive, anti-symmetric, and transitive.  
% If additionally
% \(\forall x,y\in A\,\bigl(x\preccurlyeq y \vee y\preccurlyeq x\bigr)\)
% it is a \emph{total order}.
% \end{definition}

\begin{definition}[Partial Order]\label{def:partial-order}
A relation \(\highlightnode{partord1}{\preccurlyeq}\) on a set \(A\) is called a partial order if it is \emph{reflexive}, \emph{anti-symmetric}, \emph{transitive}. See \ref{subsubsec:relation-properties}.
\end{definition}

\begin{example}\label{ex:partial-orders}
Examples of partial orders include:
\begin{itemize}
\item On \(\N,\Z,\R\), $\highlightnode{leq6}{\leq}$ and $\highlightnode{geq6}{\geq}$ are partial orders, but not $\highlightnode{le6}{<}$ or $\highlightnode{ge1}{>}$, because they are not reflexive.
\item Divisibility \(\highlightnode{divides6}{\mid}\) on \(\N\), but not on \(\Z\), because it is not reflexive there.
\item The subset relation \(\highlightnode{subseteq6}{\subseteq}\) on a power set \(\mathbb{P}(B)\) is a partial order. \qedhere
\end{itemize}
\end{example}

In a poset \((A,\preccurlyeq)\) an element \(x\) is
\begin{itemize}[leftmargin=2em]
\item a \emph{maximal} element if \(\nexists y\in A\;(y\neq x\wedge x\preccurlyeq y)\)
\item the \emph{greatest} element if \(\forall y\in A,\;y\preccurlyeq x\)
\end{itemize}

Note that for a partial order, we do not require that every pair of elements is comparable.
This condition
\[
\forall x,y\in A : x\preccurlyeq y \vee y\preccurlyeq x
\]
is only required for a total order, also called linear order.
For a total order, in the directed graph representation, all nodes are connected.




%------------------------------------------------
\subsection{Functions}\label{subsec:functions}

\begin{definition}[Function]\label{def:function}
A relation \(\highlightnode{function1}{f}\subseteq A\times B\) is a function(al relation) from \(A\) to \(B\), written \(f\colon A\to B\), if it satisfies the properties
\begin{enumerate}[label=(\roman*)]
\item \(\forall a\in A \; \exists b\in B:\;(a,b)\in f\) (existence of functional value) \label{def:function:existence}
\item \((a,b)\in f \wedge (a,b')\in f \Rightarrow b=b'\) (uniqueness) \label{def:function:uniqueness}
\end{enumerate}
\ref{def:function:existence} and \ref{def:function:uniqueness} are sometimes expressed together as \(\forall a\in A \; \exists! b\in B:\;(a,b)\in f\).
\end{definition}
The definition identifies a function with the corresponding set of pairs $(a, b)$, i.e., the graph of the function. 
The following notations are commonly used for $(a, b) \in f$:
$$
f(a)=b, \quad f: a \mapsto b
$$


\begin{definition}[injective, surjective, bijective]\label{def:injsurjbij}
Let \(f\colon A\to B\).
\begin{itemize}[leftmargin=2em]
\item \(f\) is \emph{injective} if \(f(a)=f(a')\Rightarrow a=a'\)
\item \(f\) is \emph{surjective} if \(\forall b\in B\,\exists a\in A,\;f(a)=b\)
\item \(f\) is \emph{bijective} if it is both injective and surjective \qedhere
\end{itemize}
\end{definition}


% \paragraph{Cardinality.}
The properties from Definition~\ref{def:injsurjbij} can be used to introduce a relation on sets and compare them with respect to their size (cardinality).

\begin{definition}[Cardinality]\label{def:cardinality}
For sets \(A,B\):
\[
A \mathrel{\highlightnode{setsmaller1}{\preccurlyeq}} B:\Longleftrightarrow\exists\text{ injective }f\colon A\to B,
\qquad
A \mathrel{\highlightnode{setequal1}{\approx}} B:\Longleftrightarrow\exists\text{ bijective }f\colon A\to B \qedhere
\]
\end{definition}

% The relation $\preccurlyeq$ is an order relation as shown below, and yields a hierarchy of sets with respect to size.
% $\preccurlyeq$ is reflexive, as the identity function
% $$
% \operatorname{id}: A \rightarrow A \qquad f(a)=a
% $$
% is injective (actually bijective). 
% Therefore, $A \preccurlyeq A$.
% In order to prove the transitivity of the relation, we introduce the composition of functions: 
% Given a function $f: A \rightarrow B$ and a function $g: B \rightarrow C$, we define a function $h:=g \circ f: A \rightarrow C$ by $h(a)=g(f(a))$. 
% If both $f$ and $g$ are injective, then $g \circ f$ is also injective.

% A proof of this statement can be given by contradiction: 
% If $g \circ f$ was not injective, there existed elements $a$ and $a^{\prime}$ such that $g(f(a))=g\left(f\left(a^{\prime}\right)\right)$. 
% Employing the injectivity of $f$, we know that $f(a)=: b \neq f\left(a^{\prime}\right)=: b^{\prime}$. 
% Thus, there exist two arguments $b$ and $b^{\prime}, b \neq b^{\prime}$, such that $g(b)=g\left(b^{\prime}\right)$, yielding a contradiction with $g$ being injective: 
% If $A \preccurlyeq B$ and $B \preccurlyeq C$, then there exist injective functions $f: A \rightarrow B$ and $g: B \rightarrow C$. 
% The existence of the injective function $g \circ f$ proves $A \preccurlyeq C$, and transitivity.

% To complete the proof for $\preccurlyeq$ being a partial order, it remains to show (a statement resembling) anti-symmetry:
% $$
% A \preccurlyeq B \wedge B \preccurlyeq A \quad \Rightarrow \quad A \approx B
% $$
% It is not intuitively clear why this should hold, and it is the statement of a fine, subtle theorem by Cantor, Schröder, and Bernstein.

The relation \(\preccurlyeq\) is a total order.
The following theorem shows that \(\preccurlyeq\) has a property resembling anti-symmetry (see \ref{subsubsec:relation-properties}).

\begin{theorem}[Cantor/Schröder/Bernstein]\label{thm:csb}
If \(A\preccurlyeq B\) and \(B\preccurlyeq A\), then \(A\approx B\).
\end{theorem}

\begin{proof}
Imagine a park (\(=A\)) with a house (\(=B\)).
Inside the house, there is a map of the park.
If we look closely, we see the house on the map again.
This house on the map in turn contains a map fo the park containing a house containing a map of the park, and so on, ad infinitum.
As the house is inside the park, i.e. \(B\subseteq A\), there exists an injective function \(g : B \to A\).
On the other hand, the map of the park is inside the house, i.e. \(A\subseteq B\), and there exists an injective function \(f : A \to B\).
We define a bijection \(h\colon A\to B\) as follows: Points in the set ``park without house'' (\(=A\setminus B\)) are mapped to their correspondend an iteration level below.
Points in the set ``house without map'' (\(=B\setminus A\)) are mapped to themselves.
\end{proof}

\begin{theorem}[Cantor]\label{thm:cantor}
For any set \(A\) we have \(A\prec\mathbb{P}(A)\); i.e.\ the power set of \(A\) is strictly larger than \(A\) itself.
\end{theorem}

\begin{proof}
Assume \(f\colon A\to\mathbb{P}(A)\) were surjective and define set
\(
B:=\{a\in A\mid a\notin f(a)\} \subseteq A
\).
There is no \(b\in A\) with \(f(b)=B\): if \(b\in B\), then \(b\notin f(b)=B\); if \(b\notin B\), then \(b\in f(b)=B\).  Hence \(f\) is not surjective.
\end{proof}



The results above open the door to ever larger infinities:  
\(\R\) has the same size as \(\mathbb{P}(\N)\), which in turn is dwarfed by \(\mathbb{P}(\R)\), and so on ad infinitum.












\clearpage
\section{Combinatorics}
\label{sec:combinatorics}

is a collection of methods, principles, tools, techniques, and facts to count the size of finite sets with some structure. 

% -------------------------------------------------------------
\subsection{Binomial Coefficient and Pascal's Triangle}
\label{subsec:binomial-coefficient}
% \begin{center}
% \begin{tikzpicture}[x=0.9cm, y=0.6cm,
%                     % node styles
%                     pascal/.style = {font=\footnotesize},
%                     row/.style    = {font=\footnotesize, anchor=west, shift={(180:1)}},
%                     % edge styles
%                     link/.style   = {->, thin},
%                     dotted/.style args={#1}{dash pattern = on 0.1\pgflinewidth off #1\pgflinewidth, line cap = round, shorten >=2pt, shorten <=2pt, line width = 1pt},
%                     dotted/.default = 4]
% \def\NPascal{4}   % last fully drawn row index
% \def\offset{1}    % horizontal offset for row labels

% \foreach \n in {0,...,\NPascal}{
%     \node[row] (r-\n) at (-\NPascal/2 - 1 - \offset , -\n) {$n=\n$}; % row label “n = …”
%     \foreach \k in {0,...,\n}{
%         % \node[pascal] (p-\n-\k) at (-\n/2 + \k , -\n) {$\binom{\n}{\k}$}; % entries in row n
%         \pgfmathtruncatemacro{\val}{int(factorial(\n)/(factorial(\k)*factorial(\n-\k)))}
%         \node[circle, draw, fill=blue!20, pascal, inner sep = 0pt, outer sep = 3 pt, minimum size=5.5mm] (p-\n-\k) at (-\n/2 + \k , -\n) {\val};
%     }
% }

% \node[row]    (r-N)   at (-\NPascal/2-1-\offset, -\NPascal-2) {$n=\tilde n$};
% \node[circle, draw, fill=blue!20, pascal, inner sep = 0pt, outer sep = 3 pt, minimum size=5.5mm] (p-N-0) at (-\NPascal/2-1      , -\NPascal-2) {$\binom{\tilde n}{0}$};
% \node[circle, draw, fill=blue!20, pascal, inner sep = 0pt, outer sep = 3 pt, minimum size=5.5mm] (p-N-N) at ( \NPascal/2+1      , -\NPascal-2) {$\binom{\tilde n}{\tilde n}$};

% \draw[dotted] (r-\NPascal)            -- (r-N.north-|r-\NPascal);
% \draw[dotted] (p-\NPascal-0)          -- (p-N-0);
% \draw[dotted] (p-\NPascal-\NPascal)   -- (p-N-N);
% \draw[dotted] (p-N-N)                 -- (p-N-0);

% \foreach \n in {0,...,\NPascal}{
%     \draw[link] (r-\n) -- (p-\n-0);
%     \node[pascal] (k-\n) at ($(p-\n-\n)+(0.8,1.6)$) {$k=\n$};
%     \draw[link]  (k-\n) -- (p-\n-\n);
% }
% \node[pascal] (k-N) at ($(p-N-N)+(0.8,1.6)$) {$k=\tilde n$};
% \draw[link]   (k-N) -- (p-N-N);
% \draw[link]   (r-N) -- (p-N-0);

% %------------------------ARROW BETWEEN ------------------------------
% \def\xshift{8}    % horizontal distance between the two triangles
% \node[] (r-arrow) at (\xshift/2+0.5, -\NPascal/2 -0.5) {\Huge$\rightarrow$};

% %-------------------- NUMERIC TRIANGLE (right) ----------------------
% \pgfmathtruncatemacro{\LastRow}{\NPascal-0} % one row more in numeric triangle
% \foreach \n in {0,...,\LastRow}{
%     \foreach \k in {0,...,\n}{
%         % \pgfmathtruncatemacro{\val}{int(factorial(\n)/(factorial(\k)*factorial(\n-\k)))}
%         % \node[circle, draw, fill=blue!20, pascal, inner sep = 0pt, outer sep = 3 pt, minimum size=5.5mm] (q-\n-\k) at (\xshift-\n/2 + \k , -\n) {\val};
%         \node[pascal] (q-\n-\k) at (\xshift-\n/2 + \k , -\n) {$\binom{\n}{\k}$};
%     }
% }

% \node[pascal] (q-N-0)  at (\xshift-\LastRow/2-1      , -\LastRow-2) {$\binom{\tilde n}{0}$};
% \node[pascal] (q-N-N)  at (\xshift+ \LastRow/2+1      , -\LastRow-2) {$\binom{\tilde n}{\tilde n}$};

% \draw[dotted] (q-\LastRow-0)         -- (q-N-0);
% \draw[dotted] (q-\LastRow-\LastRow)  -- (q-N-N);
% \draw[dotted] (q-N-N)                -- (q-N-0);

% \end{tikzpicture}
% \end{center}



% A \emph{binomial} is an expression of the form
% $(x+y)^n$.  If we expand it, collecting like terms, we find
% \begin{equation}
%  (x+y)^n = \sum_{k=0}^n \binom{n}{k} x^{k} y^{n-k}
% \end{equation}
% where
\begin{definition}[Binomial Coefficient]\label{def:binomial-coefficient}
\begin{equation}\label{eq:recursive_binom}
\binom{n}{k} := \frac{n!}{k!(n-k)!} = \binom{n-1}{k-1} + \binom{n-1}{k}
\end{equation}
with the base cases
\begin{equation}\label{eq:base_cases_binom}
  \binom{n}{0} := \binom{n}{n} := 1 \qedhere
\end{equation}
\end{definition}
We say ``$n$ choose $k$'' referring to $\binom{n}{k}$, because it is the number of ways in which we can choose $k$ out of $n$ elements (\nameref{par:combinations_without_repetition}).
In terms of sets, it is the number of subsets of size~$k$ of a set of size~$n$.
% One proves that the number of $k$-element subsets of an $n$-element set satisfies the same recursion relation, including the base cases, as the binomial coefficient.
Let \(S\) be an \(n\)-element set and fix \(x\in S\). 
A \(k\)-subset of \(S\) either excludes \(x\) (yielding \(\binom{n-1}{k}\) choices) or includes \(x\) (yielding \(\binom{n-1}{k-1}\) choices, 
obtained by selecting the remaining \(k-1\) elements from \(S\setminus\{x\}\)). 
Hence the number of \(k\)-subsets of \(S\) is \(\binom{n-1}{k} + \binom{n-1}{k-1}\), yielding the recursion relation on the right side in~\eqref{eq:recursive_binom}.

The value of \(\binom{n}{k}\) is completely determined by the right side in~\eqref{eq:recursive_binom}, 
as it can be computed for any $n$ and $k$ by building up the following triangle employing the recursion relation in~\eqref{eq:recursive_binom}, 
starting with the base cases in~\eqref{eq:base_cases_binom}.
\[
\begin{tikzpicture}[x=1.0cm, y=0.6cm,
                    % node styles
                    pascal/.style = {font=\footnotesize},
                    row/.style    = {font=\footnotesize, anchor=west, shift={(180:1)}},
                    % edge styles
                    link/.style   = {->, thin},
                    dotted/.style args={#1}{dash pattern = on 0.1\pgflinewidth off #1\pgflinewidth, line cap = round, shorten >=2pt, shorten <=2pt, line width = 1pt},
                    dotted/.default = 4]
\def\NPascal{5}   % last fully drawn row index
\def\offset{0.7}    % horizontal offset for row labels

\foreach \n in {0,...,\NPascal}{
    \node[row] (r-\n) at (-\NPascal/2 - 1 - \offset , -\n) {$n=\n$}; % row label “n = …”
    \foreach \k in {0,...,\n}{
        \pgfmathtruncatemacro{\val}{int(factorial(\n)/(factorial(\k)*factorial(\n-\k)))}
        \node[circle, draw, fill=green!20, pascal, inner sep = 0pt, outer sep = 3 pt, minimum size=5.5mm] (p-\n-\k) at (-\n/2 + \k , -\n) {\val};
    }
}

\coordinate (r-N)   at (-\NPascal/2-1-\offset, -\NPascal-1.3);
\coordinate (p-N-0) at (-\NPascal/2-0.65, -\NPascal-1.3);
\coordinate (p-N-N) at ( \NPascal/2+0.65, -\NPascal-1.3);

\draw[dotted] (r-\NPascal)            -- (r-N-|r-\NPascal);
\draw[dotted] (p-\NPascal-0)          -- (p-N-0);
\draw[dotted] (p-\NPascal-\NPascal)   -- (p-N-N);

\foreach \n in {0,...,\NPascal}{
    \draw[link] (r-\n) -- (p-\n-0);
    \node[pascal] (k-\n) at ($(p-\n-\n)+(0.8,1.6)$) {$k=\n$};
    \draw[link]  (k-\n) -- (p-\n-\n);
}
\coordinate (k-N) at ($(p-N-N)+(0.8,1.6)$);
\draw[dotted] (k-\NPascal) -- (k-N);
\end{tikzpicture}
\]
Algorithmically speaking, simply applying the recursion in~\eqref{eq:recursive_binom} leads to very inefficient computation.  
This is because the smaller subproblems to which the original problem is reduced are solved repeatedly, even if they have already been computed.  
Computing the coefficient would amount to adding up \(1\)'s.  
The remedy when naive recursion fails in this way is \emph{dynamic programming}:  
filling a table with intermediate results (in our case, the triangle) to avoid redundant computations.


% -------------------------------------------------------------
\subsection{Urn Model}
\label{subsec:urn_models}

% We want to find the number of ways to draw $k$ elements from an urn containing $n$ elements.
% It depends on whether the order of the draws matters and whether the elements are put back after each draw.


\subsubsection{Permutations (arrange)}
\paragraph{Arrangement without repetition.} \label{par:permutations_without_repetition}
The number of possibilities to arrange $n$ distinct elements is
\begin{equation}\label{eq:permutations_without_repetition}
  \fcolorbox{red}{white}{$\displaystyle
  P_n
  =
  n!
  $}
\end{equation}

\paragraph{Arrangement with repetition.}\label{par:permutations_with_repetition}
The number of ways to arrange $n$ repeated elements that are of $l$ distinct groups with multiplicities $m_1, \ldots, m_l$ is
\begin{equation}\label{eq:permutations_with_repetition}
  \fcolorbox{red}{white}{$\displaystyle
  P_{m_1, \ldots, m_l}
  =
  \frac{\left(\sum_{i=1}^l m_i\right)!}{\prod_{i=1}^l \left(m_i!\right)}
  =
  \frac{n!}{m_1! \cdots m_l!}
  $}
\end{equation}



\subsubsection{Variations (choose and arrange)}

\paragraph{Ordered selection with repetition.} \label{par:variations_with_repetition}
After each draw, the element is put back.
For each of the $k$ draws there are $n$ choices, so the number of $k$-tuples is
\begin{equation}\label{eq:variations_with_repetition}
  \fcolorbox{red}{white}{$\displaystyle
  \overline{V}^n_k
  =
  n^k
  $}
\end{equation}

\paragraph{Ordered selection without repetition.} \label{par:variations_without_repetition}
Elements once drawn are not put back.  
The number of $k$-tuples is
\begin{equation}\label{eq:variations_without_repetition}
  \fcolorbox{red}{white}{$\displaystyle
  V^n_k
  =
  \frac{n!}{(n-k)!}
  =: 
  n^{\underline{k}} 
  $}
\end{equation}

\subsubsection{Combinations (choose)}

\paragraph{Unordered selection without repetition.} \label{par:combinations_without_repetition}
Here we simply choose $k$ elements out of~$n$ without caring for order, hence the number of combinations is
\begin{equation}\label{eq:combinations_without_repetition}
  \fcolorbox{red}{white}{$\displaystyle
  C^n_k
  =
  \frac{n!}{k! \cdot (n-k)!}
  \overset{\eqref{eq:recursive_binom}}{=:}
  \binom{n}{k}
  $}
\end{equation}

\paragraph{Unordered selection with repetition.} \label{par:combinations_with_repetition}
Here we choose $k$ elements from $n$ elements, but we are allowed to choose the same element multiple times.
This is equivalent to putting $k$ indistinguishable balls into $n$ distinguishable boxes, where each box can hold any number of balls.
We can think of the $k$ balls as $k$ stars, and the $n-1$ dividers between the boxes as $n-1$ bars:
\[
% \underbrace{%
  \star\;\star\;\cdots\;\star    % first block of stars
  \;\big|\;                      % vertical bar (size adjusted with \big)
  \star\;\cdots\;\star           % second block
  \;\big|\;
  \cdots                         % “⋯” to suggest more blocks
  \;\big|\;
  \star\;\cdots\;\star           % last block
% }_{k\ \text{stars}}
\]
The combinations are characterized merely by the order of the stars and bars, so the number is given by the number of ways to arrange $k$ indistinguishable stars and $n-1$ indistinguishable bars (\nameref{par:permutations_with_repetition}).
\begin{equation}\label{eq:combinations_with_repetition}
  \fcolorbox{red}{white}{$\displaystyle
  \overline{C}^n_k
  =
  \underbrace{
  \frac{(n+k-1)!}{k! \cdot (n-1)!}
  }_{
  \hat{=}
  P_{k, n-1}
  }
  =
  \underbrace{
  \binom{n+k-1}{k}
  }_{
  \hat{=}
  C^{n+k-1}_{k}
  }
  $}
\end{equation}
It is also equivalent to distributing $k$ indistinguishable balls into $k + n - 1$ distinguishable spots, where each spot can hold at most one ball.
This is the same as choosing $k$ spots from $n + k - 1$ available spots (\nameref{par:combinations_without_repetition}).

% \begin{tikzpicture}[
%       box/.style={draw, diamond, thick, text centered, minimum height=0.5cm, minimum width=1cm},
%       line/.style={draw, thick, -latex'}
%     ]
%         \node [box]                                    (x3)      {$x_{3}$};
%         \node [box, below=0.5cm of x3, xshift=-3cm]    (x1sx)    {$x_{1}$};
%         \node [box, below=0.5cm of x3, xshift=3cm]     (x2dx)    {$x_{2}$};
%         \node [box, below=0.5cm of x1sx, xshift=-1cm]  (x2sx)    {$x_{2}$};
%         \node [box, below=0.5cm of x2sx, xshift=1cm]   (A2sx)    {$A_{2}$};
%         \node [box, below=0.5cm of x2sx, xshift=-1cm]  (A1sx)    {$A_{1}$};
%         \node [box, right=1cm of A2sx]                 (A3sx)    {$A_{3}$};
%         %
%         \node [box, below=0.5cm of x2dx, xshift=1cm]   (x1dx)    {$x_{1}$};
%         \node [box, below=0.5cm of x1dx, xshift=-1cm]  (A2dx)    {$A_{2}$};
%         \node [box, below=0.5cm of x1dx, xshift=1cm]   (A3dx)    {$A_{3}$};
%         \node [box, left=0.5cm of A2dx]                (A1dx)    {$A_{1}$};
%         %
%         \path [line] (x3) -|         (x2dx);
%         \path [line] (x3) -|         (x1sx);
%         \path [line] (x2dx) -|       (x1dx);
%         \path [line] (x2dx) -|       (A1dx);
%         \path [line] (x1dx) -|       (A2dx);
%         \path [line] (x1dx) -|       (A3dx);
%         \path [line] (x1sx) -|       (x2sx);
%         \path [line] (x1sx) -|       (A3sx);
%         \path [line] (x2sx) -|       (A1sx);
%         \path [line] (x2sx) -|       (A2sx);
%     \end{tikzpicture}
% -------------------------------------------------------------
\subsection{Rules and strategies}
\label{subsec:rules_strategies}

When faced with a counting problem, it is often possible to count parts of the set using the standard urn models (\ref{subsec:urn_models}). 
What remains is then the problem of composing the partial solutions.
We recall some basic principles.

\paragraph{Sum Rule.} \label{par:sum_rule} 
For a family $(A_i)_{i=1,\dots,n}$ of mutually disjoint sets, 
  \[
  \left|\bigcup_i A_i\right| = \sum_i |A_i|
  \]

\paragraph{Product Rule.} \label{par:product_rule}
For a family $(A_i)_{i=1,\dots,n}$ of sets (may not be disjoint), 
  \[
  \left|\bigtimes_i A_i\right| = \prod_i |A_i|
  \]

\paragraph{Equality Rule.} \label{par:equality_rule}
Finite sets in bijection have the same number of elements.

\subsubsection{Inclusion-Exclusion Principle.} \label{sec:inclusion_exclusion}
We generalize the \nameref{par:sum_rule} to sets that are not mutually disjoint.
\begin{theorem}[Inclusion/Exclusion]\label{thm:inclusion_exclusion}
For a family $(A_i)_{i=1,\dots,n}$ of sets (may not be disjoint),
\begin{equation} \label{eq:inclusion_exclusion}
  \left|\bigcup_{i=1}^n A_i\right| = \sum_{r=1}^n (-1)^{r-1}
  \sum_{1\le i_1<\dots<i_r\le n} \left|\bigcap_{k=1}^r A_{i_k}\right| 
\end{equation}
The outer sum runs over all possible sizes \(r\) of intersections, from \(1\), i.e., the single sets, to \(n\), i.e., the intersection of all sets.
Therefore, for \(r=1\), the inner sum contains \(n\) terms of the form \(\left|A_i\right|\), and for \(r=n\), it contains only one term, \(\left|A_1 \cap \ldots \cap A_n\right|\).
In general, the inner sum contains \(\binom{n}{r}\) terms, as it runs over all possible $r$-fold intersections of $n$ sets, i.e. all possible ways of choosing \(r\) sets out of \(n\) sets (\nameref{par:combinations_without_repetition}).
\end{theorem}
\begin{proof}\label{proof:inclusion-exclusion}
Induction over \(n\).
\begin{enumerate}[partopsep=0em, topsep=0em, label=(\roman*)]
\item \textbf{Base case.} \label{proof:inclusion-exclusion-BC}
The union of two sets (\(n=2\)), disjoint or not, is of size
$$
\left|A_1 \cup A_2\right|=\left|A_1\right|+\left|A_2\right|-\left|A_1 \cap A_2\right|
$$
The rationale is: 
If we simply add the two sizes, then the ``overlap'', i.e., the elements that are in both sets, are counted twice; 
hence, we have to ``remove them once''. \textcolor{Green}{\ding{52}} 
% (Actually, the case $n=2$ is not only important as a base case, but is explicitly invoked in the induction step.)

\item \textbf{Induction hypothesis.} \label{proof:inclusion-exclusion-IH}
Assume \eqref{eq:inclusion_exclusion} holds for the union of \(n\) sets, for some~\(n~\in~\N\).

\item \textbf{Induction step.} \label{proof:inclusion-exclusion-IS}
Consider a union of a family of $(n+1)$ sets - of which we single out the $(n+1)^\text{th}$ set.
\vspace{-\baselineskip}
\begin{align}
\left|\bigcup_{i=1}^{n+1} A_i\right| = & \left|\left(\bigcup_{i=1}^n A_i\right) \cup A_{n+1}\right| \overset{\text{\ref{proof:inclusion-exclusion-BC}}}{=} \left|\bigcup_{i=1}^n A_i\right|+\left|A_{n+1}\right|-\overbrace{\left|\left(\bigcup_{i=1}^n A_i\right) \cap A_{n+1}\right|}^{\overset{\text{\ref{subsubsec:logical-laws}}}{=}\left|\bigcup_{i=1}^n\left(A_i \cap A_{n+1}\right)\right|} \label{eq:inclusion_exclusion:step1} \\
\overset{\text{\ref{proof:inclusion-exclusion-IH}}}{=} & \sum_{r=1}^n(-1)^{r-1} \sum_{1 \leq i_1<\ldots<i_r \leq n}\left|\bigcap_{k=1}^r A_{i_k}\right|+\left|A_{n+1}\right| \notag \\
& -\sum_{r=1}^n(-1)^{r-1} \sum_{1 \leq i_1<\ldots<i_r \leq n}\left|\bigcap_{k=1}^r\left(A_{i_k} \cap A_{n+1}\right)\right| \label{eq:inclusion_exclusion:step2} \\
= & \sum_{k=1}^{n}\left|A_k\right| +\sum_{r=2}^{n} (-1)^{r-1} \sum_{1\le i_1<\dots<i_r\le n} \left|\bigcap_{k=1}^r A_{i_k}\right| +\left|A_{n+1}\right| \notag \\
& -\sum_{r=1}^{n} (-1)^{r-1} \sum_{1\le i_1<\dots<i_{r}\le n} \left|\left(\bigcap_{k=1}^{r} A_{i_k}\right)\cap A_{n+1}\right| \label{eq:inclusion_exclusion:step3} \\
= & \sum_{k=1}^{n+1}\left|A_k\right| +\sum_{r=2}^{n+1} (-1)^{r-1} \sum_{1\le i_1<\dots<i_r\le n} \left|\bigcap_{k=1}^r A_{i_k}\right| \notag \\
% & + \sum_{r=2}^{n+1} (-1)^{r-1} \underbrace{\sum_{1\le i_1<\dots<i_{r-1}\le n} \left| \left( \bigcap_{k=1}^{r-1} A_{i_k} \right) \cap A_{n+1}\right|}_{\text{\footnotesize\(\displaystyle = \sum\nolimits_{\substack{1 \le i_1 < \dots < i_{r-1} \le n\\ i_r = n+1}} \left|\bigcap\nolimits_{k=1}^{r} A_{i_k}\right|\)}} \label{eq:inclusion_exclusion:step4} \\
& + \sum_{r=2}^{n+1} (-1)^{r-1} \underbrace{\sum_{1\le i_1<\dots<i_{r-1}\le n} \left| \left( \bigcap_{k=1}^{r-1} A_{i_k} \right) \cap A_{n+1}\right|}_{ = \sum_{\substack{1 \le i_1 < \dots < i_{r-1} \le n\\ i_r = n+1}} \left|\bigcap\nolimits_{k=1}^{r} A_{i_k}\right|} \label{eq:inclusion_exclusion:step4} \\
= & \sum_{r=1}^{n+1} (-1)^{r-1} \sum_{1\le i_1<\dots<i_r\le n+1} \left|\bigcap_{k=1}^r A_{i_k}\right| \label{eq:inclusion_exclusion:step5} 
\end{align}
Note that we have used \ref{proof:inclusion-exclusion-BC} in \eqref{eq:inclusion_exclusion:step1}.
Going from \eqref{eq:inclusion_exclusion:step1} to \eqref{eq:inclusion_exclusion:step2}, we have applied \ref{proof:inclusion-exclusion-IH} twice; to \(\left|\bigcup_{i=1}^n A_i\right|\) as well as \(\left|\bigcup_{i=1}^n\left(A_i \cap A_{n+1}\right)\right|\).
In \eqref{eq:inclusion_exclusion:step2}, we split off the case \(r=1\) from the first sum, yielding \eqref{eq:inclusion_exclusion:step3}.
In order to obtain \eqref{eq:inclusion_exclusion:step4} from \eqref{eq:inclusion_exclusion:step3}, the index $r$ in the last term was shifted and the emerging $1/(-1)$ was taken in front of the sum.
Finally, to obtain \eqref{eq:inclusion_exclusion:step5} from \eqref{eq:inclusion_exclusion:step4}, the terms containing $i_r=n+1$ were combined with the terms not containing $i_r=n+1$ and $\sum_{k=1}^{n+1}\left|A_k\right|$ was absorbed into the resulting sum as $r=1$.
\textcolor{Green}{\ding{52}}
\end{enumerate}
\nopagebreak
This concludes the proof of the \nameref{thm:inclusion_exclusion} Theorem.
\end{proof}




\begin{example}[Opera]\label{ex:opera}
During an opera with $n$ guests, all the coats in the cloakroom get disordered, and every guest gets back a random coat afterwards.
We want to find the probability that at least one guest gets back their own coat.
If no guest receives their own coat, the corresponding permutation is \emph{fixed-point free}.

We denote the set containing all permutations with at least one fix-point as $A$.
We can express this set as a union of sets $A_i$, where $A_i$ is the set of permutations for which $i$ is a fixed point (there can be other fixed points). 
Then,
% \begin{equation}\label{eq:opera_set_perm_fix}
\[
A = \bigcup_{i=1}^{n} A_i
\]
% \end{equation}
and using the \nameref{thm:inclusion_exclusion} principle, we can compute the size of $A$.
At first sight it might seem unwise to reduce counting a single set to counting an exponential number of sets.
Luckily, many of these sets have the same size, which is easy to compute \emph{and} the number of sets in such groups is easy to determine as well.

The size of $A_i$ is given by the number of permutations of the remaining $n-1$ elements, which is $(n-1)!$.
Similarly, the size of $r$-fold intersections of the sets $A_i$ is given by the number of permutations of the remaining $n-r$ elements, which is $(n-r)!$.

As mentioned in Theorem~\ref{thm:inclusion_exclusion}, the number of distinct $r$-fold intersections is given by $\binom{n}{r}$.
So,
\[
\begin{aligned}
  |A| = \left|\bigcup_{i=1}^{n} A_i\right|
      & = \sum_{r=1}^{n} (-1)^{r-1} \sum_{1\le i_1<\dots<i_r\le n} \left|\bigcap_{k=1}^r A_{i_k}\right| \\
      % & = \sum_{r=1}^{n} (-1)^{r-1} \sum_{1\le i_1<\dots<i_r\le n} (n-r)! \\
      & = \sum_{r=1}^{n} (-1)^{r-1} \binom{n}{r} (n-r)! \\
      % & = \sum_{r=1}^{n} (-1)^{r-1} \frac{n!}{r!} \\
      & = n! \sum_{r=1}^{n} \frac{(-1)^{r-1}}{r!} \\
\end{aligned}
\]
The number of fixed-point free permutations is then
\[
\#\operatorname{FPFP}(n) = n! - |A| = n! \sum_{r=0}^{n} \frac{(-1)^{r}}{r!}
\]
where $\#\operatorname{FPFP}(n)$ denotes the number of fix-point free permutations of size $n$.
For large $n$, we have \(\#\operatorname{FPFP}(n) \approx \frac{n!}{e}\).
The probability that at least one guest receives their own coat is thus given by
\[
\begin{aligned}
& \Prob(\text{at least one guest receives their own coat})  \\
& = 1 - \Prob(\text{no guest receives their own coat}) \\
& = 1 - \frac{\#\operatorname{FPFP}(n)}{n!}
= 1 - \sum_{r=0}^{n} \frac{(-1)^{r}}{r!}
\xrightarrow{n\to\infty} 1 - \frac{1}{e} 
\end{aligned}
\]
What is remarkable is that the probability quickly converges to a constant value that is independent of the number of guests.
\end{example}



\subsubsection{The Pigeonhole Principle}
\label{subsubsec:pigeonhole}
``When $n$ objects are distributed among $k$ boxes with $k<n$, at least one box contains at least two objects.''
% \begin{proof}
% Induction over $k$.
% \end{proof}

\begin{theorem}[Pigeonhole Principle]\label{thm:pigeonhole}
If a set of $n$ objects is partitioned into $k<n$ sets, then at least one of these sets contains at least 
\[
% \fcolorbox{red}{white}{$\displaystyle
\left\lceil \frac{n}{k} \right\rceil
% $}
\]
objects. 
\end{theorem}
\begin{proof}
Contradiction. Suppose that all sets in the partition have at most $\left\lceil\frac{n}{k}\right\rceil-1$ objects. Then the total number of objects is at most $k\left(\left\lceil\frac{n}{k}\right\rceil-1\right)$, which is smaller than $n$ because
\[
\begin{verticalhack}
k\left(\left\lceil\frac{n}{k}\right\rceil-1\right)<k\left(\left(\frac{n}{k}+1\right)-1\right)=k\left(\frac{n}{k}\right)=n
\end{verticalhack}
\qedhere
\]
\end{proof}


% A classic application shows that any sequence of $m^{2}+1$ distinct numbers
% contains a monotonic subsequence of length $m+1$.
% \begin{example}[Monotonic Subsequences]\label{ex:monotonic_subsequences} % intferes with qed from proof

\begin{proposition}[Erd\H{o}s-Szekeres]
A sequence of \(m^{2}+1\) distinct numbers contains a monotonic subsequence of length \(m+1\), 
and this bound is thight.
\end{proposition}

\begin{proof}
Contradiction.
Consider a sequence of \(m^{2}+1\) elements
\[
\left(a_1, \ldots, a_{m^{2}+1}\right)
\]
and assume that the longest subsequence has length \(l\leq m\).
To each element \(a_i\), assign a pair \((c_i, d_i) \in \N^2\) representing the length of the longest monotonically increasing/decreasing subsequence starting with (and including) \(a_i\).
By the assumption, we have \(c_i, d_i \leq m \; \forall i \in \{1, \ldots, m^{2}+1\}\).
Thus, there are at most \(m^{2}\) (\nameref{par:variations_with_repetition}) possible different ordered pairs \((c_i, d_i)\).
By the \nameref{thm:pigeonhole}\footnote{The \(m^2\) possible pairs correspond to the holes and the \(m^2+1\) elements of the sequence to the pigeons.}, at least two elements \(a_i\) and \(a_j\) must have the same pair \((c, d)\).
We can distinguish two cases:
\begin{enumerate}[partopsep=0em, topsep=0em, label=(\roman*)]
\item \(a_i \leq a_j \Rightarrow c_i \overset{\text{\textcolor{red}{\Lightning}}}{>} c_j\) \label{proof:monotonic_subseq:case1}
\item \(a_i \geq a_j \Rightarrow d_i \overset{\text{\textcolor{red}{\Lightning}}}{>} d_j\) \label{proof:monotonic_subseq:case2}
\end{enumerate}
In both cases, \ref{proof:monotonic_subseq:case1} and \ref{proof:monotonic_subseq:case2}, the assumption leas to a contradiction.
Hence, the longest monotonic subsequence must have length \(l \geq m+1\).
The bound is also tight, as it is easy to construct a sequence of length \(m^{2}\) of which the longest monotonic subsequence has length only \(m\).
\end{proof}


\subsubsection{Double counting}
\label{subsubsec:double_counting}

A relation $S\subseteq A\times B$ can be counted ``row-wise'' or
``column-wise'':
\[
  \left| S \right| =  \sum_{a\in A} \left|\left\{b\mid(a,b)\in S\right\}\right| = \sum_{b\in B} \left|\left\{a\mid(a,b)\in S\right\}\right|
\]

\begin{example}[Average Number of Divisors]\label{ex:average_number_of_divisors}
We want to find the average number of divisors of numbers up to $n$.
We introduce function counting the number of divisors of $k$:
\[
\nu(k) := \left|\left\{l > 0 \mid l\text{ divides }k\right\}\right|
\]
i.e. \(\nu(1) = 1\), \(\nu(2) = 2\), \(\nu(3) = 2\), \(\nu(4) = 3\), \(\nu(5) = 2\), \(\nu(6) = 4\), \(\nu(7) = 2\), etc.
Furthermore, for every prime number \(p\), we obtain \(\nu(p) = 2\).
For a given number \(n\), we are interested in calculating
\[
\frac{1}{n}\sum_{k=1}^{n} \nu(k)
\]
Summing up the values of $\nu(k)$ is equivalent to counting the dots in the left image column by column. 

\begin{figure}[ht]
\centering
\begin{subfigure}[t]{0.49\textwidth}
\centering
\begin{tikzpicture}[xscale=.35, yscale=.35, every node/.style={font=\footnotesize}]
  % ----------- parameters you can change ------------------
  \def\n{12}
  \def\m{7}

  % axes ---------------------------------------------------
  \draw[->] (0,0) -- (\n+1,0) node[right] {$k$};
  \draw[->] (0,0) -- (0,\n+1) node[right] {$l$};

  % tick marks and labels ---------------------------------
  \foreach \i in {1,...,\n}{
      \draw (\i,-0.1) -- +(0,0.2);
      \ifnum\i<\numexpr\m+1\relax
        \node[below=2pt] at (\i,0) {\i};
      \fi
  }
  \node[below=2pt] at ({(\n+\m)/2},0) {$\ldots$};
  \node[below=2pt] at (\n,0) {$n$};

  \foreach \i in {1,...,\n}{
      \draw (-0.1,\i) -- +(0.2,0);
      \ifnum\i<\numexpr\m+1\relax
        \node[left=2pt] at (0,\i) {\i};
      \fi
  }
  \node[left=2pt] at (0,{(\n+\m)/2}) {$\vdots$};
  \node[left=2pt] at (0,\n) {$n$};

  % dots at (k,l) whenever l divides k --------------------
  \foreach \k in {1,...,\n}{
    \foreach \l in {1,...,\n}{
      \pgfmathtruncatemacro{\isDivisor}{mod(\k,\l)==0 ? 1 : 0}
      \ifnum\isDivisor=1
        \fill (\k,\l) circle [radius=4pt];
      \fi
    }
  }
\end{tikzpicture}
% \caption*{Pillars of mathematics}
\end{subfigure}
\begin{subfigure}[t]{0.49\textwidth}
\centering
\begin{tikzpicture}[xscale=0.35, yscale=4.2, every node/.style={font=\footnotesize}] % 4.2 = \n * 0.35 = 12 * 0.35
  \def\n{12}
  \def\m{3}
  \def\k{7} 
  \pgfmathsetmacro{\Ymax}{1.075} % top of the y–axis

  % axes 
  \draw[->] (0,0) -- (\n+1,0) node[right] {$x$};
  \draw[->] (0,0) -- (0,\Ymax)  node[right] {};

  % rectangles
  \foreach \i in {1,...,\n}{
      \draw[fill=gray!40] (\i-1,0) rectangle  (\i,{1/(\i)});
  }

  %  curves
  \draw[thick, green!0!black, domain=1:\n, samples=140] plot (\x,{1/(\x)});
  \draw[thick, green!0!black, domain=0:\n, samples=140] plot (\x,{1/(\x+1)});
  % \node[green!0!black, right] at (\n, {1/(\n)}) {$\frac{1}{x}$};

  % ticks and labels
  \node[below=2pt, black] at (0,0) {$0$};
  \foreach \i in {1,...,\n}{
      % \draw[red, dotted, line width=0.6pt] (0,{1/(\i)}) -- (\i,{1/(\i)});  % dotted helper line
      \draw (-0.1,1/\i) -- +(0.2,0);
      \ifnum\i<\numexpr\m+1\relax
      \node[left=2pt, black] at (0,{1/(\i)}) {$1/\i$};
      \fi
      \ifnum\i<\numexpr\k+1\relax
      \node[below=2pt, black] at (\i,0) {$\i$};
      \fi
      % red point on the curve
      % \node [draw, circle, red, fill=red, inner sep=0pt, minimum size=2pt] at (\i,{1/(\i)}) {};
  }
  \node[left=2pt, black, yshift=1ex] at (0,{(1/(\n)+1/(\m))/2}) {$\vdots$};
  \node[left=2pt, black] at (0,{1/(\n)}) {$1/n$};
  % \draw (\n,0) -- ++(0,.12) node[below=4pt] {$n$};
  \node[below=2pt] at ({(\n+\k)/2},0) {$\ldots$};
  \node[below=2pt] at (\n,0) {$n$};

  % label for the area of the gray rectangles
  % \draw[thin, black, {Circle[open,scale=.8]}-] (\m+1.6,{ln(\m+0.5)/2}) to[out=0,in=180] (\n+1.2,{ln(\m-0.5)}) node[right] {$\displaystyle \log(n!)$};
  \draw[thin, black, <-] (1.5, 1/1.5) to[out=0,in=180] (4.5, 0.8) node[right] {\normalsize$\displaystyle \frac{1}{x}$};
  \draw[thin, black, <-] (0.8, 1/1.8) to[out=2,in=180] (6.5, 0.4) node[right] {\normalsize$\displaystyle \frac{1}{x+1}$};

\end{tikzpicture}
% \caption*{Matching \(\Q\) and \(\N\)}
\end{subfigure}
\end{figure}
Whereas the columns have a irregular pattern, the rows are very regular: 
Every second number is even, every third is divisible by three, etc. 
In particular, the number of points within each row is given by $\left\lfloor\frac{n}{k}\right\rfloor$: 
The fraction $\frac{n}{k}$ is rounded to the next smaller integer value. 
% (The floor function is used since 0 is not in the set: You always have to wait maximally long for the first point.) 
Replacing the sum over columns by a sum over rows, we obtain
$$
\frac{1}{n} \sum_{k=1}^n \nu(k)=\frac{1}{n} \sum_{l=1}^n\left\lfloor\frac{n}{k}\right\rfloor
$$

Now, we want to estimate how this number grows with $n$.
Note that
$
\frac{n}{l}-1 \leq\left\lfloor\frac{n}{l}\right\rfloor \leq \frac{n}{l}
$.
Therefore, we can bound the average as
$$
\left(\sum_{l=1} \frac{1}{l}\right)-1 \leq \frac{1}{n} \sum_{l=1}^n\left\lfloor\frac{n}{l}\right\rfloor \leq \sum_{l=1}^n \frac{1}{l}
$$
Thus, we require bounds on the sum $\sum_{l=1}^n 1 / l$. As shown in the image on the right, we can approximate with functions and integrate to obtain the area below the graph.

The upper bound is then
$$
\sum_{l=1}^n \frac{1}{l} \leq 1+\int_1^n \frac{1}{x} \dif x=1+\ln (n)
$$
where $\ln(\cdot)$ is the natural logarithm. Similarly, we obtain the lower bound
$$
\sum_{l=1}^n \frac{1}{l} \geq \int_0^n \frac{1}{x+1} \dif x=\ln (n+1) \geq \ln (n)
$$
Putting all this together, the average of the number of divisors for numbers between 1 and $n$ can be estimated as
\[
\ln (n)-1 \leq \frac{1}{n} \sum_{l=1}^n\left\lfloor\frac{n}{l}\right\rfloor \leq \ln (n)+1 \qedhere
\]
\end{example}





%-------------------------------------------------------------
\subsection{Binomial Coefficients: Properties and Approximations}
\label{subsec:binom-properties}

% Throughout this subsection we fix \(n,k\in\N\) with \(0\le k\le n\).  
Recall the definition of the \nameref{def:binomial-coefficient} (Definition~\ref{def:binomial-coefficient}).

%.............................................................
\subsubsection{Symmetry}
\label{subsubsec:binom-symmetry}
%.............................................................
The binomial coefficient reflects the symmetry of the Pascal triangle as
\begin{equation}\label{eq:binom-symmetry}
  \binom{n}{k} = \frac{n!}{k!(n-k)!} = \frac{n!}{(n-k)!k!} = \binom{n}{n-k}
\end{equation}
% In combinatorial terms, choosing \(k\) elements out of an \(n\)-set is equivalent to choosing the \(n-k\) elements that are \emph{not} selected.

%.............................................................
\subsubsection{Vandermonde Identity}
\label{subsubsec:vandermonde}
%.............................................................

Suppose an \(n\)-element set is partitioned into a red part of size \(r\) and a blue part of size \(n-r\):
\[
\begin{tikzpicture}[scale=0.5,every node/.style={font=\footnotesize}]

% geometry -------------------------------------------------
\def\R{4}            % radius of the big circle  (n)
\def\r{1.5}          % radius of the small circle (k)

% --- left (red) half of the big circle
\begin{scope}
  \clip (0,0) circle (\R);
  \path[fill=red!35] (-\R-1,-\R-1) rectangle (0,\R+1);
\end{scope}

% --- right (blue) half of the big circle
\begin{scope}
  \clip (0,0) circle (\R);
  \path[fill=blue!35] (0,-\R-1) rectangle (\R+1,\R+1);
\end{scope}

% outlines --------------------------------------------------
  \node[draw, circle, minimum size=0.5*2*\R cm, inner sep=0pt, thick] (bigR) at (0,0) {};
  \draw (0,-\R) -- (0,\R);
  \node[draw, circle, minimum size=0.5*2*\r cm, inner sep=0pt, thick] (smallR) at (0,-1.3) {};
% interior labels ------------------------------------------
\node at (-1.5, 2)   {$r$};
\node at ( 1.5, 2)   {$n-r$};
\node at (-0.65,-1.3){$t$};
\node at ( 0.65,-1.3){$k-t$};

% arrows and their labels ----------------------------------
\draw[<-]
  ([shift={(40:0.0)}] 40:\R) 
  to[out=40,in=190] 
  +(2,1.1)           
  node[right]{$n$};
\draw[<-]
  ([shift={(-35:0)}] $(0,-1.3)+(-35:\r)$) 
  to[out=-35,in=160]          
  +(2,-1.2)     
  node[right]{$k$};

\end{tikzpicture}
\]

The number of possibilities to choose \(k\) elements such that \(t\) are red and \(k-t\) is the product of the number of ways to choose \(t\) red elements from the red part and \(k-t\) blue elements from the blue part:
\[
  \binom{r}{t} \cdot \binom{n-r}{k-t}
\]
The total number of possibilities to choose \(k\) elements from the \(n\)-set is then the sum over all possible values of \(t\):
\begin{equation}\label{eq:vandermonde}
  \binom{n}{k} = \sum_{t=0}^{k} \binom{r}{t} \cdot \binom{n-r}{k-t}
\end{equation}
We set \(\binom{n}{k} = 0\) whenever \(k>n\) or \(k<0\).
Equation~\eqref{eq:vandermonde} is known as the \emph{Vandermonde identity}.

%.............................................................
\subsubsection{Binomial Theorem}
\label{subsubsec:binomial-theorem}
\begin{theorem}[Binomial Theorem]\label{thm:binomial-theorem}
For all \(x,y \in \R\) and \(n \in \N\) one has
\begin{equation}\label{eq:binomial-theorem}
  (x+y)^n
  =
  \sum_{k=0}^{n}\binom{n}{k}x^{k}y^{n-k} \qedhere
\end{equation}
\end{theorem}

Special choices of \((x,y)\) in \eqref{eq:binomial-theorem} give useful corollaries of Theorem \ref{thm:binomial-theorem}:

If we set $x=y=1$, we obtain the sum over one row in Pascal's triangle \eqref{eq:binomial-theorem.a}.
This is equal to the number of all subsets of an $n$-set and thus the size of the power set.
Another interesting case is \(x=-1\) and \(y=1\), which gives the alternating sum over one row in Pascal's triangle \eqref{eq:binomial-theorem.b}.
\addtocounter{equation}{-1}
\vspace{-1\baselineskip}
\begin{subequations}
  \begin{align}
  x=y=1  &\Longrightarrow \sum_{k=0}^{n}\binom{n}{k} \label{eq:binomial-theorem.a} = 2^{n} \\
  x=-1,y=1  &\Longrightarrow \sum_{k=0}^{n}(-1)^{k}\binom{n}{k}=0 \label{eq:binomial-theorem.b} 
  \end{align}
\end{subequations}

\begin{remark}[Parity of bit strings]\label{rem:even-odd-strings} 
Equation~\eqref{eq:binomial-theorem.b} shows that, for every \(n\), the number of even-parity \(n\)-bit strings (an even number of ones) equals the number of odd-parity strings.  
When \(n\) is odd this is witnessed by the involution that flips every bit, \(w\mapsto\overline{w}\).  
For even \(n\) the equality follows algebraically from the vanishing alternating sum above.
\end{remark}

%.............................................................
\subsubsection{Approximations}
\label{subsubsec:binom-approx}
%.............................................................
Exact evaluation of \(\binom{n}{k}\) can be expensive for large instances.  
Simple bounds follow directly from the factorial definition (Definition \ref{def:binomial-coefficient}, Equation~\eqref{eq:recursive_binom}).
Rearranging the factors in \eqref{eq:recursive_binom} gives
\[
\binom{n}{k} = \frac{n!}{k!(n-k)!}
= \frac{\prod_{i=0}^{k-1}(n-i)}{\prod_{i=0}^{k-1}(k-i)}
= \prod_{i=0}^{k-1}\frac{n-i}{k-i}
\geq \prod_{i=0}^{k-1}\frac{n}{k} 
= \left(\frac{n}{k}\right)^{k}
\]
How much bigger can $\binom{n}{k}$ be than $(n / k)^k$?
Consider the following ratio:
% $$
% \frac{\binom{n}{k}}{\left(\frac{n}{k}\right)^k}=\underbrace{\frac{n(n-1)(n-2) \cdots(n-k+1)}{n^k}}_{\leq 1} \underbrace{\frac{k^k}{k(k-1)(k-2) \cdots 1}}_{\leq e^k} \leq\left(\frac{n}{k}\right)^k e^k .
% $$
\[
\frac{\binom{n}{k}}{\left(\frac{n}{k}\right)^k}=\underbrace{\frac{\prod_{i=0}^{k-1}(n-i)}{\prod_{i=0}^{k-1}n}}_{\leq 1} \cdot \underbrace{\frac{k^k}{\prod_{i=0}^{k-1}(k-i)}}_{\leq e^k} \leq e^k
\]
To upper bound the second part, we used the expansion of the exponential. 
One summand is less than the entire series.

Summarizing this we obtain the bounds:
\begin{equation}\label{eq:binom-basic-bounds}
\left(\frac{n}{k}\right)^k \leq\binom{ n}{k} \leq\left(\frac{n}{k}\right)^k \cdot e^k
\end{equation}

A sharper estimate uses Stirling's formula 
\(
  n! \approx \sqrt{2\pi n}\bigl(\tfrac{n}{e}\bigr)^{n}
\) 
to estimate the factorial and can thus be used to estimate the binomial coefficient:
\[
\begin{aligned}
\binom{n}{k} & =\frac{n!}{k!(n-k)!} \approx \frac{1}{\sqrt{2 \pi}} \frac{\sqrt{n}}{\sqrt{k(n-k)}}\left(\frac{n}{e}\right)^n\left(\frac{e}{k}\right)^k\left(\frac{e}{n-k}\right)^{n-k} \\
& \approx \frac{n^n}{k^k(n-k)^{n-k}}=\frac{1}{\left(\frac{k}{n}\right)^k\left(\frac{n-k}{n}\right)^{n-k}} =\left(\frac{1}{\left(\frac{k}{n}\right)^{k / n}\left(\frac{n-k}{n}\right)^{(n-k) / n}}\right)^n
\end{aligned}
\]
In the last two steps we merely reformulate the expression in a form that turns out to yield some insight later.

Now define $x:=k / n$. Thus, $(n-k)/n = 1-x$, and we can write the estimate above as
\[
\binom{n}{k} \approx\left(x^x(1-x)^{1-x}\right)^{-n}=2^{n\left(-x \log _2 x-(1-x) \log _2(1-x)\right)}
\]
Further, we introduce the function
\[
h(x):=-x \log _2 x-(1-x) \log _2(1-x)
\]
The function \(h\) is symmetric about \(x=\tfrac12\), attains its maximum \(1\) there, and vanishes at the endpoints \(x=0\) and \(x=1\).
\[
\begin{tikzpicture}
  \begin{axis}[
      width=5cm, height=4cm,
      domain=0.0001:0.9999,            % avoid log(0)
      samples=250,
      % xlabel={$x$},
      xmin=-0.1, xmax=1.1,
      ymin=-0.1, ymax=1.1,
      grid=both,
      thick,
      axis line style={black},
      tick align=inside,
      tick label style={font=\footnotesize},
      ytick pos=right,
      ytick={0,0.2,0.4,0.6,0.8,1},
      xtick={0,0.2,0.4,0.6,0.8,1},
      yticklabel pos=right,
      ylabel={$h(x)$}
    ]
    % Binary entropy: h(x) = -x log2(x) - (1-x) log2(1-x)
    \addplot[
      Green,
    ]
      { -(x)*ln(x)/ln(2)  -  (1 - x)*ln(1 - x)/ln(2) };
  \end{axis}
\end{tikzpicture}
\]
% The graph of $h$ is shown in Figure 4.13. 
All this new formalism yields the estimate
\[
\log _2\binom{n}{k} \approx n\cdot h(x)
\]
The function $h$ plays an important role in information theory.


% \begin{digression}[Data Compression]\label{digression:data-compression}
% Consider a binary source emitting \(n\) independent bits with
% \(\Prob(0)=x>\tfrac12\) and \(\Prob(1)=1-x\). This means there is redundancy and we can compress it to a smaller length \(l\) (if \(x=\tfrac12\) we cannot compress the string).
% Strings with exactly \(xn\) zeros occur with probability
% \[
%   \binom{n}{xn} x^{xn} (1-x)^{(1-x)n}
%   \approx
%   2^{n h(x)}2^{-n h(x)} = 1 % doesnt
% \]
% where we used the results from Section~\ref{subsubsec:binom-approx}.
% Therefore, a lossless compression protocol merely encodes the roughly $\binom{n}{x \cdot n} \approx$ $2^{n h(x)}$ strings with $n \cdot x$ zeros and $(1-x) \cdot n$ ones using $l=n h(x)$ bits. 
% So the binary entropy $h(x)$ characterizes the ultimate data compression rate or the relative information content of a source.
% \end{digression}

%-------------------------------------------------------------
\subsection{Special Counting Problems}
\label{subsec:special-counting}


\subsubsection{Relations}
\label{subsubsec:relations}
A relation \(R\) on a set \(S\) is a subset of the Cartesian product \(S\times S\) (Section~\ref{subsec:relations}, Definition~\ref{def:binary-relation}).
The number of relations on a set \(S\) of size \(n\) is given by:
\begin{equation}\label{eq:number-of-relations}
\fcolorbox{red}{white}{$\displaystyle
\# R = \left| \mathbb{P}(S\times S) \right| = 2^{\left| S\times S \right|} = 2^{n^2}
$}
\end{equation}
because each element can either be in the relation or not (\nameref{par:variations_with_repetition}).

%.............................................................
\subsubsection{Equivalence Relations}
\label{subsubsec:equiv-relations}
Let \(S\) be an \(n\)-element set. 
We want to find the number of equivalence relations on \(S\), i.e. the number of partitions of \(S\) into non-empty blocks (\emph{Bell number} \(B_n\)).
Instead of counting the equivalence relations directly, we first want to find the number of partitions of \(S\) into exactly \(k\) non-empty blocks (\emph{Stirling number of the second kind} \(S_{n,k}\)).
We can separate an element \(s\in S\) and distinguish two cases:
\begin{enumerate}
\item \(s\) is in its own set and therefore adds a block by itself.
Then we have to find a \(k-1\) partition of the remaining \(n-1\) elements. 
\label{case:equiv-relations:own-set}
\item \(s\) is in a set containing also other elements.
Imaginge the remaining \(n-1\) elements are partitioned into \(k\) sets.
Then we could add \(s\) to any of those \(k\) sets.
\label{case:equiv-relations:with-other-elements}
\end{enumerate}
This gives the recursion
\begin{equation}\label{eq:stirling-second}
  S_{n,k}
  =
  S_{n-1,k-1}
  +
  k \cdot S_{n-1,k}%,
  % \quad
  % \begin{cases}
  %   S_{0,0} = 1, \\
  %   S_{n,0} = 0, \\
  %   S_{n,n} = 1
  % \end{cases}
\end{equation}
with base cases \(S_{0,0}=1\), \(S_{n,0}=0\), and \(S_{n,n}=1\). % for all \(n\geq 1\).
The total number of equivalence relations is thus 
\begin{equation}\label{eq:bell}
  B_{n} := \sum_{k=0}^{n} S_{n,k}
\end{equation}
which does not have a closed formula.

%.............................................................
\subsubsection{Permutations with Cycles}
\label{subsubsec:permutations}
A permutation is a bijective map
\[
  \pi: \{1,\ldots,n\} \to \{1,\ldots,n\}
\]
A common notation for permutations is 
\[
  \pi=\begin{pmatrix}
    1&\cdots&n\\
    \pi(1)&\cdots&\pi(n)
  \end{pmatrix}
\]
A fixed point is an element \(i\) with \(\pi(i)=i\).

Let \(S\) be an \(n\)-element set. 
We want to find the number of permutations of \(S\) with exactly \(k\) cycles (\emph{unsigned Stirling number of the first kind} \(S_{n,k}\)).
We can separate an element \(s\in S\) and distinguish two cases:
\begin{enumerate}
\item \(s\) is a fixed point and therefore adds a cycle by itself.
Then we have to find a permutation of the remaining \(n-1\) elements with \(k-1\) cycles.
\label{case:permutations:own-cycle}
\item \(s\) is in a cycle containing also other elements.
Imaginge the remaining \(n-1\) elements are permuted into \(k\) cycles.
Then we could put \(s\) at \(n-1\) positions, essentially after each of the existing elements (independent of which cycle they are in).
\label{case:permutations:with-other-elements}
\end{enumerate}
This gives the recursion
\begin{equation}\label{eq:stirling-first}
  S_{n,k}
  =
  S_{n-1,k-1}
  + 
  (n-1) \cdot S_{n-1,k}
\end{equation}
with base cases \(S_{0,0}=1\), \(S_{n,0}=0\), and \(S_{n,n}=1\).

% \begin{example}[Self-inverse permutation]\label{ex:self-inverse}
%   The permutation
%   \[
%     \pi=\begin{pmatrix}
%       1&2&3&4&5\\
%       5&4&3&2&1
%     \end{pmatrix}
%   \]
%   contains one fixed point and two \(2\)-cycles; hence
%   \(k=3\).  Because all cycles have length at most two we have
%   \(\pi=\pi^{-1}\).
% \end{example}

% \begin{example}[Mixed cycle lengths]\label{ex:mixed-cycles}
%   For
%   \[
%     \sigma=\begin{pmatrix}
%       1&2&3&4&5&6&7&8&9&10\\
%       5&9&8&10&7&6&1&3&4&2
%     \end{pmatrix}
%   \]
%   the cycle decomposition is
%   \(
%     (1\,5\,7)(2\,9\,4\,10)(3\,8)(6),
%   \)
%   consisting of one \(4\)-cycle, one \(3\)-cycle, one
%   \(2\)-cycle, and one fixed point, so \(k=4\).
% \end{example}






% \clearpage
% \section{Set Theory}
% \subsection{Basic notions}
% \subsubsection{Cantor's Paradise}
% \subsubsection{Zermelo/Fraenkel/Choice (ZFC) set theory}
% \subsubsection{Laws derived from logic}
% \subsubsection{The Cartesian product}
% \subsection{Relations}
% \subsubsection{Representation of relations}
% \subsubsection{Properties of relations}
% \subsubsection{Equivalence relations}
% \subsubsection{Order relations}
% \subsection{Functions}

% \clearpage
% \section{Combinatorics}


% \begin{tikzpicture}[
%   box/.style={rectangle, draw=black, fill=white, very thick, inner sep=0.1cm},
%   line/.style={->, thick, draw=black!50!white},
% ]
%   \node [box]     (x2dx)    {x{2}};

%   %
%   \node [box, below=0.5cm of x2dx, xshift=1cm]   (x1dx)    {x{1}};
%   \node [box, below=0.5cm of x1dx, xshift=-1cm]  (A2dx)    {A{2}};
%   \node [box, below=0.5cm of x1dx, xshift=1cm]   (A3dx)    {A{3}};
%   \node [box, left=0.5cm of A2dx]                (A1dx)    {A{1}};
  
%   %
%   \path [line] (x2dx) -|       (x1dx);
%   \path [line] (x2dx) -|       (A1dx);
%   \path [line] (x1dx) -|       (A2dx);
%   \path [line] (x1dx) -|       (A3dx);

% \end{tikzpicture}


% \begin{tikzpicture}[
%   node/.style={%
%     draw,
%     rectangle,
%   },
% ] 
%   \coordinate (starting_point) at (0,0);
%   \coordinate (first_div) at ($(starting_point) + (0,-1)$);
%   \coordinate (all_elements_corner) at ($(first_div) + (-2,0)$);

%   \node [node] (A) at (starting_point) {IDS $>$ 13.5?};
%   \path (A) ++(-135:3cm) node [node] (B) {exit 1};
%   \path (A) ++(-45:3cm) node [node] (C) {LCIanx $>$ 0.3604?};
%   \path (C) ++(-135:3cm) node [node] (D) {exit 2};
%   \path (C) ++(-45:3cm) node [node] (E) {exit 3};

%   \draw[->, thick, draw=black!50!white] (A) -| (B) node [above,pos=0.25] {no}(A);
%   \draw (A) -- (C) node [right,pos=0.25] {yes}(A);
%   \draw (C) -- (D) node [left,pos=0.25] {no}(A);
%   \draw (C) -- (E) node [right,pos=0.25] {yes}(A);
% \end{tikzpicture}



% \subsection{Basic notions}
% \subsection{Urn models}
% \subsection{Rules and strategies}
% \subsubsection{The pigeonhole principle}
% \subsubsection{Double counting}
% \subsection{Binomial coefficients}
% \subsubsection{Symmetry}
% \subsubsection{Vandermonde identity}
% \subsubsection{Binomial theorem}
% \subsubsection{Approximation of the binomial coefficient}
% \subsection{An excursion into information theory}
% \subsection{Special counting problems}
% \subsubsection{Equivalence relations}
% \subsubsection{Permutations}

\clearpage
\section{Graph Theory}
\subsection{Motivation}
graphs are useful models for many discrete problems, because they allow to focus the analysis on specific relations between objects

% \begin{example}[Noisy Channel]
% A channel is an abstract model of a device used to transmit some information (for instance, a wire, an optical fiber, or a radio signal).
% A channel can be depicted by a graph, where the input is connected with each of the possible outputs.
% Generally, channels are noisy, in that errors might occur while transmitting the signal.%
% \footnote{In a more complete model, one would assign a probability distribution over the outputs for each input. %
% Typically the ``right'' output would occur with rather high probability. %
% If the channel is noisy, there are other outputs that occur with probability larger than zero and cause the ambiguity for the receiver.}
% \begin{center}
% \begin{tikzpicture}[
%   every node/.style={font=\footnotesize},  % match the font in the picture
%   x=0.5cm, y=0.3cm]

% % ---- node styles ----------------------------------------------------------
% \tikzset{
% dot/.style = {circle, fill, inner sep=1.2pt, outer sep=5pt},   % the little black dots
% lbl/.style = {inner sep=0pt, outer sep=0pt}      % label nodes
% }

% % ---- left column (x_i) ----------------------------------------------------
% \foreach \i/\y in {0/0, 1/-1.5, 2/-3, 3/-4.5} {
% \node[dot]           (xd\i)  at (0,\y) {};
% \node[lbl, left=0mm of xd\i] (x\i) {$x_{\i}$};
% }
% % ellipsis + last item
% \node[lbl]  at (0,-6)    {$\vdots$};
% \node[dot]  (xdn)         at (0,-7.5) {};
% \node[lbl, left=0mm of xdn] (xn) {$x_{n}$};

% % ---- right column (y_j) ---------------------------------------------------
% \foreach \j/\y in {0/0, 1/-1.5, 2/-3, 3/-4.5} {
% \node[dot]           (yd\j) at (5.4,\y) {};
% \node[lbl, right=0mm of yd\j] (y\j) {$y_{\j}$};
% }
% % ellipsis + last item
% \node[lbl] at (5.4,-6)   {$\vdots$};
% \node[dot] (ydn)         at (5.4,-7.5) {};
% \node[lbl, right=0mm of ydn] (yn) {$y_{n}$};

% % ---- arrows ---------------------------------------------------------------
% % from x_0
% \draw[->] (xd0) -- (yd0);
% \draw[->] (xd0) -- (yd1);
% \draw[->] (xd0) -- (yd2);

% % from x_1
% \draw[->] (xd1) -- (yd0);
% \draw[->] (xd1) -- (yd1);

% % from x_2
% \draw[->] (xd2) -- (yd2);

% % from x_3
% \draw[->] (xd2) -- (yd3);
% \end{tikzpicture}
% \end{center}

% Nodes representing the input in the graph might be connected to more than one element in the output alphabet \(\mathcal{Y}\).
% As outputs can stem from different inputs, the receiver cannot tell which input the sender chose.
% Is there a way to transmit information without error through such a noisy channel?
% To answer the question, we first consider a different graph representation.

% An \emph{ambiguity graph} shows the conflicting inputs in a noisy channel.
% The nodes correspond to the inputs in \(\mathcal{X}\).
% Two nodes are connected if the corresponding inputs might yield the same output.

% In order to resolve the ambiguity of the inputs, the sender and the receiver agree on a code:
% The sender uses merely a subset of the input alphabet \(\mathcal{X}\) without conflicts in the output.
% This subset has to be chosen such that none of the elements are neighbors in the ambiguity graph:
% This is called an \emph{independent set}.
% The optimal code is an independent set of maximal size; to find one in a given general graph is a problem that is NP-complete.
% \end{example}



\subsection{Basic notions}
\begin{definition}[Graph]\label{def:graph}
A graph \(G=(V,E)\) consists of
\begin{itemize}
  \item a non-empty, finite vertex set \(V\), i.e. \(0 < |V| = n < \infty\).
  \item an edge-set \(E\subseteq V\times V\): \(E\) is a relation on \(V\). \qedhere
\end{itemize}
\end{definition}
This definition allows for loops, but not for more than two edges connecting the same pair of nodes. 
Graphs not having this limitatation are called \emph{multigraphs}.
\begin{definition}[Undirected graph]\label{def:undirected-graph}
A graph \(G\) is undirected if
\[
(u,v) \in E \Leftrightarrow (v,u) \in E \qedhere
\]
\end{definition}
In this case, edges edges are sometimes taken to be unordered sets (instead of ordered pairs): \(e=\{u,v\}\).
When an undirected graph is drawn, a single line is normally drawn between \(u\) and \(v\) (instead of a pair of arrows).

\begin{definition}[Simple graph]\label{def:simple-graph}
A graph \(G\) is called simple if it does not contain loops:
\[
\begin{tikzpicture}[
  every node/.style={font=\footnotesize},  % match the font in the picture
  x=0.15cm, y=0.15cm]
\useasboundingbox (-3.2,-3.2) rectangle (3.2,3.2);;

% ---- node styles ----------------------------------------------------------
\tikzset{
dot/.style = {circle, fill, inner sep=1.2pt, outer sep=3pt},   % the little black dots
lbl/.style = {inner sep=0pt, outer sep=0pt}      % label nodes
}
  \node[dot] (v) at (-1.6, -1.6) {};
  % \draw[->] (v) .. controls (-1.6, 1.6) and (1.6, 1.6) and (1.6, -1.6) .. (v);
  \draw[->, out=90, in=0, looseness=14] (v) to (v);
  % \draw[->] plot [smooth, tension=1] coordinates {(v) (-1.6, 1.6) (1.6, 1.6) (1.6, -1.6) (v)};

  %--- red cross ----------------------------------------------------------
  \draw[line width=1pt, red, line cap=round]
    (-3.2,3.2) -- (3.2,-3.2);
  \draw[line width=1pt, red, line cap=round]
    (-3.2,-3.2) -- (3.2,3.2);

\end{tikzpicture}
\qedhere
\]
\end{definition}

\begin{definition}[Neighbourhood]\label{def:neighbourhood}
For a vertex \(v\in V\) we call the set
\[
\Gamma(v) := \{w\in V\mid (v,w)\in E\}
\]
the \emph{neighbourhood} of \(v\):
\[
\begin{tikzpicture}
  \tikzset{
dot/.style = {circle, fill, inner sep=1.2pt, outer sep=3pt}
}
% \draw[blue, dashed] (\a*4,0) circle(1.5cm);
\node[regular polygon, regular polygon sides=5, minimum size=2cm] at (5*4,0) (A) {};
\foreach \i in {1,...,5}
    % \node[dot, label={[xshift=-4pt]right:$w_\i$}] (w\i) at (A.corner \i) {};
     \node[dot] (w\i) at (A.corner \i) {};

\node[dot, label={[xshift=-5pt, yshift=-2pt]above right:$v$}] (v) at (A.center) {};
\foreach \i in {1,...,5}
    \draw[->] (v) -- (w\i);
\end{tikzpicture}
\qedhere
\]
\end{definition}

\begin{definition}[Degree]
The number of edges towards a node \(v\) is called the in-degree \(\deg^-(v)\).
The number of from a node \(v\) is called the out-degree \(\deg^+(v)\).
For undirected graphs the number of edges ending in a node \(v\) is simply called the degree \(\deg(v)\).
\end{definition}

In a directed graph, any ingoing edge of a node is an outgoing edge of another node.
Further, any edge is an ingoing and an outgoing edge for some node.
Thus we obtain for directed graphs the following equation:
% \begin{equation}\label{eq:degree_sum_formula_directed}
\[
\sum_{v\in V} \deg^-(v) = \sum_{v\in V} \deg^+(v) = |E|
\]
% \end{equation}
% \begin{equation}\label{eq:degree_sum_formula}
\begin{lemma}[Handshaking]\label{lem:handshaking}
Let \(G=(V,E)\) be an undirected graph, where \(V\) is the vertex set and \(E\) the edge set.
Then
\[
\sum_{v\in V} \deg(v) = 2|E|
\qedhere
\]
\end{lemma}
% \end{equation}
A consequence of Lemma~\ref{lem:handshaking} is that in every finite undirected graph, the number of vertices with odd degree is even.
\begin{example}[Party]\label{ex:handshaking-party}
If people shake hands at a party, the number of people who shake an odd number of other people's hands is even.
\end{example}

\subsubsection{Basic notions for simple undirected graphs}
\begin{definition}[Walk]\label{def:walk}
A \(v_1\)-\(v_l\)-walk with length \(l\) is a sequence of vertices \(w = (v_1, \ldots, v_l)\) with 
\[
  (v_i, v_{i+1}) \in E \quad \forall i \in \{1, \ldots, l-1\} \qedhere
\]
\end{definition}
\begin{definition}[Trail]\label{def:trail}
  A \(v_1\)-\(v_l\)-trail is a \(v_1\)-\(v_l\)-walk for which all edges are distinct.
\end{definition}
\begin{definition}[Path]\label{def:path}
  A \(v_1\)-\(v_l\)-path is a \(v_1\)-\(v_l\)-walk for which all vertices (and therefore also all edges) are distinct:
\[
\begin{tikzpicture}
  \tikzset{
dot/.style = {circle, fill, inner sep=1.2pt, outer sep=3pt}
}
\node[dot, label={[yshift=-3pt]above:$v_1$}] (v1) at (0,0) {};
\node[dot, label={[yshift=-3pt]above:$v_2$}] (v2) at (1,0) {};
\node[dot, label={[yshift=-3pt]above:$v_3$}] (v3) at (2,0) {};
\node[] (v4) at (3,0) {$\ldots$};
\node[dot, label={[yshift=-3pt]above:$v_{l-1}$}] (vlm1) at (4,0) {};
\node[dot, label={[yshift=-3pt]above:$v_l$}] (vl) at (5,0) {};
\draw[->] (v1) -- (v2);
\draw[->] (v2) -- (v3);
\draw[] (v3) -- (2.65,0);
\draw[->] (3.35,0) -- (vlm1);
\draw[->] (vlm1) -- (vl);
\end{tikzpicture} \qedhere
\]
\end{definition}
\begin{definition}[Circuit]\label{def:circuit}
  A circuit is a closed trail (Def~\ref{def:trail}), i.e. a sequence of vertices \(c = (v_1, \ldots, v_l)\) with
\[
  (v_i, v_{i+1}) \in E \quad\forall i \in \{1, \ldots, l-1\} \quad \text{and} \quad
  (v_l, v_1) \in E \qedhere
\]
\end{definition}
\begin{definition}[Cycle]\label{def:cycle}
  A cycle is a closed path (Def~\ref{def:path}), i.e. a circuit where all vertices are distinct.
\end{definition}
\begin{definition}[Subgraph]\label{def:subgraph}
The pair \(G'=(V',E')\) is a subgraph of \(G=(V,E)\) if
\[
  V' \subseteq V \qquad E' \subseteq E \qquad E' \subseteq V'\times V' \qedhere
\]
\end{definition}
Given a vertex  set \(V' \subseteq V\), the subgraph of \(G\) induced by \(V'\), denoted by \(G[V']\), is the subgraph of \(G\) with vertex set \(V'\) and all possible edges which are also in \(G\):
\[
  \forall u,v\in V': (u,v)\in E \implies (u,v)\in E'
\]

\begin{definition}[Connected Components]\label{def:connected-components}
Let \((V_i)_{i \in I}\) be a partition of the vertex set of a graph \(G\) such that all elements within each partition \((V_i)\) are connected by a path (Def~\ref{def:path}).
That is
\[
\exists \text{$(u,v)$-path} \iff \exists i \in I: u,v \in V_i
\]
Then the induced subgraphs \(G[V_i]\) are called the \emph{connected components} of \(G\).
\end{definition}
Note that the existence of a path between vertices leads to an equivalence relation (Def~\ref{def:equivalence-relation}) on the vertex set \(V\).
Correspondingly, it can also be seen as a partition of the vertex set \(V\) into equivalence classes.

\begin{definition}[Bridge]\label{def:bridge}
An edge \(e\in E\) is called a bridge if the graph \(G' := (V,E\setminus\{e\})\) has one more connected component than \(G\).
\end{definition}

\begin{theorem}\label{thm:connected-components-inequality}
A graph \(G = (V,E)\) has at least \(|V| - |E|\) connected components, i.e.
\[
  |V| - |E| \leq c
\]
where \(c\) is the number of connected components.
\end{theorem}
\begin{proof}
  The graph \(G = (V,\emptyset)\) has \(|V|\) connected components, each containing one vertex.
  Any edge inserted into the graph reduced the number of connected components by at most one:
  \begin{itemize}
  \item If it connects two previously separate connected components, it reduces the number of connected components by one.
  \item If it connects two vertices within a connected component, it does not change the number of connected components. \qedhere
  \end{itemize}
\end{proof}

\begin{corollary}
  \label{cor:connected-components-inequality}
  If a graph \(G = (V,E)\) is connected, i.e. \(c=1\), then
  \[\fcolorbox{red}{white}{$\displaystyle
    |V| - |E| \leq 1
    $} \qedhere
  \]
\end{corollary}

In a minimally connected graph, the diﬀerence of the number of edges and the number of vertices reaches the bound from Corollary~\ref{cor:connected-components-inequality}.
All edges are bridges (Def~\ref{def:bridge}).
These minimally connected graphs are called trees (see Section~\ref{subsec:trees}).


\subsection{Trees} \label{subsec:trees}
A tree is a minimally connected graph.
A graph containing trees as its connected components is called a forest.
% \begin{center}
% \tikzset{
%   treetop/.style = {decoration={random steps, segment length=0.4mm}, decorate},
%   trunk/.style   = {decoration={random steps, segment length=2mm, amplitude=0.2mm}, decorate}
%   }
% \tikzset{
%    my tree/.pic={
%      \foreach \w/\f in {0.3/30,0.2/50,0.1/70} {
%        \fill [brown!\f!black, trunk] (-\w/2,0) rectangle +(\w,3);
%      }
%      \foreach \n/\f in {1.4/40,1.2/50,1/60,0.8/70,0.6/80,0.4/90} {
%        \fill [green!\f!black, treetop](0,3) ellipse (\n/1.5 and \n);
%      }
%    }
% }
% \begin{tikzpicture}[scale=0.6,transform shape]
%   \pic at (0,2)    {my tree};
%   \pic at (3,2.5)  {my tree};
%   \pic at (6,1.75) {my tree};
% \end{tikzpicture}
% \end{center}
\begin{definition}[Forest]\label{def:forest}
An undirected (Def.~\ref{def:undirected-graph}), simple (Def.~\ref{def:simple-graph}) graph without cycles (Def.~\ref{def:cycle}) is a forest.
\end{definition}
\begin{definition}[Tree]\label{def:tree}
A connected (Def.~\ref{def:connected-components}) forest is a tree. %(Def.~\ref{def:forest}) is a tree.
\end{definition}
\begin{definition}[Leaf]\label{def:leaf}
A node \(v\in V\) with \(\deg(v)=1\) is called a leaf.
\end{definition}
\begin{theorem}\label{thm:atleast-two-leaves}
  A tree with at least two vertices has at least two leaves.
\end{theorem}
\begin{theorem}[Characterization of Trees]\label{thm:characterization-of-trees}
  Given an undirected graph \(G = (V,E)\), the following statements are equivalent:
  \begin{enumerate}[label=\textbf{(\arabic*)}, itemindent=0.25cm, labelsep=0.25cm]
    \item \(G\) is a tree, i.e., a connected graph without cycles. \label{thm:char-tree-connected-graph-no-cycles}
    \item \(G\) is connected and \(|V| = |E| + 1\). \label{thm:char-tree-connected-v-e-1}
    \item \(G\) has no cycles and \(|V| = |E| + 1\). \label{thm:char-tree-no-cycles-v-e-1}
    \item \(G\) is connected and every edge is a bridge. \label{thm:char-tree-connected-bridge}
    \item \(G\) has no cycles and if an additional edge is added, it creates a cycle. \label{thm:char-tree-no-cycles-add-edge}
    \item For all vertices \(u,v\in V\) there is a unique \(u\)-\(v\)-path. \label{thm:char-tree-unique-path} \qedhere
  \end{enumerate}
\end{theorem}
\begin{proof}
In total, \(2 \cdot \binom{6}{2} = 30\) implications are claimed in Theorem~\ref{thm:characterization-of-trees}.
By transitivity of implication, it is sufficient to show a loop of implications.
Any statement then follows from any other by following the implications in the loop.
We can show the implications in the following cycle:
\[
\newcommand{\scalefraction}{0.68}
\begin{tikzcd}[column sep={\scalefraction*1cm,between origins}, row sep={\scalefraction*1.732050808cm,between origins}, arrows=Rightarrow]
  & \ref{thm:char-tree-unique-path}
  \arrow[dl,""{pos=.5,sloped,overlay, inner sep=0pt, label={[black]center:\hyperlink{proof:char-tree-6-4}{\phantom{\rule{10pt}{15pt}}}}}]
  &
  & \ref{thm:char-tree-connected-graph-no-cycles}
  \arrow[ll,""{pos=.5,sloped,overlay, inner sep=0pt, label={[black]center:\hyperlink{proof:char-tree-1-6}{\phantom{\rule{15pt}{5pt}}}}}]
  &
  \\
  \ref{thm:char-tree-connected-bridge}
  \arrow[dr,""{pos=.5,sloped,overlay, inner sep=0pt, label={[black]center:\hyperlink{proof:char-tree-4-2}{\phantom{\rule{10pt}{15pt}}}}}]
  &
  &
  &
  & \ref{thm:char-tree-no-cycles-add-edge}
  \arrow[ul,""{pos=.5,sloped,overlay, inner sep=0pt, label={[black]center:\hyperlink{proof:char-tree-5-1}{\phantom{\rule{10pt}{15pt}}}}}]
  \\
  & \ref{thm:char-tree-connected-v-e-1}
  \arrow[rr,""{pos=.5,sloped,overlay, inner sep=0pt, label={[black]center:\hyperlink{proof:char-tree-2-3}{\phantom{\rule{15pt}{5pt}}}}}]
  & 
  & \ref{thm:char-tree-no-cycles-v-e-1}
  \arrow[ur,""{pos=.5,sloped,overlay, inner sep=0pt, label={[black]center:\hyperlink{proof:char-tree-3-5}{\phantom{\rule{10pt}{15pt}}}}}]
  &
\end{tikzcd}
\]

\vspace{-\baselineskip}
\hypertarget{proof:char-tree-1-6}{
\underline{\ref{thm:char-tree-connected-graph-no-cycles} $\Rightarrow$
          \ref{thm:char-tree-unique-path}:}}

Let \(u,v\in V\).  
Because \(G\) is connected (Definition~\ref{def:connected-components}), there is at least one \(u\text{-}v\)-path.  
It remains to show that this path is unique. 
Assume, for a contradiction, that there are two distinct \(u\text{-}v\)-paths.  
Starting at their first branching vertex, follow one path until they meet again, then return along the other path; 
this produces 
% a closed walk in which all internal vertices are distinct, i.e. 
a cycle, contradicting \ref{thm:char-tree-connected-graph-no-cycles}.
Hence the \(u\text{-}v\)-path is unique, establishing
\ref{thm:char-tree-unique-path}.

\hypertarget{proof:char-tree-6-4}{
\underline{\ref{thm:char-tree-unique-path} $\Rightarrow$
          \ref{thm:char-tree-connected-bridge}:}}

As there exists a path for any pair $u,v\in V$, we obtain immediately that $G$ is connected. 
It remains to show that every edge is a bridge. 
Assume, for a contradiction, that there is an edge $e\in E$ that is not a bridge. 
Then, we can remove $e$ and $G$ is still connected, i.e., there is a path connecting the vertices adjacent to $e$.
In other words, with $e$ there are two paths connecting these two vertices: 
the original edge $e$ and the detour through the remaining graph.
This contradicts the uniqueness in \ref{thm:char-tree-unique-path}.
Hence every edge is a bridge, proving \ref{thm:char-tree-connected-bridge}.

\hypertarget{proof:char-tree-4-2}{
\underline{\ref{thm:char-tree-connected-bridge} $\Rightarrow$
          \ref{thm:char-tree-connected-v-e-1}:}}

As connectedness is already given, it remains to show that $|V|=|E|+1$.
We show this by induction over the number of edges \(|E|\).
  \begin{enumerate}[partopsep=0em, topsep=0em, label=(\roman*)]
    \item \textbf{Base case.}  
    If \(|E|=1\), the edge joins two vertices, so \(|V|=2\). \textcolor{Green}{\ding{52}}
    \item \textbf{Induction hypothesis.}  
    Assume that every connected graph in which every edge is a bridge satisfies \(|V| = |E| + 1\). 
    \label{proof:char-tree-4-2-IH}
    \item \textbf{Induction step.}  
    As all edges are bridges, we can simply split the graph into two connected components by removing any of the edges:
    Then, we are left with two connected graphs. By \ref{proof:char-tree-4-2-IH} and considering that all remaining edges are still bridges, we have
    $$
    \left|E_1\right|+1=\left|V_1\right| \quad\left|E_2\right|+1=\left|V_2\right|
    $$
    By adding these two equations, we obtain
    $$
    \underbrace{\left|E_1\right|+\left|E_2\right|}_{=|E|-1}+2=\left|V_1\right|+\left|V_2\right|=|V|
    $$
    and therefore \(|V|=|E|+1\). \textcolor{Green}{\ding{52}}
\end{enumerate}
Thus \ref{thm:char-tree-connected-v-e-1} holds.

\hypertarget{proof:char-tree-2-3}{
\underline{\ref{thm:char-tree-connected-v-e-1} $\Rightarrow$
          \ref{thm:char-tree-no-cycles-v-e-1}:}}

Let \(G\) be a connected graph with \(|V|=|E|+1\).
Assume, for a contradiction, that \(G\) contains a cycle and let \(e\) be an edge of one such cycle.
Since \(e\) connects two vertices \(u,v\) in the cycle, there is a path from \(u\) to \(v\) that does not use \(e\).
In other words, \(e\) is not a bridge (Def~\ref{def:bridge}).
Removing \(e\) therefore leaves the graph connected (since the rest of the cycle still links its endpoints), so \(G'=(V',E')=(V,E\setminus\{e\})\) is connected with the same number of vertices but one edge less, i.e. \(|V'|=|V|\) and \(|E'|=|E|-1\). 
For this new graph \(G'\) we have
% \[
% |V'| - |E'| = |V| - (|E|-1) = |V| - |E| + 1 = 2 \text{ \textcolor{red}{\Lightning}}
% \]
\[
|V'| - |E'| \overset{\text{\textcolor{red}{\Lightning}}}{=} 2
\]
which is impossible for a connected graph by Corollary~\ref{cor:connected-components-inequality}.  
Hence no cycles exist and \ref{thm:char-tree-no-cycles-v-e-1} follows.

\hypertarget{proof:char-tree-3-5}{
\underline{\ref{thm:char-tree-no-cycles-v-e-1} $\Rightarrow$
          \ref{thm:char-tree-no-cycles-add-edge}:}}

We have to show that adding an edge \(e'\) to the graph \(G\) creates a cycle.
First, we show by induction on \(|V|\) that whenever \(|E|\ge |V|\), there must be a cycle.

\begin{enumerate}[partopsep=0em, topsep=0em, label=(\roman*)]
\item \textbf{Base case.}
The first interesting case is (\(|V|=3\)).
Three edges on three vertices force the triangle \(K_3\), which has a cycle. \textcolor{Green}{\ding{52}}
\item \textbf{Induction hypothesis.}
Assume a graph \(G\) with \(|E| \geq |V|\) contains a cycle.
\label{proof:char-tree-3-5-IH}
\item \textbf{Induction step.}  
We distinguish two cases:

If there exists a leaf in the graph, we merely remove that vertex and the corresponding edge. 
This reduces the number of vertices and the number of edges by one. 
If $|V| \leq|E|$ then also $|V|-1 \leq|E|-1$ and the graph contains a cycle by \ref{proof:char-tree-3-5-IH}.
So, also the graph containing the leaf has a cycle. \textcolor{Green}{\ding{52}}

If there is no leaf, then all vertices have $\deg(v) \geq 2$. 
Then we can simply construct a cycle by going from one vertex to the next. As there is no leaf, there is no dead-end. 
As the number of vertices is finite, we have to end up at the initial vertex sooner or later and thereby closing a cycle. \textcolor{Green}{\ding{52}}
\end{enumerate}
From \ref{thm:char-tree-no-cycles-v-e-1} we have $|V| = |E| + 1 = |E'|$ for \(E' := E\cup\{e'\}\).
Thus \(G'=(V,E')\) contains a cycle, as desired.

\hypertarget{proof:char-tree-5-1}{
\underline{\ref{thm:char-tree-no-cycles-add-edge} $\Rightarrow$
          \ref{thm:char-tree-connected-graph-no-cycles}:}}

We already have that \(G\) is acyclic. 
So it remains to show that \(G\) is connected.
Assume, for a contradiction, it were \emph{not} connected, with two components \(G_1,G_2\).
Then, we could add a vertex $e'$ to $G$ by linking the two connected components. 
This yields a contradiction with \ref{thm:char-tree-no-cycles-add-edge}, as this insertion of an edge did not create a cycle.
Hence \(G\) must be connected.

This closes the circle of implications, proving Theorem~\ref{thm:characterization-of-trees}.
\end{proof}


\subsubsection{Counting trees: Cayley's theorem}
How many \emph{labeled} trees exist with \(n\) vertices?
The question can be rephrased, asking for the number of different \emph{spanning trees} of the complete graph (\emph{clique}) with \(n\) vertices, \(K_n\).
\begin{definition}[Spanning tree]\label{def:spanning-tree}
Given a connected graph \(G=(V,E)\), a graph \(H=(V,E')\) with the same vertex set is called a spanning tree of \(G\) if \(H\) is a tree (Def~\ref{def:tree}) and \(E' \subseteq E\).
\end{definition}
Generally, there exist many different spanning trees for a given graph.
An interesting algorithmic problem is to find the one that optimizes, for examle, the total weight, given that each edge has a weight.
The most famous algorithms are greedy and due to Kruskal (1956) and Prim (1957).

% So how many spanning trees are there for complete graphs \(K_n\) with \(n\) vertices?
\begin{example}\label{ex:spanning-trees-small-n}
Let us first consider some cases for small \(n\):
\begin{itemize}
\item \(n=1\): There is only one vertex and no edges and thus only one spanning tree.
\item \(n=2\): There is merely one connected graph with two nodes. The single spanning tree is just the graph itself.
\item \(n=3\): The completely connected graph \(K_3\)
\[
\tikzset{dot/.style={circle, fill, inner sep=1.2pt, outer sep=3pt}}
\begin{tikzpicture}[every node/.style={dot, font=\small,label distance=-5pt}]
  \node[label=right:$v_0$] (v0) at (0,0.866) {}; % sqrt(3)/2
  \node[label=left:$v_1$]  (v1) at (-0.5,0) {}; 
  \node[label=right:$v_2$]  (v2) at (0.5,0) {}; 
  \foreach \a/\b in {v1/v0,v1/v2,v0/v2}{
    \draw (\a)--(\b);
  }
\end{tikzpicture}
\]
has \(3\) spanning trees:
\[
\tikzset{dot/.style={circle, fill, inner sep=0.8pt, outer sep=2pt}}
\begin{tikzpicture}[every label/.style={font=\small}, scale=0.6]
  \def\dx{2.25}
  \begin{scope}[shift={(0,0)}]
    \node[dot] at (0,0.886) (A) {}; % sqrt(3)/2
    \node[dot] at (-0.5,0) (B) {};
    \node[dot] at (0.5,0) (C) {};
    \draw (A)--(C) (C)--(B);
  \end{scope}
  \begin{scope}[shift={(\dx,0)}]
    \node[dot] at (0,0.886) (A) {}; % sqrt(3)/2
    \node[dot] at (-0.5,0) (B) {};
    \node[dot] at (0.5,0) (C) {};
    \draw (B)--(A) (A)--(C);
  \end{scope}
  \begin{scope}[shift={(2*\dx,0)}]
    \node[dot] at (0,0.886) (A) {}; % sqrt(3)/2
    \node[dot] at (-0.5,0) (B) {};
    \node[dot] at (0.5,0) (C) {};
    \draw (A)--(B) (B)--(C);
  \end{scope}
\end{tikzpicture}
\]
\item \(n=4\): The completely connected graph \(K_4\)
\[
\tikzset{dot/.style={circle, fill, inner sep=1.2pt, outer sep=3pt}}
% \begin{tikzpicture}[every label/.style={font=\small}, scale=0.5]
%   % Draw K4
%   \node[dot,label={[xshift=5pt, yshift=-2pt]above left:$v_1$}] (v1) at (0,3) {};
%   \node[dot,label={[xshift=-5pt, yshift=-2pt]above right:$v_0$}] (v0) at (2,3) {};
%   \node[dot,label={[xshift=5pt, yshift=2pt]below left:$v_2$}] (v2) at (0,1) {};
%   \node[dot,label={[xshift=-5pt, yshift=2pt]below right:$v_3$}] (v3) at (2,1) {};
%   \foreach \a/\b in {v1/v0,v1/v2,v1/v3,v0/v2,v0/v3,v2/v3}{
%     \draw (\a)--(\b);
%   }
% \end{tikzpicture}
\begin{tikzpicture}[scale=0.5, every node/.style={dot, font=\small,label distance=-5pt}]
  \node[label=left:$v_1$]  (v1) at (0,3) {};
  \node[label=right:$v_0$] (v0) at (2,3) {};
  \node[label=left:$v_2$]  (v2) at (0,1) {};
  \node[label=right:$v_3$] (v3) at (2,1) {};
  \foreach \a/\b in {v1/v0,v1/v2,v1/v3,v0/v2,v0/v3,v2/v3}{
    \draw (\a)--(\b);
  }
\end{tikzpicture}
\]
has \(12+4=16=4^2\) spanning trees:
\[
\tikzset{dot/.style={circle, fill, inner sep=0.8pt, outer sep=2pt}}
\begin{tikzpicture}[every label/.style={font=\small}, scale=0.6]

  % Parameters for grid
  \def\dx{2.25}
  \def\dy{-2.25}

  % Draw the 12 path trees in a 4x3 grid
  %% Row 0: zigzag shapes
  \begin{scope}[shift={(0,0)}]
    \node[dot] at (0,1) (A) {};
    \node[dot] at (1,1) (B) {};
    \node[dot] at (0,0) (C) {};
    \node[dot] at (1,0) (D) {};
    \draw (A)--(B) (B)--(C) (C)--(D);
  \end{scope}
  \begin{scope}[shift={(\dx,0)}]
    \node[dot] at (0,1) (A) {};
    \node[dot] at (1,1) (B) {};
    \node[dot] at (0,0) (C) {};
    \node[dot] at (1,0) (D) {};
    \draw (A)--(B) (A)--(D) (C)--(D);
  \end{scope}
  \begin{scope}[shift={(2*\dx,0)}]
    \node[dot] at (0,1) (A) {};
    \node[dot] at (1,1) (B) {};
    \node[dot] at (0,0) (C) {};
    \node[dot] at (1,0) (D) {};
    \draw (C)--(A) (A)--(D) (D)--(B);
  \end{scope}
  \begin{scope}[shift={(3*\dx,0)}]
    \node[dot] at (0,1) (A) {};
    \node[dot] at (1,1) (B) {};
    \node[dot] at (0,0) (C) {};
    \node[dot] at (1,0) (D) {};
    \draw (C)--(B) (B)--(D) (C)--(A);
  \end{scope}

  %% Row 1: axis-only bracket shapes
  \begin{scope}[shift={(0,\dy)}]
    \node[dot] at (0,1) (A) {};
    \node[dot] at (1,1) (B) {};
    \node[dot] at (0,0) (C) {};
    \node[dot] at (1,0) (D) {};
    \draw (A)--(B) (A)--(C) (C)--(D);
  \end{scope}
  \begin{scope}[shift={(\dx,\dy)}]
    \node[dot] at (0,1) (A) {};
    \node[dot] at (1,1) (B) {};
    \node[dot] at (0,0) (C) {};
    \node[dot] at (1,0) (D) {};
    \draw (C)--(A) (C)--(D) (D)--(B);
  \end{scope}
  \begin{scope}[shift={(2*\dx,\dy)}]
    \node[dot] at (0,1) (A) {};
    \node[dot] at (1,1) (B) {};
    \node[dot] at (0,0) (C) {};
    \node[dot] at (1,0) (D) {};
    \draw (A)--(B) (B)--(D) (D)--(C);
  \end{scope}
  \begin{scope}[shift={(3*\dx,\dy)}]
    \node[dot] at (0,1) (A) {};
    \node[dot] at (1,1) (B) {};
    \node[dot] at (0,0) (C) {};
    \node[dot] at (1,0) (D) {};
    \draw (A)--(C) (A)--(B) (B)--(D);
  \end{scope}

  %% Row 2: cross shapes (two diagonals + one axis edge)
  \begin{scope}[shift={(0,2*\dy)}]
    \node[dot] at (0,1) (A) {};
    \node[dot] at (1,1) (B) {};
    \node[dot] at (0,0) (C) {};
    \node[dot] at (1,0) (D) {};
    \draw (A)--(B) (A)--(D) (B)--(C);
  \end{scope}
  \begin{scope}[shift={(\dx,2*\dy)}]
    \node[dot] at (0,1) (A) {};
    \node[dot] at (1,1) (B) {};
    \node[dot] at (0,0) (C) {};
    \node[dot] at (1,0) (D) {};
    \draw (A)--(D) (B)--(C) (A)--(C);
  \end{scope}
  \begin{scope}[shift={(2*\dx,2*\dy)}]
    \node[dot] at (0,1) (A) {};
    \node[dot] at (1,1) (B) {};
    \node[dot] at (0,0) (C) {};
    \node[dot] at (1,0) (D) {};
    \draw (C)--(D) (A)--(D) (B)--(C);
  \end{scope}
  \begin{scope}[shift={(3*\dx,2*\dy)}]
    \node[dot] at (0,1) (A) {};
    \node[dot] at (1,1) (B) {};
    \node[dot] at (0,0) (C) {};
    \node[dot] at (1,0) (D) {};
    \draw (B)--(D) (A)--(D) (B)--(C);
  \end{scope}

  %% Box around the first three rows
  \draw[dotted,Green,rounded corners, thick] (-0.5,2*\dy-0.5) rectangle (3*\dx+1.5,1.5);

  %% Row 3: star shapes
  \begin{scope}[shift={(0,3*\dy)}]
    \node[dot] at (0,1) (A) {};
    \node[dot] at (1,1) (B) {};
    \node[dot] at (0,0) (C) {};
    \node[dot] at (1,0) (D) {};
    \draw (A)--(B) (A)--(C) (A)--(D);
  \end{scope}
  \begin{scope}[shift={(\dx,3*\dy)}]
    \node[dot] at (0,1) (A) {};
    \node[dot] at (1,1) (B) {};
    \node[dot] at (0,0) (C) {};
    \node[dot] at (1,0) (D) {};
    \draw (B)--(A) (B)--(C) (B)--(D);
  \end{scope}
  \begin{scope}[shift={(2*\dx,3*\dy)}]
    \node[dot] at (0,1) (A) {};
    \node[dot] at (1,1) (B) {};
    \node[dot] at (0,0) (C) {};
    \node[dot] at (1,0) (D) {};
    \draw (C)--(A) (C)--(B) (C)--(D);
  \end{scope}
  \begin{scope}[shift={(3*\dx,3*\dy)}]
    \node[dot] at (0,1) (A) {};
    \node[dot] at (1,1) (B) {};
    \node[dot] at (0,0) (C) {};
    \node[dot] at (1,0) (D) {};
    \draw (D)--(A) (D)--(B) (D)--(C);
  \end{scope}
\end{tikzpicture}
\]
We can group them into 2 sets: The ``snake'' type and the ``star'' type.
The first 12, the ``snakes'', are the same if we consider them unlabeled: They are isomorphic (Def.~\ref{def:isomorphism}). 
\item \(n=5\): There are 3 different shapes of trees with 5 vertices:
\begin{itemize}
\item ``snake'': For any permutation of the 5 vertices, we obtain another isomorphic graph - except when we simply reverse the order.
Therefore, there are \(5!/2=60\) different labeled trees of this type.
\item ``star'': For each of the 5 vertices being in the center, we obtain a different graph.
Swapping the leaves does not change the graph.
So we have 5 different labeled trees of this type.
\item ``Y'': This type has one vertex in the center, two short ends with 1 edge each (the upper part of the Y) and one long end with 2 edges (the lower part of the Y).
The permutations of the vertices give the isometries.
But swapping the two leaves at the short ends does not change the graph so we have again \(5!/2=60\) different labeled trees of this type.
\end{itemize}
Thus, in total we have \(60+5+60=125=5^{3}\) different labeled trees with 5 vertices.
\qedhere
\end{itemize}
\end{example}
\begin{definition}[Isomorphism]\label{def:isomorphism}
Two graphs \(G=(V,E)\) and \(G'=(V',E')\) are called \emph{isomorphic}, written \(G\cong G'\), if there exists a bijection
\[
  \varphi:V\;\longrightarrow\;V'
  \quad\text{satisfying}\quad
  \forall\,u,v\in V:\;(u,v)\in E\;\Longleftrightarrow\;\bigl(\varphi(u),\varphi(v)\bigr)\in E'
\]
Such a function \(\varphi\) that preserves the edge relations is called an \emph{isomorphism}.
\end{definition}

In particular, to count all labelled trees on \(n\) vertices one may first classify all \emph{unlabelled} tree-shapes (up to isomorphism), and then multiply by the number of ways to assign labels to each shape (dividing out automorphisms of the shape).
A more direct approach uses Cayley's theorem:

\begin{theorem}[Cayley]\label{thm:cayley}
The number of different labelled trees on \(n\) vertices (equivalently, the number of spanning trees of the completely connected graph \(K_n\)) is
\[
\fcolorbox{red}{white}{$\displaystyle
n^{\,n-2}
$} \qedhere
\]
\end{theorem}

\begin{proof}
We construct a bijection
\[
  \bigl\{\text{trees on }[n]\text{ with two distinguished marks (head, tail)}\bigr\}
  \quad\longleftrightarrow\quad
  \bigl\{\,f\colon[n]\to[n]\,\bigr\}
\]
Since there are \(n^2\) ways to choose the head and the tail in a tree on \(n\) vertices, this implies that there are 
\[
\frac{n^n}{n^2} = n^{n-2}
\]
different labelled trees on \(n\) vertices.
\begin{itemize}
\item function \(\to\) tree
\begin{enumerate}[label=(\alph*)]
  \item 
  Start with any \(f:[n]\to[n]\).
  Draw its functional digraph: each \(i\) points to \(f(i)\).
  Every component has exactly one directed cycle.
  \item 
  Let \(M\subseteq[n]\) be the set of all vertices on these cycles.  
  On \(M\), \(f\) restricts to a permutation.
  % \item \emph{Mark} the smallest element of \(M\) (in the cycle you choose) as the \emph{head}, and follow the permutation to list out all of \(M\) in cycle‐order; the last element is the \emph{tail}.  Erase the final arrow closing the cycle: what remains is the \emph{spine path}.
  \item
  Starting from the head, the \(i^\text{th}\) vertex on the spine is chosen as \(f(M[i])\) where \(M[i]\) is the \(i^\text{th}\) smallest element of \(M\).
  So in particular the head is \(f(M[1]) = f(\min M)\) and the tail is \(f(M[|M|]) = f(\max M)\).
  \item 
  % Attach each \(v\notin M\) as a leaf to its unique neighbour on the spine (or on a branch closer to the spine) given by the arrow \(v\to f(v)\).
  For every \(v\notin M\) join \(v\) to \(f(v)\). This yields an undirected, labeled tree with two additional marks: the head and the tail.
\end{enumerate}
\item tree \(\to\) function
\begin{enumerate}[label=(\alph*)]
  \item Given a tree on \([n]\) with a marked head and a marked tail, read off the unique simple path from head to tail; its vertices are exactly the set \(M\).
  \item Declare \(f\) to send \(M[i]\) to the \(i^\text{th}\) vertex on the path from head to tail.
  \item For any other vertex \(v\), send \(f(v)\) to the unique neighbor of \(v\) on the path toward the spine.
\end{enumerate}
\end{itemize}
These two constructions are clear inverses of one another.  
Therefore the number of labeled trees with two marks is exactly the number of functions \(n^n\), and dividing by the \(n^2\) choices of the marks (head, tail) gives the claimed result.
\end{proof}


\subsection{Some special graphs}
\paragraph{Complete Graphs} \(K_n\) (also called \emph{cliques}) are graphs with \(n\) vertices an edge between every pair of vertices.
For a complete graph \(K_n\) with \(n\) vertices, \(K_n\), there are 
\[\fcolorbox{red}{white}{$\displaystyle
|E| = \binom{n}{2} = \frac{n(n-1)}{2}
$}\]
edges.

\paragraph{Cycles} \(C_n\) are graphs with all vertices connected in a single cycle without additional edges.
The smallest cycle is \(C_3\) (triangle).
The cycle \(C_n\) with \(n\) vertices has \(n\) edges.

\paragraph{Mesh Graphs} \(M_{m,n}\) are graphs with a vertex set
\[
V = \{(i,j) \mid 1 \leq i \leq m, 1 \leq j \leq n\}
\]
and an edge set containing merely the closest neighbors of each vertex
\[
E = \{((i_1,j_1),(i_2,j_2)) \mid |i_1-i_2| + |j_1-j_2| = 1\}
\]
and no diagonals:
\[
\begin{tikzpicture}[scale=0.5, every node/.style={}]
  \def\m{4} % number of rows
  \def\n{5} % number of columns

  % Draw grid points and edges
  \foreach \i in {0,...,\m} {
    \foreach \j in {0,...,\n} {
      \node (v\i\j) at (\j,-\i) {};
      \draw[fill=black] (v\i\j) circle (2pt);
      \ifnum\j<\n
        \draw (\j,-\i) -- (\j+1,-\i); % horizontal edge
      \fi
      \ifnum\i<\m
        \draw (\j,-\i) -- (\j,-\i-1); % vertical edge
      \fi
    }
  }

  % Horizontal brace and label
  \draw[decorate,decoration={brace,amplitude=6pt}] 
    (0,0.25) -- (\n,0.25) node[midway, anchor=south, yshift=1ex] {$n$};

  % Vertical brace and label
  \draw[decorate,decoration={brace,amplitude=6pt,mirror}]
    (-0.25,0) -- (-0.25,-\m) node[midway, anchor=east, xshift=-1ex] {$m$};

\end{tikzpicture}
\]
If we interpret the mesh in a cyclic way, we obtain different topological shapes, depending on how we connect the boundaries:
\begin{itemize}[leftmargin=1em]
\item \emph{Disk}: If we do not connect any boundaries, we obtain a 2-dimensional orientable manifold with a single boundary component homeomorphic to a circle, and it embeds in \(\mathbb{R}^2\) without self-intersection.
\item \emph{Cylinder}: The nodes are connected horizontally, i.e. left and right boundaries are connected, but the top and bottom are not:
\[
E_{\text{Cylinder}} = E \cup \{((i,1),(i,n)) \mid 1\leq i\leq m\}
\]
It is a 2-dimensional orientable manifold with two boundary circles; although intrinsically flat, it requires embedding in \(\mathbb{R}^3\) as a tube to avoid self-intersection.
\item \emph{Möbius strip}: The left and right boundaries are connected, but in reverse order:
\[
E_{\text{Möbius}} = E \cup \{((i,1),(m-i+1,n)) \mid 1\leq i\leq m\}
\]
The Möbius strip is a 2-dimensional non-orientable surface with one boundary circle.
Three spatial dimensions (\(\R^3\)) are needed to embed it without tearing or intersecting itself.
\item \emph{Torus} (`Donut'): The nodes are connected horizontally as well as vertically:
\[
E_{\text{Torus}} = \underbrace{E \cup \{((i,1),(i,n)) \mid 1\leq i\leq m\}}_{E_{\text{Cylinder}}} \cup \{((1,j),(m,j)) \mid 1\leq j\leq n\}
\]
It is a closed (no boundary), 2-dimensional orientable manifold of genus 1, and it embeds smoothly in \(\mathbb{R}^3\) as the familiar donut shape.
\item \emph{Klein bottle}: The left and right boundaries are connected in reverse order (as in the Möbius strip), while the top and bottom boundaries are connected without reversal:
\[
E_{\text{Klein}} = \underbrace{E \cup \{((i,1),(m-i+1,n)) \mid 1\leq i\leq m\}}_{E_{\text{Möbius}}} \cup \{((1,j),(m,j)) \mid 1\leq j\leq n\}
\]
It is a closed (no boundary), 2-dimensional non-orientable manifold. 
Unlike a sphere or a torus, the Klein bottle does not have an inside or outside.
Any realization in \(\mathbb{R}^3\) self-intersects, so a true embedding requires \(\mathbb{R}^4\).
\item \emph{Projective plane}: Both pairs of opposite boundaries are connected in reverse order:
\[
E_{\mathbb{RP}^{2}} = E \cup \{((i,1),(m-i+1,n)) \mid 1\leq i\leq m\} \cup \{((1,j),(m,n-j+1)) \mid 1\leq j\leq n\}
\]
\end{itemize}

\paragraph{Complete bipartite graph}
\(K_{m,n}\) is a graph with two sets of vertices \(V_1\) and \(V_2\) with \(|V_1|=m\) and \(|V_2|=n\), and an edge between every pair of vertices \(v_i\in V_1\) and \(v_j\in V_2\).
It has \(|V|=m+n\) vertices and \(|E|=mn\) edges:
\[
\begin{tikzpicture}[scale=0.5]
  \def\m{8} % number of rows
  \def\d{8} % distance between the two sets

  % vertices
  \foreach \i in {0,...,\m} {
    \node[circle, inner sep=1.2pt, outer sep=3pt, fill=Blue] (vb\i) at (0,-\i) {};
    \node[circle, inner sep=1.2pt, outer sep=3pt, fill=red!80!black]  (vr\i) at (\d,-\i) {};
  }

  % edges
  \foreach \i in {0,...,\m} {
    \foreach \j in {0,...,\m} {
      \draw (vb\i) -- (vr\j);
    }
  }

  % braces
  \draw[decorate,decoration={brace,amplitude=6pt,mirror}]
    (-0.25,0.2) -- (-0.25,-\m-0.2) node[midway, anchor=east, xshift=-1ex] {$m$};
  \draw[decorate,decoration={brace,amplitude=6pt}]
    (\d +0.25,0.2) -- (\d +0.25,-\m-0.2) node[midway, anchor=west, xshift=1ex] {$n$};
\end{tikzpicture}
\]

\paragraph{Hypercube} \(Q_d\) is the \(d\)-dimensional hypercube with vertices that are \(d\)-bit strings, i.e.
\[
V = \{0,1\}^d = \{(b_1, \ldots, b_d) \mid b_i \in\{0,1\}\}
\]
It has \(|V|=2^d\) vertices.
The edges are given by pairs with Hamming distance (Def~\ref{def:hamming-distance}) \(1\), i.e. the two strings differ in exactly one bit:
\[
(u, v) \in E \quad :\Leftrightarrow \quad d_H(u,v) = 1
\]
Every vertex has $d$ neighbors, i.e., $\forall v \in V: \operatorname{deg}(v)=d$. 
The total number of edges is
$$
|E|=\frac{d \cdot 2^d}{2}=d \cdot 2^{d-1}
$$
as each of the $2^d$ degrees is $d$.
We draw the graph as follows: 
For obtaining the graph $Q_{d+1}$ we draw twice the graph $Q_d$. 
To one of the two we add a $0$ after all bit strings labelling the vertices, to the second respectively a $1$. 
Finally, we add edges to connect the corresponding vertices of the two copies of $Q_d$.
We start from $Q_0$, containing merely the empty word $\epsilon$.
Then the hypercube $Q_1$ is constructed following the procedure above.
Repeating the same process yields $Q_d$ for larger $d$.
\begin{definition}[Hamming distance]\label{def:hamming-distance}
The Hamming distance between two bit strings of same length, \(d_H(x,y)\), with \(x,y\in\{0,1\}^d\) is the number of bits in which \(x\) and \(y\) differ.
\end{definition}




\subsection{Euler Tours and Hamilton Cycles}
In this section we will see two extrema of computational hardness (linear time vs. NP-complete) in two very similar-sounding problems.
\begin{example}[Bridges of Königsberg]\label{ex:bridges}
\[
\begin{tikzpicture}[scale=0.5]

% map of the bridges
\begin{scope}[line cap=round, line join=round, scale=0.9]

\clip (-5,-2.5) rectangle (3.5,2.5);
\useasboundingbox (-5,-2.5) rectangle (3.5,2.5);

% land
\fill[color=Green!50!white, draw=none] (-10,-5) rectangle (10,5);

% water
\path[fill=white!10!cyan]
  plot [smooth] coordinates {(-5,-0.4) (-4.3,-0.4) (-4.1,0.8) (-3,1.7) (-1,1.9) (1.3,1.4) (3,1.8) (5,2.1)}
    --
  plot [smooth] coordinates {(5,-2.7) (3,-2.4) (1.8,-1.9) (0,-1.1) (-2,-1.3) (-3.5,-1) (-5,-1.1)}
  -- cycle;
\draw[line width=2pt,brown!70!black]
  plot [smooth] coordinates {(-5,-0.4) (-4.3,-0.4) (-4.1,0.8) (-3,1.7) (-1,1.9) (1.3,1.4) (3,1.8) (5,2.1)};
\draw[line width=2pt,brown!70!black]
  plot [smooth] coordinates {(5,-2.7) (3,-2.4) (1.8,-1.9) (0,-1.1) (-2,-1.3) (-3.5,-1) (-5,-1.1)};

% islands
\draw[line width=2pt,brown!70!black, fill=Green!50!white] (-1.6,0.3) ellipse (2 and 1);
\draw[line width=2pt,brown!70!black, fill=Green!50!white] (6.1,-0.3) ellipse (5.3 and 1.8);

% bridges
\newcommand{\Bridge}[3]{%
  \begin{scope}[shift={(#1,#2)},rotate=#3]
    \fill[brown!45!yellow] (-0.15,-0.7) rectangle (0.15,0.7);
    \draw[thick] (-0.15,-0.7) -- (-0.15,0.7);
    \draw[thick] (0.15,-0.7) -- (0.15,0.7);
  \end{scope}
}
\Bridge{-3.0}{1.35}{20}
\Bridge{-0.6}{1.45}{-20}
\Bridge{-3.0}{-0.9}{-20}
\Bridge{-1}{-0.9}{10}
\Bridge{0.6}{0}{80}
\Bridge{2}{1.2}{30}
\Bridge{1.8}{-1.6}{-30}
\end{scope}


% arrow between map and graph
\node at (5.5, 0) {\Huge$\rightarrow$};


% (multi)graph representation
\begin{scope}[xshift=8cm] 
\node[circle, inner sep=1.2pt, outer sep=3pt, fill=black] (C) at (0,2) {};
\node[circle, inner sep=1.2pt, outer sep=3pt, fill=black]  (A) at (0,0) {};
\node[circle, inner sep=1.2pt, outer sep=3pt, fill=black] (B) at (0,-2) {};
\node[circle, inner sep=1.2pt, outer sep=3pt, fill=black]  (D) at (3,0) {};

\draw (C) -- (D);
\draw (A) -- (D);
\draw (B) -- (D);

\draw[bend left=25] (A) to (C);
\draw[bend right=25] (A) to (C);
\draw[bend left=25] (A) to (B);
\draw[bend right=25] (A) to (B);
\end{scope}

\end{tikzpicture}
\]
In 1736, Leonhard Euler considered the question of whether there is a tour crossing every of the seven bridges in the Prussian city of Königsberg exactly once. 
This question is often said to mark the beginning of graph theory. 
The situation is reflected in the above (multi-)graph.
We are looking for a closed way that passes each edge exactly once - a so-called Euler tour (Def.~\ref{def:euler-tour}).
\end{example}
\begin{definition}[Euler Tour]\label{def:euler-tour}
An Euler tour is a closed sequence of edges of a graph $G=(V, E)$ that contains each edge of the graph exactly once.
\end{definition}


\begin{theorem}[Euler]\label{thm:euler}
A connected graph has an Euler tour if and only if all degrees are even. That is
\[
\text{$G$ has an Euler tour} \quad \Leftrightarrow \quad \forall v \in V: \quad \text{$\deg(v)$ is even} \qedhere
\]
\end{theorem}
\begin{proof}
We first show that the condition
`$\deg(v)$ is even for all $v \in V$'
is necessary for the existence of an Euler tour.

\underline{`$\Longrightarrow$':}

This follows from the observation that any vertex is reached and left the same number of times. Whenever we reach or leave the edge, we have to use an other edge to form an Euler tour. Thus, the number of available edges has to be even if we finally have to have passed all edges. Note that the same holds for the starting vertex.

It remains to show that the condition is sufficient.

\underline{`$\Longleftarrow$':}

We construct an Euler tour for a graph satisfying the condition above. 
First, we choose an initial vertex $v_1 \in V$. 
Following any yet unused edge one now proceeds to other vertices. 
As there is an even number of edges ending at each of the vertices, and as edges are always used (i.e., removed) in pairs (arrive + leave), we always finds such an unused edge for continuing, except at $v_1$. 
As the number of edges and the number of vertices are finite, we must eventually end up in $v_1$, and thus obtain a first closed way.

This way may not contain all edges. 
Then, as the graph is connected, there must be a vertex $v_2$ in the already-found way with still (at least two) unused edges. 
We use this vertex $v_2$ as the starting point of an additional way, using the same procedure of following the unused edges.
Note that it is still true that all degrees in the graph are even since, again, we removed edges always in pairs with respect to any vertex: arrive and leave.

Repeatig this iteratively until we used all edges yields the desired Euler tour.
\end{proof}

As the degree of any of the vertices of the multigraph in Example~\ref{ex:bridges} is 3 or 5, thus odd, there does not exist an Euler tour.

% \begin{example} \label{euler-tour-complete-hypercube}
The complete graph \(K_n\) has an Euler tour if and only if \(n\) is odd:
\[
\deg(v) = n-1 \quad \forall v\in V
\]

The hypercube \(Q_d\) has an Euler tour if and only if \(d\) is even:
\[
\deg(v) = d \quad \forall v\in V \qedhere
\]
% \end{example}

The proof of Theorem~\ref{thm:euler} uses a simple \emph{greedy}\footnote{
At each vertex we choose any unused edge, independent of any previous choice. The choice does not depend on any global properties.
} algorithm, that finds an Euler tour in \(O(|E|)\) time. The given proof above uses a simple greedy algorithm, that finds an Euler tour in linear time in the number of edges $|E|$. 
Finding on Euler tour is, hence, among the computationally easiest problems (you simply have to go through the entire graph, and you have it). 
If we modify the problem a little replacing ``edge'' by ``vertex'', we end up with a hard problem, the Hamilton cycles.

\begin{definition}[Hamilton Cycles]\label{def:hamilton-cycle}
A cycle that visits every vertex exactly once is called a Hamilton cycle.
A graph containing such a cycle is called Hamiltonian.
\end{definition}

% \begin{example} \label{ex:hamilton-cycles}
The cycles $C_k$ are Hamiltonian for all $k \geq 3$.

The complete graphs $K_n$ contain the cycles $C_n$ for all $n \geq 3$, and therefore a Hamilton cycle. 
Thus they are Hamiltonian.

Wheel graphs $W_n$ are Hamiltonian.

Trees are \emph{not} Hamiltonian, as they contain no cycles at all.

Mesh graphs $M_{m,n}$ are Hamiltonian if and only if $m \cdot n$ is even.
To see this, note that $M_{m,n}$ is bipartite.
A bipartite graph can be Hamiltonian only if it contains the same number of nodes of each of the two colors, because in the (closed) Hamiltonian cycle, the colors must switch in every step.
Therefore the number of nodes (\(m \cdot n\)) must be even, showing that the latter is a \emph{necessary} condition.
If \(m \cdot n\) is even, either \(m\) or \(n\) must be even.
Without loss of generality, assume \(m\) is even (otherwise we can just rotate by \(90^\circ\)).
Then we can construct a Hamiltonian cycle by starting at the top left corner and going down the first column, then going up in a zig-zag pattern through the remaining columns.

Complete bipartite graphs \(K_{m,n}\) are are Hamiltonian if and only if \(m = n\):
\[
\begin{tikzpicture}[scale=0.5]
  \def\m{4} % number of rows
  \def\d{8} % distance between the two sets
  % vertices
  \foreach \i in {0,...,\m} {
    \node[circle, inner sep=1.2pt, outer sep=3pt, fill=Blue] (vb\i) at (0,-\i) {};
    \node[circle, inner sep=1.2pt, outer sep=3pt, fill=red!80!black]  (vr\i) at (\d,-\i) {};
  }
  % edges
  \foreach \i in {0,...,\m} {
    \foreach \j in {0,...,\m} {
      \draw[thin, color=gray] (vb\i) -- (vr\j);
    }
  }
  % hamilton cycle
  \foreach \i in {0,...,\m}{
    \draw[very thick,black] (vb\i) -- (vr\i); % connect vert i on the blue side to vert i on the red side
    \pgfmathtruncatemacro{\next}{mod(\i+1,\m+1)} % compute the successor index (wrap to 0 after \m)
    \draw[very thick,black] (vb\next) -- (vr\i); % connect successor on the blue side to vert i on the red side
  }
\end{tikzpicture}
\]

Hypercubes \(Q_d\) are Hamiltonian for \(d\geq 2\). 
\(Q_0\) and \(Q_1\) are trees and thus cycleless.
The square \(Q_2\) is Hamiltonian (base case).
Assume \(Q_d\) is Hamiltonian (induction hypothesis).
To construct a Hamilton cycle in \(Q_{d+1}\) we remove two corresponding edges \((a_10,a_20)\) and \((a_11,a_21)\), and then add \((a_10,a_21)\) and \((a_10,a_21)\) (induction step).

The problem wether a general graph is Hamiltonian is believed to be hard: It is NP-complete.








% \end{example}
\subsection{Planar Graphs}
\begin{definition}[Planar graph]\label{def:planar-graph}
A graph \(G=(V,E)\) is planar if it can be drawn in the plane such that no edges cross.
Such a drawing is called a planar embedding of \(G\).
Sometimes when we say planar graph, we refer to a planar embedding. 
\end{definition}
\begin{example}\label{ex:k5-k33-not-planar}
The complete graph \(K_5\) and the complete bipartite graph \(K_{3,3}\) are not planar:
\[
\begin{tikzpicture}[]
\begin{scope}[]
\tikzset{
dot/.style = {circle, fill, inner sep=1.2pt, outer sep=3pt}
}
\def\n{5}
\node[regular polygon, regular polygon sides=\n, minimum size=2cm] at (0,0) (A) {};
\foreach \i in {1,...,\n}
    % \node[dot, label={[xshift=-4pt]right:$w_\i$}] (w\i) at (A.corner \i) {};
     \node[dot] (w\i) at (A.corner \i) {};

\foreach \i in {1,...,\n}
  \foreach \j in {\i,...,\n}
  {
    \ifnum\i<\j
        \draw[] (w\i) -- (w\j);
    \fi
  }
\end{scope}
\begin{scope}[xshift=2cm, scale=0.95]
  \def\n{2}
  \def\d{1.6} % distance between the two sets
  % vertices
  \foreach \i in {0,...,\n} {
    \node[circle, inner sep=1.2pt, outer sep=3pt, fill=black] (vb\i) at (\i,\d/2) {};
    \node[circle, inner sep=1.2pt, outer sep=3pt, fill=black]  (vr\i) at (\i,-\d/2) {};
  }
  % edges
  \foreach \i in {0,...,\n} {
    \foreach \j in {0,...,\n} {
      \draw[] (vb\i) -- (vr\j);
    }
  }
\end{scope}
\end{tikzpicture}
\qedhere
\]
\end{example}
To actually see that the latter two graphs are not planar, we derive necessary properties all planar graphs share.

\begin{definition}[Face]\label{def:face}
A face or region \(f\) of a planar graph \(G=(V,E)\) is a connected region of the plane.
The set of regions is denoted by \(F\).
\end{definition}

The boundary of a face is the subgraph formed from all vertices and edges that touch the face. 
Two faces are adjacent if their boundaries have at least one edge in common.
A boundary walk of a face is a closed walk once around the perimeter of the face boundary. 
The degree of a face \(f\) is the length of the boundary walk of the face, denoted \(\deg(f)\).

\begin{lemma}[Faceshaking]\label{lem:faceshaking}
Let \(G=(V,E)\) be a graph with a planar embedding, where \(F\) is the set of all faces.
Then
\[
\sum_{f \in F} \operatorname{deg}(f)=2|E| \qedhere
\]
\end{lemma}
\begin{proof}
Each edge has \(2\) sides, and each side contributes \(1\) to a boundary walk, so the edge contributes \(2\) to the sum of degrees.
\end{proof}

\begin{theorem}[Euler's polyhedron formula]\label{thm:euler-polyhedron}
Let \(G=(V,E)\) be a connected planar graph that divides the plane into \(|F|\) regions (including the region outside the graph).
Then
\[
\fcolorbox{red}{white}{$%\displaystyle
|V| + |F| - |E| = 2
$}
\]
But not the other way round!
\end{theorem}
\begin{proof}
Theorem~\ref{thm:euler-polyhedron} is true for trees, as they have \(|F|=1\) and \(|E|=|V|-1\).
We can now reduce any connected planar graph to a tree by removing edges from cycles.
Removing an edge from a cycle reduces the number of regions and the number of edges by one, while the number of vertices remains the same.
% \[
% f \to f-1 \quad |E| \to |E|-1 \quad |V| \to |V|
% \]
Repeating this process until there are no cycles left, we obtain a tree that satisfies Theorem~\ref{thm:euler-polyhedron} as seen above.
Thus, Theorem~\ref{thm:euler-polyhedron} also holds for the initial planar graph.
\end{proof}
But if there are crossings, the notion of a region and thus also the number of regions \(|F|\) is not defined.
We find bounds on the number of regions \(|F|\) in terms of the number of edges \(|E|\).





Since a simple graph (Definition~\ref{def:simple-graph}) does not contain loops or multiple edges between two vertices, every region must be surrounded by at least three edge-sides.
The number of edge-sides is twice the number of edges, and thus
\[
3|F| \leq 2|E| \implies |F| \leq \frac{2}{3}|E|
\]
If the graph is bipartite, every region is bounded by at least four edge-sides, and thus
\[
4|F| \leq 2|E| \implies |F| \leq \frac{1}{2}|E|
\]
Inserting those formulae into Theorem~\ref{thm:euler-polyhedron} yields
\begin{corollary}
In a finite, connected, simple, planar graph \(G=(V,E)\)
\begin{equation}\label{eq:planar-graph-inequality}
|E| \leq 3 |V| - 6
\end{equation}
if \(|V| \geq 3\).
If the graph is also bipartite, then
\begin{equation}\label{eq:planar-bipartite-graph-inequality}
|E| \leq 2 |V| - 4 %\qedhere
\end{equation}
\end{corollary}
\begin{proof}
% from https://www.cse.iitb.ac.in/~ranade/408/planar.pdf
If the graph is simple, then every face has at least 3 edges. 
Now $3 |F|$ would count every edge 2 times, so we have $3 |F| \leq 2 |E|$. 
But $|E|+2=|V|+|F| \leq$ $|V|+2 |E| / 3$. 
So $3 |E|+6 \leq 3 |V|+2 |E|$. So $|E| \leq 3 |V|-6$. 
In a bipartite graph every face must have at least 4 sides. 
Thus $4 |F| \leq 2 |E|$, and the result follows similarly.
\end{proof}

\paragraph{Average degree}
The inequalities \eqref{eq:planar-graph-inequality} and \eqref{eq:planar-bipartite-graph-inequality} imply for the average degree
\[
\overline{\deg}(G) := \frac{1}{|V|} \sum_{v \in V} \deg(v) = \frac{2|E|}{|V|}
\]
For a planar graph,
\[
|E| \leq 3|V| - 6 < 3|V| \implies \overline{\deg}(G) < 6
\]
For a bipartite planar graph,
\[
|E| \leq 2|V| - 4 < 2|V| \implies \overline{\deg}(G) < 4
\]

\paragraph{Characterizing planarity}
In Example~\ref{ex:k5-k33-not-planar}, $K_5$ violates \eqref{eq:planar-graph-inequality} and $K_{3,3}$ violates \eqref{eq:planar-bipartite-graph-inequality}.
In fact, it turns out that \(K_5\) and \(K_{3,3}\) are fundamental obstacles to planarity (Theorem~\ref{thm:kuratowski}).

A subdivision of a graph \(G\) is a graph \(G'\) obtained by inserting vertices into edges of \(G\) zero or more times.
\begin{theorem}[Kuratowski]\label{thm:kuratowski}
A graph is planar if and only if it does not contain a subgraph that is a subdivision of \(K_5\) or \(K_{3,3}\) (Example~\ref{ex:k5-k33-not-planar}).
\end{theorem}
So every non-planar graph ``contains'' \(K_5\) or \(K_{3,3}\) in some sense.


% In other words, the degree of every face must be at least \(3\), and therefore
% \[
% 3|F| \leq \sum_{f \in F} \deg(f) = 2|E| \implies |F| \leq \frac{2}{3}|E|
% \]
% Inserting this into Theorem~\ref{thm:euler-polyhedron} yields
% \begin{corollary}
% Any planar graph with \(|V| \geq 3\) satisfies the following inequality:
% \[
% |E| \leq 3|V| - 6 \qedhere
% \]
% \end{corollary}

% A face in a bipartite graph is bounded by at least \(4\) edges, so
% \[
% 4|F| \leq 2|E| \implies |F| \leq \frac{1}{2}|E|
% \]
% Inserting this into Theorem~\ref{thm:euler-polyhedron} yields
% \begin{corollary}
% Any bipartite planar graph satisfies
% \[
% |E| \leq 2|V| - 4 \qedhere
% \]
% \end{corollary}





\subsection{Graph Colorings}
\begin{definition}[Coloring]\label{def:coloring}
A \(k\)-coloring of a graph \(G=(V,E)\) is a function
\[
c: V \to \{c_1, \ldots, c_k\}
\]
such that \(c(v_1) \neq c(v_2)\) for all edges \((v_1,v_2) \in E\).
\end{definition}
\begin{definition}[Chromatic number]\label{def:chromatic-number}
  The chromatic number \(\chi(G)\) of a graph \(G=(V,E)\) is the minimal \(k\) such that a \(k\)-coloring of \(G\) exists.
\end{definition}
\begin{example}[Chromatic numbers of special graphs]\label{ex:chromatic-numbers}
\hfill
\begin{itemize}
\item Planar graphs $G_p$ have
$
\chi(G_p) \leq 4
$.
\item Complete graphs $K_n$ have $\chi(K_n)=n$ as every vertex needs its own color.
\item The complete bipartite graphs have a chromatic number $\chi(K_{m, n})=2$ by definition.
\item Mesh graphs are bipartite: $\chi(M_{m, n})=2$ (except for $m=n=1$).
\item Hypercubes are bipartite: $\chi(Q_d)=2$ for $d \geq q$. This follows from an inductive argument: Obviously, $Q_1$ is two-colorable. If $Q_d$ is twocolorable, then we can color the second $Q_d$ with flipped colors before connecting the two to construct $Q_{d+1}$.
\item For cycles, it depends on the parity of their length:
$
\chi(C_n)= \begin{cases}2 & n \text { even } \\ 3 & n \text { odd }\end{cases}
$
\item Trees are two-colorable, $\chi(T)=2$ (for any tree with at least an edge), as we can arrange the nodes in ``levels''. 
Then all levels with odd parity (depth) are colored in one color and those with even parity in another. \qedhere
\end{itemize}
\end{example}

\begin{theorem}\label{thm:bipartite-cycle}
A graph \(G=(V,E)\) is bipartite if and only if it does not contain an odd-length cycle.
\end{theorem}
\begin{proof}
If $G$ is bipartite, it cannot contain an odd cycle, since then $\chi(G) \geq 3$. 
It remains to show that, if $G$ contains no odd cycle, then it is also bipartite. 
Let's consider a spanning tree of $G$. 
We can then color the levels with two colors. 
We now need to show that there are no edges in $G$ that connect two levels of the same color. 
This follows from the observation that adding an edge connecting two vertices of levels with same parity always introduces an odd cycle, and thus a contradiction.
\end{proof}

Theorem~\ref{thm:bipartite-cycle} yields a linear-time algorithm to decide whether a graph is two-colorable. 
To decide whether a graph is \(3\)-colorable is an NP-complete problem. 
Again, we see the two extremes of computational hardness in two very similar-sounding problems.


% \clearpage
% \section{Cryptography}
% \subsection{Diffie-Hellman key exchange}
% \subsection{The RSA cryptosystem}
% \subsubsection{Euclid's algorithm}
% \subsubsection{The extended Euclidean algorithm}
% \subsubsection{The Chinese remainder theorem}
% \subsubsection{Fermat's little theorem}
% \subsubsection{Chinese Euclid}
% \subsubsection{The RSA protocol}
% \subsection{Zero-knowledge proofs}
% \subsubsection{The magic cave}
% \subsubsection{Zero-knowledge proof of discrete logarithm}
% \subsection{Cryptography and the quantum computer}






% \renewcommand{\emph}[1]{\textcolor{black}{#1}}
% \printbibliography[heading=bibintoc,title={References}]



\end{document}