% !TEX root = ../algo-summary.tex
\section{Dynamic Programming}\label{sec:dynamic_programming}


\textcolor{AccentBlue}{Greedy}. 
Build up a solution incrementally, myopically optimizing some local criterion.

\medskip

\textcolor{AccentBlue}{Divide-and-conquer}. 
Break up a problem into two sub-problems of roughly equal size; 
solve each sub-problem independently; 
combine the two sub-problem solutions to form solution to original problem.

\medskip

\textcolor{AccentRed}{Dynamic programming}. 
Break up a problem into many overlapping sub-problems; 
combine solutions to small subproblems to build up solutions to larger and larger sub-problems.
\begin{itemize}
\item overlapping sub-problem = a sub-problem whose results can be reused many times
\item Hint: express your optimal solution by a recursive formula
\end{itemize}

\medskip

\textcolor{AccentBlue}{Some famous dynamic programming algorithms}:
\begin{itemize}
  \item Viterbi for Hidden Markov Models
  \item Unix diff for comparing two files
  \item Smith-Waterman for sequence alignment
  \item \nameref{alg:bellmanford} for shortest path routing in networks
  \item Cocke-Kasami-Younger for parsing context-free grammars
\end{itemize}


\bigskip


Powerful technique for solving optimization problems, which have certain well-defined clean structural properties.

\bigskip

\begin{definition}[Principle of optimality]\label{def:principle_of_optimality}
\hl[5]{For the global problem to be solved optimally, each subproblem must be solved optimally.}
\end{definition}
\indent \(\rightarrow\) this principle must hold to apply DP

\bigskip
\smallskip

The global optimal solution consists of optimal solutions to subproblems.

\medskip
Polynomial time DP:
\begin{itemize}
\item Polynomially-many distinct overlapping subproblems (polynomial in input size).
\item DP does not necessarily lead to a polynomial time algorithm
\end{itemize}
\medskip

DP selection principle: \hl{When given a set of feasible options to choose from, try them all and take the best}












\subsection{Weighted Interval Scheduling}








We denote by \(p(j)\) the largest index \(i\) with \(i < j\) and such that job \(i\) is compatible with \(j\), i.e.
\begin{equation}\label{eq:interval_scheduling_pj}
  p(j)
  :=
  \begin{cases}
    \max\{ i \mid f_i \leq s_j\} & \exists i \text{ with } f_i \leq s_j\\
    0 & \text{otherwise}
  \end{cases}
\end{equation}



\begin{algorithm}[h]
\caption{Compute \(p(j)\) values}\label{alg:compute_pj}
\begin{algorithmic}[1]
\Require Jobs \(1, \ldots, n\) sorted according to their finish time \(f_1 \leq \cdots \leq f_n\)
\State Sort all starting and finish times in a single array \(A\)
\State \(\texttt{index} \gets 0\)
\ForAll{\(e \in A\)} \Comment{go through all times (`events') in ascending order}
  \If{\(e = f_i\)}
    \State \(\texttt{index} \gets i\)
  \ElsIf{\(e = s_j\)}
    \State \(p[j] \gets \texttt{index}\)
  \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

The algorithm goes through every time slot, and it remembers the index of the last finish time.
When we encounter a starting time $s_j$, its $p$ value is the largest index $i$ such that $f_i \leq s_j$, which is exactly the value of \texttt{index} in the algorithm. 
Hence, \autoref{alg:compute_pj} is correct.

Sorting takes $O(n \log n)$ time, and it is easy to see that the for loop takes $O(n)$ time.



% \textcolor{AccentBlue}{Dynamic programming formulation.}
Assume jobs are labeled so that \(f_1 \le \cdots \le f_n\).
Let \(v_j\) be the value/weight of job \(j\), and let
\(
\operatorname{OPT}(j)
\)
denote the maximum total weight of a subset of mutually compatible jobs among \(1,\ldots,j\).
We use the convention \(\operatorname{OPT}(0) = 0\).

\textcolor{AccentBlue}{Recurrence.}
For \(j \ge 1\) we have
\[
\operatorname{OPT}(j)
=
\max\bigl\{
  v_j + \operatorname{OPT}(p(j)),\;
  \operatorname{OPT}(j-1)
\bigr\}
\]
\begin{itemize}
  \item Either the optimal solution does \emph{not} take job \(j\): then its value is \(\operatorname{OPT}(j-1)\).
  \item Or the optimal solution \emph{does} take job \(j\): then we gain \(v_j\) plus the best compatible subset among jobs \(\{1,\ldots,p(j)\}\), which has value \(\operatorname{OPT}(p(j))\).
\end{itemize}

% To reconstruct an optimal set of jobs, start from \(j = n\) and trace backwards:
% \begin{itemize}
%   \item if \(v_j + \operatorname{OPT}[p[j]] \ge \operatorname{OPT}[j-1]\) then take job \(j\) and continue with \(p(j)\);
%   \item otherwise skip job \(j\) and continue with \(j-1\).
% \end{itemize}


\begin{algorithm}[h]
\caption{Weighted Interval Scheduling}\label{alg:weighted_interval_scheduling_dp}
\begin{algorithmic}[1]
\Require jobs \(1,\ldots,n\) sorted by finish time, values \(v_j\), and precomputed \(p(j)\) as in \eqref{eq:interval_scheduling_pj}
\State \(\operatorname{OPT}[0] \gets 0\)
\For{$j = 1 \TO n$}
  \State \(\operatorname{OPT}[j] \gets \max\{\,v_j + \operatorname{OPT}[p[j]],\; \operatorname{OPT}[j-1]\,\}\)
\EndFor
\State \Return \(\operatorname{OPT}[n]\)
\end{algorithmic}
\end{algorithm}

\textcolor{AccentBlue}{Running time.}
Computing the \(\operatorname{OPT}[j]\) table takes \(O(n)\) time once the jobs are sorted and the \(p(j)\) values are known.
Together with sorting and the \(p(j)\)-computation, the total running time is \(O(n \log n)\) and the space usage is \(O(n)\).


\subsection{Segmented Least Squares}
Given \(n\) points \((x_1,y_1),\ldots,(x_n,y_n)\) in the plane (assume \(x_1 < \cdots < x_n\)),
we want to approximate them by a small number of straight-line segments.

\begin{itemize}
  \item For any \(1 \le i \le j \le n\), consider fitting a single line to the points
        \((x_i,y_i),\ldots,(x_j,y_j)\) by least squares.
  \item Let \(e(i,j)\) be the sum of squared vertical errors of this best-fit line:
        \[
          e(i,j)
          = \sum_{k=i}^{j}
            \bigl(y_k - (\hat a_{ij} x_k + \hat b_{ij})\bigr)^2,
        \]
        where \(\hat a_{ij}, \hat b_{ij}\) are the least-squares coefficients.
  \item Each segment we use incurs a fixed penalty \(C > 0\) (to discourage using too many segments).
\end{itemize}

\textcolor{AccentBlue}{Goal.}
Partition the sequence of points into contiguous blocks
\(
[1,\ell_1], [\ell_1+1,\ell_2], \ldots, [\ell_{m-1}+1,n]
\)
and fit each block with its own least-squares line so as to minimize
\[
  \sum_{r=1}^{m} e(\ell_{r-1}+1,\ell_r) + m C
  \quad
  (\ell_0 := 0)
\] 

\textcolor{AccentBlue}{Precomputation.}
Using prefix sums of \(x_k, y_k, x_k^2, x_k y_k\), all \(e(i,j)\) values can be computed in total \(O(n^2)\) time. 

Define
\[
\operatorname{OPT}(j)
:= \text{minimum total cost for approximating points } (x_1,y_1),\ldots,(x_j,y_j)
\]
using any number of segments, and set \(\operatorname{OPT}(0) := 0\).

\textcolor{AccentBlue}{Recurrence.}
For \(j \ge 1\),
\[
\operatorname{OPT}(j)
=
\min_{1 \le i \le j}
\bigl\{
  \operatorname{OPT}(i-1) + C + e(i,j)
\bigr\}.
\]
\begin{itemize}
  \item The last segment in an optimal solution for the first \(j\) points must cover
        some suffix \(i,\ldots,j\).
  \item Its cost is \(e(i,j)\) plus the penalty \(C\), plus the optimal cost for the prefix \(1,\ldots,i-1\).
\end{itemize}

\begin{algorithm}[h]
\caption{Segmented Least Squares via DP}\label{alg:segmented_least_squares}
\begin{algorithmic}[1]
\Require points \((x_1,y_1),\ldots,(x_n,y_n)\) with \(x_1 < \cdots < x_n\), segment penalty \(C\)
\State precompute all \(e(i,j)\) for \(1 \le i \le j \le n\)
\State \(\operatorname{OPT}[0] \gets 0\)
\For{$j = 1 \TO n$}
  \State \(\operatorname{OPT}[j] \gets \infty\)
  \For{$i = 1 \TO j$}
    \State \(\operatorname{OPT}[j] \gets
           \min\bigl\{
             \operatorname{OPT}[j],\;
             \operatorname{OPT}[i-1] + C + e(i,j)
           \bigr\}\)
  \EndFor
\EndFor
\State \Return \(\operatorname{OPT}[n]\)
\end{algorithmic}
\end{algorithm}

\textcolor{AccentBlue}{Running time.}
Precomputing all errors \(e(i,j)\) takes \(O(n^2)\) time.
The DP itself also takes \(O(n^2)\) time and \(O(n)\) space for the \(\operatorname{OPT}\) array.
Backtracking through the choices of \(i\) at each \(j\) yields the optimal segmentation.


\subsection{Knapsack Problem}

\begin{problem}[0/1 Knapsack Problem]\label{prob:knapsack}
Given \(n\) items, each with a weight \(w_i \in \N\) and a value \(v_i \in \N\), and a ``knapsack'' with capacity \(W \in \N\),
fill the knapsack (respecting its capacity) to maximize the total value.
\end{problem}

0/1 refers to the fact that the items are not divisible, i.e. each item can either be included (1) or excluded (0) from the knapsack, but not fractionally included (otherwise it would be the fractional knapsack problem, which can be solved greedily by taking items in descending order of value-to-weight ratio).

\[
\begin{tikzpicture}[line join=round, line cap=round, font=\sffamily\footnotesize, scale=0.4]

% ---------- helpers ----------
\definecolor{bagyellow}{RGB}{248,188,46}
\definecolor{baggold}{RGB}{230,160,40}
\definecolor{bagred}{RGB}{194,62,40}

% book(pic): (x,y), angle, fill, price, weight, scale
\newcommand{\book}[7]{%
    \colorlet{bookbase}{#4}
    \colorlet{bookback}{bookbase!60!black}
    \colorlet{bookspine}{bookbase!50!white}
  \begin{scope}[shift={(#1,#2)},rotate=#3,scale=#7,]
    % cover
    \fill[rounded corners=0pt,fill=bookbase] (-1.9,-2.6) rectangle (1.9,2.6);
    \fill[rounded corners=0pt,fill=bookback] (-1.9,-2.6) rectangle (-1.6,2.6);
    % pages strip
    \fill[rounded corners=0pt,white] (-1.7,-2.5) rectangle (1.9,-2.2);
    % \draw[black!12, very thin] (-1.65,-2.45) -- (-1.65,-2.25);
    \foreach \y in {-2.45, -2.4, -2.35, -2.3, -2.25}{%
      \draw[black!12, very thin] (-1.65,\y) -- (1.9,\y);
    }
    % text
    \node[align=center,scale=1.2, rotate=#3] at (0,0.5) {\$#5};
    \node[align=center,scale=1.1, rotate=#3] at (0,-0.4) {#6 kg};
  \end{scope}%
}

% ---------- backpack ----------
% side pockets
\fill[bagred,rounded corners=6pt] (-3.6,-3) rectangle (-2.6,0.6);
\fill[bagred,rounded corners=6pt] ( 2.6,-3) rectangle ( 3.6,0.6);

% top handle
\draw[line width=6pt,bagred] (0.55,4.1) arc (0:180:0.55 and 0.48);
% \draw[line width=8pt,bagred] (0,4.1) arc (90:270:0.55 and 0.48);


% main body
\fill[baggold!85!black,rounded corners=6pt] (-3.0,-3.4) rectangle (3.0,4.1);
\fill[bagyellow!95!white,rounded corners=6pt] (-3.0,-2.4) rectangle (3.0,4.1);


% front pocket
\fill[bagyellow!95!white] (-2.0,-0.4) -- (2.0, -0.4) [rounded corners=5pt] -- (2.0,-3) [rounded corners=5pt] -- (-2, -3) [rounded corners=0pt] -- cycle;
\fill[baggold!85!black] (-2.0,-0.4) -- (2.0, -0.4)  -- (2.0,-1.5)  .. controls (1,-2) and (-1,-2) .. (-2, -1.5) -- cycle;
\fill[black] (0,-1.25) circle (0.2);

% flap
\fill[baggold!85!black] (-2.6,4.1) -- (2.6, 4.1) [rounded corners=5pt] -- (2.6, 1) .. controls (1.0, 0) and (-1, 0) .. (-2.6, 1) [rounded corners=0pt] -- cycle;


% shoulder straps + buckles
\fill[rounded corners=2pt,bagred] (-2.3,4.2) rectangle (-1.7,0.2);
\fill[rounded corners=2pt,bagred] ( 2.3,4.2) rectangle ( 1.7,0.2);
\draw[black,rounded corners=2pt, line width=2pt] (-2.4,1.4) rectangle (-1.6,2.2);
\draw[black,rounded corners=2pt, line width=2pt] ( 1.6,1.4) rectangle (2.4,2.2);

% weight label
\node[font=\sffamily] at (0,2.5) {15 kg};

% ---------- books ----------
\book{-6}{2.0}{20}{green!40}{4}{12}{0.8}
\book{-6.2}{-2.0}{-15}{gray!30}{2}{1}{0.4}
\book{-9}{0}{15}{orange!40}{1}{1}{0.5}
\book{5.7}{2.6}{-25}{cyan!35}{2}{2}{0.6}
\book{5.7}{-1.0}{25}{yellow!60}{10}{4}{0.7}

\end{tikzpicture}
\]


% \[
% \operatorname{opt}(i, w) 
% =
% \begin{cases}
% 0 & \text{if } i = 0\\
% \operatorname{opt}(i-1, w) & \text{if } w_i > w\\
% \max\{ v_i + \operatorname{opt}(i-1, w - w_i), \operatorname{opt}(i-1, w)\} & \text{otherwise}
% \end{cases}
% \]
% % where \(i\) is the index of the current item, \(w\) is the remaining weight capacity, \(w_i\) is the weight of item \(i\) and \(v_i\) is the value of item \(i\).     



\definecolor{knapblue}{RGB}{0,130,130}
\definecolor{knapred}{RGB}{180,0,0}
recursive formulation:
\[
\operatorname{opt}(i, w)
=
\left\{
\begin{array}{@{}l@{\quad}l@{\quad}l@{}}
{0}
  & {i = 0}
  & \textcolor{knapred}{\triangleright\ \text{no items left}}\\[0.3em]
{\operatorname{opt}(i-1, w)}
  & {w_i > w}
  & \textcolor{knapred}{\triangleright\ \text{item too heavy}}\\[0.3em]
{\max\{v_i + \operatorname{opt}(i-1, w - w_i),\ \operatorname{opt}(i-1, w)\}}
  & {\text{otherwise}}
  & \textcolor{knapred}{\triangleright\ \text{include or exclude item $i$?}}
\end{array}
\right.
\]



% \begin{algorithmic}[1]
% \Function{opt}{$i, w$}
%   \If{$i = 0$} \Comment{no items left}
%     \State \Return \(0\)
%   \ElsIf{$w_i > w$} \Comment{item too heavy}
%     \State \Return \Call{opt}{$i-1, w$}
%   \Else \Comment{include or exlude item \(i\)?}
%     \State \Return \(\max\{ v_i + \Call{opt}{i-1, w - w_i}, \Call{opt}{i-1, w} \}\)
%   \EndIf
% \EndFunction
% \end{algorithmic}



% \[
% \operatorname{opt}(i,w) 
% =
% \begin{cases}
% 0 & \text{if } i = 0 \text{ (no items left)}\\[0.3em]
% \operatorname{opt}(i-1, w) & \text{if } w_i > w \text{ (too heavy)}\\[0.3em]
% \max\{ v_i + \operatorname{opt}(i-1, w - w_i), \operatorname{opt}(i-1, w)\}
%   & \text{otherwise (include or exclude item $i$?)}
% \end{cases}
% \]



top down memoized:
\begin{algorithm}[h]
\caption{Knapsack (top-down, memoized)}\label{alg:knapsack_topdown}
\begin{algorithmic}[1]
\Function{Opt}{$i, w$}
  \If{\(M[i,w] = \nil\)} 
    \If{\(i = 0\)} 
    \State \(M[i,w] \gets 0\) \Comment{no items left}
    \ElsIf{\(w_i > w\)} 
    \State \(M[i,w] \gets \Call{Opt}{i-1, w}\) \Comment{item too heavy}      
    \Else 
    \State \(M[i,w] \gets \max\{ v_i + \Call{Opt}{i-1, w - w_i}, \Call{Opt}{i-1, w} \}\) \Comment{include or exclude?}
    \EndIf
  \EndIf
  \State \Return \(M[i,w]\)
\EndFunction
\end{algorithmic}
\end{algorithm}


\textcolor{AccentBlue}{Bottom-up (tabulation) version.}
Instead of recursion + memoization, we can fill a DP table iteratively.
Let \(W\) be the knapsack capacity.
We build a table \(\operatorname{OPT}[i,w]\) for \(i = 0,\ldots,n\) and \(w = 0,\ldots,W\),
where
\(
\operatorname{OPT}[i,w]
\)
denotes the maximum value achievable using items \(1,\ldots,i\) with capacity \(w\).


\begin{algorithm}[h]
\caption{Knapsack (bottom-up DP)}\label{alg:knapsack_bottomup}
\begin{algorithmic}[1]
\Require items \(1,\ldots,n\) with weights \(w_i\), values \(v_i\); capacity \(W\)
\For{$w = 0 \TO W$}
  \State \(\operatorname{OPT}[0,w] \gets 0\) \Comment{no items}
\EndFor
% \For{$i = 0 \TO n$}
%   \State \(\operatorname{OPT}[i,0] \gets 0\) \Comment{zero capacity}
% \EndFor
\For{$i = 1 \TO n$}
  \For{$w = 0 \TO W$}
    \If{$w_i > w$}
      \State \(\operatorname{OPT}[i,w] \gets \operatorname{OPT}[i-1,w]\) \Comment{item \(i\) too heavy}
    \Else
      \State \(\operatorname{OPT}[i,w] \gets
        \max\bigl\{
          v_i + \operatorname{OPT}[i-1, w - w_i],\;
          \operatorname{OPT}[i-1, w]
        \bigr\}\)
    \EndIf
  \EndFor
\EndFor
\State \Return \(\operatorname{OPT}[n,W]\)
\end{algorithmic}
\end{algorithm}

\textcolor{AccentBlue}{Running time.}
The table has \((n+1)(W+1) = O(nW)\) entries, 
and each is filled in \(O(1)\) time, so the running time is \(O(nW)\) and the space is \(O(nW)\).
By keeping only two rows at a time, the space can be reduced to \(O(W)\).

\begin{remark}
0/1 Knapsack is NP-hard in general, so we do not expect a polynomial-time algorithm in the input size.
The DP runs in time polynomial in \(n\) and \(W\), which is called \emph{pseudo-polynomial} time,
because \(W\) is exponential in the number of bits needed to represent it (i.e. the input size).
\end{remark}


\textcolor{AccentBlue}{Approximation algorithm}.
There exists a polynomial algorithm that produces a feasible solution that has value within 0.01\% of the optimum.






\subsection{Sequence Alignment}

\textcolor{AccentBlue}{Goal}.
Given two strings \(X = x_1x_2\cdots x_m\) and \(Y = y_1y_2\cdots y_n\),
find an {alignment of minimum total cost}.


\begin{definition}[Alignment]\label{def:alignment}
An \textcolor{AccentRed}{alignment} \(M\) between two strings \(X\) and \(Y\) is a set of ordered pairs \((x_i, y_j)\) such that each item occurs in at most one pair and no two pairs cross.
\end{definition}

\begin{definition}\label{def:cross}
Two pairs \((x_i, y_j)\) and \((x_{i'}, y_{j'})\) \textcolor{AccentRed}{cross} if \(i < i'\) but \(j > j'\).
\end{definition}

\begin{definition}[Cost Model]\label{def:cost_model}
% \textcolor{AccentBlue}{Cost model}.
Aligning \(x_i\) with \(y_j\) incurs a \emph{mismatch} penalty \(\alpha_{x_i y_j}\).
Leaving a character unmatched (aligning it with a gap ``\(-\)'') incurs a \emph{gap} penalty \(\delta > 0\).
\end{definition}
  % The \hl{edit distance} between \(X\) and \(Y\) is the minimum cost over all alignments.

\begin{definition}[Cost]\label{def:alignment_cost}
The \textcolor{AccentRed}{cost} of an alignment \(M\) is
\[
  \operatorname{cost}(M)
  =
  \underbrace{\sum_{(x_i, y_j) \in M} \alpha_{x_i y_j}}_{\text{mismatch}} 
  + 
  \underbrace{\sum_{i \colon \text{\(x_i\) unmatched}} \delta+\sum_{j \colon \text{\(y_j\) unmatched}} \delta}_{\text {gap }}
\]
i.e. it is the sum of all gap and mismatch penalties.
\end{definition}

\begin{definition}[Edit distance]\label{def:edit_distance}
The \textcolor{AccentRed}{edit distance} between strings \(X\) and \(Y\) is the minimum \nameref{def:alignment_cost} over all \nameref{def:alignment}s  
\end{definition}

\textcolor{AccentBlue}{DP formulation}.
Let
\(
\operatorname{OPT}(i,j)
\) 
denote the minimum cost of aligning the prefixes \(x_1\cdots x_i\) and \(y_1\cdots y_j\).
If either of the prefixes is empty and the other has length \(k\), the only option is to align all characters of the other prefix with gaps, incurring a cost of \(\delta\) per gap, i.e.
\(\operatorname{OPT}(k,0) = \operatorname{OPT}(0,k) = k \cdot \delta\).
Otherwise we have three choices:
\begin{itemize}
  \item Case 1: match \(x_i\) with \(y_j\): \(\alpha_{x_i y_j} + \operatorname{OPT}(i-1,j-1)\)
  \begin{itemize}
    \item[-] pay mismatch cost \(\alpha_{x_i y_j}\) plus whatever the minimum cost is for aligning the prefixes \(x_1\cdots x_{i-1}\) and \(y_1\cdots y_{j-1}\)
  \end{itemize}
  \item Case 2a: leave \(x_i\) unmatched (align with gap): \(\delta + \operatorname{OPT}(i-1,j)\)
  \begin{itemize}
    \item[-] pay gap cost \(\delta\) for \(x_i\) plus min cost for aligning \(x_1\cdots x_{i-1}\) and \(y_1\cdots y_{j}\)
  \end{itemize}
  \item Case 2b: leave \(y_j\) unmatched (align with gap): \(\delta + \operatorname{OPT}(i,j-1)\)
  \begin{itemize}
    \item[-] pay gap cost \(\delta\) for \(y_j\) plus min cost for aligning \(x_1\cdots x_{i}\) and \(y_1\cdots y_{j-1}\)
  \end{itemize}
\end{itemize}


We get the recurrence:
\[
\operatorname{OPT}(i,j)
=
\begin{cases}
j \cdot \delta & \text{if \(i = 0\)}\\
i \cdot \delta & \text{if \(j = 0\)}\\
\min
\begin{cases}
\alpha_{x_i y_j} + \operatorname{OPT}(i-1,j-1)\\
\delta + \operatorname{OPT}(i-1,j)\\
\delta + \operatorname{OPT}(i,j-1)
\end{cases}
& \text{otherwise}
\end{cases}
\]

\medskip
To compute \(\operatorname{OPT}(m,n)\):
\begin{itemize}
\item build up the values of \(\operatorname{OPT}(i,j)\) using the recurrence -- fill an \(m \times n\) table
\item \(O(mn)\) subproblems
\item to compute \(\operatorname{OPT}(i,j)\) we need: \(\operatorname{OPT}(i-1,j)\), \(\operatorname{OPT}(i-1,j-1)\), \(\operatorname{OPT}(i,j-1)\)
\item can fill table \emph{row by row} or \emph{column by column}
\item cost we are looking for will be in bottom-right cell \(\operatorname{OPT}(m,n)\)
\end{itemize}


\begin{algorithm}[h]
\caption{Sequence Alignment}\label{alg:sequence_alignment}
\begin{algorithmic}[1]
\Require strings \(X=x_1\cdots x_m\), \(Y=y_1\cdots y_n\); gap penalty \(\delta\); mismatch costs \(\alpha_{pq}\)
\For{$i = 0 \TO m$}
  \State \(M[i,0] \gets i\delta\)
\EndFor
\For{$j = 0 \TO n$}
  \State \(M[0,j] \gets j\delta\)
\EndFor
\For{$i = 1 \TO m$}
  \For{$j = 1 \TO n$}
    \State \(M[i,j] \gets \min\bigl\{\alpha_{x_i y_j}+M[i-1,j-1], \delta+M[i-1,j], \delta+M[i,j-1]\bigr\}\) \label{line:seqalign_dp_decision}
  \EndFor
\EndFor
\State \Return \(M[m,n]\)
\end{algorithmic}
\end{algorithm}

\textcolor{AccentBlue}{Running time}.
The table has \((m+1)(n+1) = O(mn)\) entries and each entry is computed in \(O(1)\),
so the running time is \(O(mn)\) and the space usage is \(O(mn)\).


\begin{remark}
We can reduce the space to \(O(\min\{m,n\})\) if only the optimal cost is needed by keeping only two rows/columns in memory at a time while ``filling the table''.
Because to compute \(M[i,j]\) we only need \(M[i-1,j-1]\), \(M[i-1,j]\), and \(M[i,j-1]\).
\end{remark}


\textcolor{AccentBlue}{Recovering an optimal alignment}.
Two options:
\begin{itemize}
\item Do a postprocessing step after filling the table:
Starting at \((m,n)\), backtrack to \((0,0)\) by comparing \(M[i,j]\) with the three candidate predecessors:
\((i-1,j-1)\), \((i-1,j)\), \((i,j-1)\).
Record the corresponding aligned characters (\(x_i\) vs. \(y_j\), \(x_i\) vs. ``\(-\)'', or ``\(-\)'' vs. \(y_j\)).
Ties correspond to multiple optimal alignments.
\item Store another array (hint table):
Every time we make a decision in \autoref{line:seqalign_dp_decision} of \autoref{alg:sequence_alignment}, 
keep a hint of where we came from.
\end{itemize}


% \clearpage










\subsection{Longest Common Subsequence}
Let us think of character strings as sequences of characters. 
Given $X=\left\langle x_1, \ldots, x_l\right\rangle$, we say that $Z=\left\langle z_1, \ldots, z_k\right\rangle$ is a subsequence of $X$ if there is a strictly increasing sequence of $k$ indices $\left\langle i_1, \ldots, i_k\right\rangle$ such that $Z=\left\langle x_{i_1}, \ldots, x_{i_k}\right\rangle$. 

Given two strings \(X=\left\langle x_1, \ldots, x_m\right\rangle\) and \(Y = \left\langle y_1, \ldots, y_n \right\rangle\), the longest common subsequence of $X$ and $Y$ is a longest sequence $Z$ that is a subsequence of both $X$ and $Y$. 

\subsubsection{Brute force}
A brute force approach would be to enumerate all subsequences of $X$ and check for each whether it is also a subsequence of $Y$. 
There are $2^m$ subsequences of \(X\) and checking wether it is a subsequence of \(Y\) takes \(O(n)\) time:
Scan \(Y\) for first occurence, from there scan for second, and so on.
In total this takes \(\Theta(n 2^m)\) time.


\subsubsection{Dynamic programming formulation}
A prefix \(X_i\) of a sequence \(X\) is just an initial string of values, \(X_i = \langle x_1, \ldots, x_i \rangle\).
\(X_0\) is the empty sequence.
The idea is to compute the longest common subsequence for every possible pair of prefixes (there are \(O(mn)\) such pairs).

Let $\operatorname{lcs}(i, j)$ denote the length of the longest common subsequence of $X_i$ and $Y_j$.
We have the following recursive relation:
\[
\operatorname{lcs}(i, j)
= 
\begin{cases}
  0 & \text { if } i=0 \text { or } j=0 \\ 
  \operatorname{lcs}(i-1, j-1)+1 & \text { if } i, j>0 \text { and } x_i=y_j \\ 
  \max (\operatorname{lcs}(i-1, j), \operatorname{lcs}(i, j-1)) & \text { if } i, j>0 \text { and } x_i \neq y_j
\end{cases}
\]


Last characters match:
\[
\begin{tikzpicture}[scale=0.5, font=\footnotesize]

%--- left strings Xi, Yj ------------------------------------------
% Xi row
\node at (0,1.2) {$X_i$};
\draw[fill=gray!30] (0.6,0.8) rectangle (3.6,1.6);   % prefix of Xi
\draw              (3.6,0.8) rectangle (4.4,1.6);     % last char
\node at (4.0,1.2) {$A$};
\node at (4.0,2.0) {$x_i$};

% Yj row
\node at (0,-1.2) {$Y_j$};
\draw[fill=gray!30] (0.6,-1.6) rectangle (2.8,-0.8);  % prefix of Yj
\draw              (2.8,-1.6) rectangle (3.6,-0.8);   % last char
\node at (3.2,-1.2) {$A$};
\node at (3.2,-0.4) {$y_j$};

%--- branching + arrow --------------------------------------------
\coordinate (c) at (5.2,0);
\draw (4.8,1.0) -- (c) -- (4.8,-1.0);
\draw[->] (c) -- (7.4,0);

%--- right strings Xi-1, Yj-1 -------------------------------------
% Xi-1 row
\node at (7.4,1.2) {$X_{i-1}$};
\draw[fill=gray!30] (8.2,0.8) rectangle (11.2,1.6);

% Yj-1 row
\node at (7.4,-1.2) {$Y_{j-1}$};
\draw[fill=gray!30] (8.2,-1.6) rectangle (10.4,-0.8);
% vertical lcs arrow and label
\draw[<->] (8.5,0.8) -- (8.5,-0.8);
\node[right] at (8.5,0) {$\operatorname{lcs}(i-1,j-1)$};

%--- "+1" and two A boxes -----------------------------------------
\node[left] at (14,0) {$+1$};

\draw (13.6,0.8)  rectangle (14.4,1.6);
\draw (13.6,-1.6) rectangle (14.4,-0.8);
\node at (14.0,1.2) {$A$};
\node at (14.0,-1.2) {$A$};
\draw[<->] (14.0,0.8) -- (14.0,-0.8);

\end{tikzpicture}
\]


Last characters do not match:
\[
\begin{tikzpicture}[scale=0.5, font=\footnotesize]

%--- left strings Xi, Yj ------------------------------------------
% Xi row
\node[left] at (0.6,1.2) {$X_i$};
\draw[fill=gray!30] (0.6,0.8) rectangle (3.6,1.6);   % prefix of Xi
\draw              (3.6,0.8) rectangle (4.4,1.6);     % last char
\node at (4.0,1.2) {$A$};
\node at (4.0,2.0) {$x_i$};

% Yj row
\node[left] at (0.6,-1.2) {$Y_j$};
\draw[fill=gray!30] (0.6,-1.6) rectangle (2.8,-0.8);  % prefix of Yj
\draw              (2.8,-1.6) rectangle (3.6,-0.8);   % last char
\node at (3.2,-1.2) {$B$};
\node at (3.2,-0.4) {$y_j$};

%--- branching + arrow --------------------------------------------
\coordinate (c) at (5.2,0);
\draw (4.8,1.0) -- (c) -- (4.8,-1.0);
\draw[->] (c) -- (6.6,0);
\node[above] at (5.9,0) {$\max$};
\draw (6.6,3.0) -- (6.6,-3.0);
\draw[->] (6.6,3) -- (8.4,3);
\draw[->] (6.6,-3) -- (8.4,-3);


\begin {scope}[shift={(1, 3)}]
%--- right strings Xi-1, Yj-1 -------------------------------------
% Xi-1 row
\node[left] at (8.2,1.2) {$X_{i-1}$};
\draw[fill=gray!30] (8.2,0.8) rectangle (11.2,1.6);

\draw (12,0.8) rectangle (12.8,1.6);
\node at (12.4,1.2) {$A$};

% cross
\draw[line width=0.6pt] (11.9,0.7) -- (12.9,1.7);
\draw[line width=0.6pt] (11.9,1.7) -- (12.9,0.7);
\node[right] at (13,1.2) {skip $x_i$};

% Yj-1 row
\node[left] at (8.2,-1.2) {$Y_{j}$};
\draw[fill=gray!30] (8.2,-1.6) rectangle (10.4,-0.8);
\draw[fill=gray!30] (10.4,-1.6) rectangle (11.2,-0.8);
\node at (10.8,-1.2) {$B$};
% vertical lcs arrow and label
\draw[<->] (8.5,0.8) -- (8.5,-0.8);
\node[right] at (8.5,0) {$\operatorname{lcs}(i-1,j)$};
\end{scope}

\begin {scope}[shift={(1, -3)}]
%--- right strings Xi-1, Yj-1 -------------------------------------
% Xi-1 row
\node[left] at (8.2,1.2) {$X_{i}$};
\draw[fill=gray!30] (8.2,0.8) rectangle (11.2,1.6);
\draw[fill=gray!30] (11.2,0.8) rectangle (12.0,1.6);
\node at (11.6,1.2) {$A$};

% Yj-1 row
\node[left] at (8.2,-1.2) {$Y_{j-1}$};
\draw[fill=gray!30] (8.2,-1.6) rectangle (10.6,-0.8);

\draw (12.0,-1.6) rectangle (12.8,-0.8);
\node at (12.4,-1.2) {$B$};

% cross
\draw[line width=0.6pt] (11.9,-1.7) -- (12.9,-0.7);
\draw[line width=0.6pt] (11.9,-0.7) -- (12.9,-1.7);
\node[right] at (13, -1.2) {skip $y_j$};


% vertical lcs arrow and label
\draw[<->] (8.5,0.8) -- (8.5,-0.8);
\node[right] at (8.5,0) {$\operatorname{lcs}(i,j-1)$};
\end{scope}


\end{tikzpicture}
\]





\textcolor{AccentBlue}{DP table.}
We store values in a table \(L[0\ldots m, 0\ldots n]\) where
\(L[i,j] = \operatorname{lcs}(i,j)\).
We can go recursive/top-down with memoization (\autoref{alg:lcs_topdown}) or iterative/bottom-up (\autoref{alg:lcs}).


\begin{algorithm}[h]
\caption{Longest Common Subsequence (top-down DP)} \label{alg:lcs_topdown}
\begin{algorithmic}[1]
\Function{LCS}{$i, j$}
  \If{\(L[i,j] = \nil\)}
    \If{\(i = 0 \OR j = 0\)} \Comment{base case}
      \State \(L[i,j] \gets 0\)
    \ElsIf{\(x_i = y_j\)} \Comment{last characters match}
      \State \(L[i,j] \gets \Call{LCS}{i-1, j-1} + 1\)
    \Else \Comment{last characters do not match}
      \State \(L[i,j] \gets \max\{\Call{LCS}{i-1, j}, \Call{LCS}{i, j-1}\}\)
    \EndIf
  \EndIf
  \State \Return \(L[i,j]\) \Comment{return stored value}
\EndFunction
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[h]
\caption{Longest Common Subsequence (bottom-up DP)}\label{alg:lcs}
\begin{algorithmic}[1]
\Require strings \(X = x_1\cdots x_m\), \(Y = y_1\cdots y_n\)
\State \(L \gets \text{new array \([0 \ldots m, 0 \ldots n]\)}\)
\For{$i = 0 \TO m$}
  \State \(L[i,0] \gets 0\)
\EndFor
\For{$j = 0 \TO n$}
  \State \(L[0,j] \gets 0\)
\EndFor
\For{$i = 1 \TO m$} \Comment{iterate over rows}
  \For{$j = 1 \TO n$} \Comment{iterate over columns}
    \If{$x_i = y_j$} 
      \State \(L[i,j] \gets L[i-1,j-1] + 1\) \Comment{take \(x_i = y_j\) for LCS}
    \Else
      \State \(L[i,j] \gets \max\{L[i-1,j], L[i,j-1]\}\)
    \EndIf
  \EndFor
\EndFor
\State \Return \(L[m,n]\)
\end{algorithmic}
\end{algorithm}




\textcolor{AccentBlue}{Running time} of \autoref{alg:lcs}.
The table has \((m+1)(n+1) = O(mn)\) entries, each filled in \(O(1)\) time,
so the running time is \(O(mn)\) and the space is \(O(mn)\).




\subsubsection{Reconstructing the actual subsequence}

To retrieve the actual subsequence, we can use another table, a ``hint'' table \(h[0\ldots m, 0\ldots n]\) that records which of the three cases was used to compute each \(L[i,j]\) (\autoref{alg:lcs_with_hints}).

\begin{algorithm}[h]
\caption{LCS (bottom-up) with hints}\label{alg:lcs_with_hints}
\begin{algorithmic}[1]
\Require strings \(X = x_1\cdots x_m\), \(Y = y_1\cdots y_n\)
\State \(L \gets \text{new array \([0 \ldots m, 0 \ldots n]\)}\) \Comment{stores lcs lengths}
\State \(h \gets \text{new array \([0 \ldots m, 0 \ldots n]\)}\) \Comment{hint table}
\For{$i = 0 \TO m$} \Comment{initialize column 0}
  \State \(L[i,0] \gets 0\)
  \State \(h[i,0] \gets \text{skipX}\)
\EndFor
\For{$j = 0 \TO n$} \Comment{initialize row 0}
  \State \(L[0,j] \gets 0\)
  \State \(h[0,j] \gets \text{skipY}\)
\EndFor
\For{$i = 1 \TO m$}
  \For{$j = 1 \TO n$}
    \If{$x_i = y_j$}
      \State \(L[i,j] \gets L[i-1,j-1] + 1\)
      \State \(h[i,j] \gets \text{addXY}\)
    \Else
      \If{$L[i-1,j] \ge L[i,j-1]$}
        \State \(L[i,j] \gets L[i-1,j]\)
        \State \(h[i,j] \gets \text{skipX}\)
      \Else
        \State \(L[i,j] \gets L[i,j-1]\)
        \State \(h[i,j] \gets \text{skipY}\)
      \EndIf
    \EndIf
  \EndFor
\EndFor
\State \Return \(L[m,n]\), \(h\)
\end{algorithmic}
\end{algorithm}


Now we can use the hints to reconstruct the LCS:
\begin{itemize}
  \item start at the last entry of the table, \([m,n]\)
  \item if \(h[i,j] = \text{addXY}\) (\(\nwarrow\)), then \(x_i = y_j\) is appended to LCS, continue with \([i-1,j-1]\)
  \item if \(h[i,j] = \text{skipX}\) (\(\uparrow\)), then \(x_i\) is not in LCS, continue with \([i-1,j]\)
  \item if \(h[i,j] = \text{skipY}\) (\(\leftarrow\)), then \(y_j\) is not in LCS, continue with \([i,j-1]\)
  \item stop when reaching \([0,0]\)
\end{itemize}

Because the characters are generated in revers order, prepend them to a sequence, so that when we are done, the sequence is in correct order.
Or use recursion to print them in correct order directly (\autoref{alg:print_lcs}).

\begin{algorithm}[h]
\caption{recursive LCS printing using hints}\label{alg:print_lcs}
\begin{algorithmic}[1]
\Function{Print-LCS}{$h, X, i, j$}
  \If{\(i = 0 \OR j = 0\)}
    \State \Return
  \EndIf
  \If{\(h[i,j] = \text{addXY}\)}
    \State \Call{Print-LCS}{$h, X, i-1, j-1$}
    \State {print} \(x_i\)
  \ElsIf{\(h[i,j] = \text{skipX}\)}
    \State \Call{Print-LCS}{$h, X, i-1, j$}
  \Else
    \State \Call{Print-LCS}{$h, X, i, j-1$}
  \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}





Alternatively, the hints can be discovered based on the filled \(L\) table (\autoref{alg:print_lcs_discovered}, \autoref{alg:print_lcs_discovered_alt}).
If \(x_i = y_j\) and \(L[i,j] = L[i-1,j-1]+1\), then \(x_i\) is part of some LCS and we move to \([i-1,j-1]\); 
otherwise, if \(L[i-1,j] \ge L[i,j-1]\) move to \([i-1,j]\), else move to \([i,j-1]\).


\begin{algorithm}[h]
\caption{recursive LCS printing discovering hints}\label{alg:print_lcs_discovered}
\begin{algorithmic}[1]
\Function{Print-LCS}{$L, X, Y, i, j$}
  \If{\(i = 0 \OR j = 0\)}
    \State \Return
  \EndIf
  \If{\(x_i = y_j \AND L[i,j] = L[i-1,j-1]+1\)}
    \State \Call{Print-LCS}{$L, X, Y, i-1, j-1$} \Comment{take \(x_i = y_j\)}
    \State {print} \(x_i\)
  \ElsIf{\(L[i-1,j] \ge L[i,j-1]\)}
    \State \Call{Print-LCS}{$L, X, Y, i-1, j$} \Comment{skip \(x_i\)}
  \Else
    \State \Call{Print-LCS}{$L, X, Y, i, j-1$} \Comment{skip \(y_j\)}
  \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[h]
\caption{recursive LCS printing discovering hints (alternative condition)}\label{alg:print_lcs_discovered_alt}
\begin{algorithmic}[1]
\Function{Print-LCS}{$L, X, i, j$}
  \If{\(i = 0 \OR j = 0\)}
    \State \Return
  \EndIf
  \If{\(L[i,j] = L[i-1,j]\)}
    \State \Call{Print-LCS}{$L, X, i-1, j$} \Comment{move up i.e. skip \(x_i\)}
  \ElsIf{\(L[i,j] = L[i,j-1]\)}
    \State \Call{Print-LCS}{$L, X, i, j-1$} \Comment{move left i.e. skip \(y_j\)}
  \Else \Comment{\(L[i,j]\) must be different from all 3 neighbors, so \(x_i = y_j\)}
    \State \Call{Print-LCS}{$L, X, i-1, j-1$} \Comment{move diagonally, take \(x_i = y_j\)}
    \State {print} \(x_i\)
  \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

 \autoref{alg:print_lcs}, \autoref{alg:print_lcs_discovered} and \autoref{alg:print_lcs_discovered_alt} are called with initial parameters \(i = m\) and \(j = n\).




\begin{remark}
Again, we can reduce the space to \(O(\min\{m,n\})\) if only the length of the LCS is needed by keeping only two rows/columns in memory at a time while ``filling the table''.
Because to compute \(L[i,j]\) we only need \(L[i-1,j-1]\), \(L[i-1,j]\), and \(L[i,j-1]\).
\end{remark}










% Sequence Alignment in Linear Space

% Combine Dynamic Programming with Divide and Conquer Sequence Alignment: Linear Space

% Q. Can we avoid using quadratic space?

% Easy: Optimal value in O(m + n) space and O(mn) time.

% – Use a 2xn matrix. Compute OPT(i, •) from OPT(i-1, •).

% – But no longer a simple way to recover the alignment itself.

% Theorem. [Hirschberg 1975]

% 

% 

% Optimal alignment in O(m + n) space and O(mn) time. Clever combination of divide-and-conquer and dynamic programming.

% Any DP matrix can be seen as a graph (a DAG)

% 

% Optimal alignment – shortest path in the DAG from node-(0,0) to (m,n)

% 87 Sequence Alignment: Linear Space

% Edit distance graph.

% 

% 

% 

% 

% A node for every M[i,j] An edge from M[i-1,j-1], M[i-1,j], M([i.j-1] to M[I,j] with weights

% Let f(i, j) be (length of) shortest path from (0,0) to (i, j). Observation: f(i, j) = M[i,j]= OPT(i, j).

% e

% y1 

% y2 

% y3 

% y4 

% y5 

% y6 

% e

% 0-0

% x1 

% α x i y j d

% x2 

% x3 

% d

% i-j

% €

% m-n

% 88 Sequence Alignment: Linear Space

% Edit distance graph.

% OPT(m,n): length of shortest path P from (0,0) to (m,n). OPT(i, j) = f(i,j);

% 

% 

% Let f(i, j) be (length of) shortest path from (0,0) to (i, j). Let g(i, j) be (length of) shortest path from (i, j) to (m,n).

% e

% y1 

% y2 

% y3 

% y4 

% y5 

% y6 

% e

% 0-0

% x1 

% α x i y j d

% x2 

% x3 

% d

% i-j

% €

% m-n

% 89 Sequence Alignment: Linear Space

% Edit distance graph.

% 

% 

% 

% Let f(i, j) be (length of) shortest path from (0,0) to (i, j). Let g(i, j) be (length of) shortest path from (i, j) to (m,n).

% Observation: OPT(m,n) = f(i, j) + g(i, j) for any node (i,j) on P, P= shortest path from (0,0) to (m,n).

% e

% y1 

% y2 

% y3 

% i-j

% y4 

% y5 

% y6 

% e

% 0-0

% x1 

% x2 

% x3 

% m-n

% 91 Computing f (•, j)

% 

% Given a column j, we can compute the value of f (•, j) = OPT[•, j] in O(mn) time and O(m + n) space.

% – Run space efficient sequence alignment to compute OPT(m,n) and keep the entries of column j in a separate array

% e

% y1 

% y2 

% y3 

% j y4 

% i-j

% y5 

% y6 

% e

% 0-0

% x1 

% x2 

% x3 

% m-n

% 94 Computing g(i, j)

% 

% We can compute g(i, j) by reversing the edge orientations (formula indices) and inverting the roles of (0, 0) and (m, n)

% – We start with g(m, n) = 0 and reverse indices in the recursive formula for OPT(i,j) = min{…}

% e

% y1 

% y2 

% y3 

% i-j

% y4 

% y5 

% y6 

% e

% 0-0

% x1 

% x2 

% x3 

% d

% αx i y j 

% d

% €

% m-n

% 96 Computing g(i, j)

% The backward function g(i,j)

%  g(m,n) = 0;

% 

% 

% 

% g(m,j) = (n-j)δ ; g(i,n)= (m-i)δ ;

% Substitute i-1 by i+1 in the recursive formula for OPT(i,j) Substitute j-1 by j+1 in the recursive formula for OPT(i,j)

% ! # #

% OPT(i, j) = "

% # # $

% jδ

% if

% i

% =

% 0

% ! α x i y j +OPT(i −1, j −1) # min "

% δ

% +OPT(i

% −1,

% j)

% # δ +OPT(i, j −1) $

% iδ

% otherwise

% if

% j=

% 0

% 97 Computing g(•, j)

% 

% 

% 

% Given a column j, we can compute the values of g (•, j) in O(mn) time and O(m + n) space.

% Run space efficient sequence alignment to compute OPT(0,0), (reversing the role of nodes (m,n) and (0,0) and reversing indices

% j Keep the entries of column j in a separate array) e y 1 y 2 y 3 y4 

% e

% 0-0

% x1 

% i-j

% x2 

% x3 

% y5 

% y6 

% m-n

% 98 Computing an alignment pair

% 

% 

% 

% Given a column j, compute f(•, j) + g (•, j) in a separate array Let q be the index of the minimum; then f(q.j) + g(q,j) = OPT(m,n)

% We have found one pair x q - y j (alignment x q - y j / xq -gap /yj - gap) in O(mn) time and O(m + n) space.

% e

% y1 

% y2 

% n/2 y3 

% i-j

% y4 

% y5 

% y6 

% e

% 0-0

% x1 

% x2 

% x3 

% q

% m-n

% 99 Sequence Alignment: Linear Space

% 

% 

% 

% 

% 

% Divide & Conquer: chose column j=n/2 Compute f(•,n/2). Compute g (•,n/2). Compute f(•, n/2) + g (•, n/2) Let q be an index that minimizes f(i,n/2) + g(i,n/2) Store the pair x q - y n/2 (possible alignment x q - y n/2 / xq -gap /yn/2 - gap) Recurse for X[1..q],Y[1../n/2] and X[q..m] ,Y[n/2…n]

% e

% y1 

% y2 

% y3 

% i-j

% y4 

% y5 

% y6 

% e

% 0-0

% x1 

% x2 

% x3 

% q

% m-n

% 102 Sequence Alignment: Linear Space

% Divide: Compute the value of f(i,n/2) and g(i,n/2), for all i, using two space-efficient DP algorithms (time O(mn), space O(m+n)).

% 

% Find the index q that minimizes f(i, n/2) + g(i, n/2).

% Store pair x q - y n/2 (align x q - y n/2 or xq -gap or yn/2 - gap). Conquer: recursively compute optimal alignment in blue and yellow n/2 pieces.

% 

% e

% y1 

% y2 

% y3 

% i-j

% y4 

% y5 

% y6 

% e

% 0-0

% x1 

% x2 

% x3 

% m-n

% q

% 103 an be easily avoided.

% Divide-and-Conquer-Alignment (X ,Y )

% Divide-and-Conquer-Alignment(X ,Y )

% Let m be the number of symbols in X Let n be the number of symbols in Y

% If m ≤ 2 or n ≤ 2 then

% Compute optimal alignment using Alignment(X ,Y ) Call Space-Efficient-Alignment(X ,Y[1 : n/2]) Call Backward-Space-Efficient-Alignment(X ,Y[n/2 + 1 : n]) Let q be the index minimizing f(q, n/2) + g(q, n/2) Add (q, n/2) to global list P Divide-and-Conquer-Alignment(X[1 : q],Y[1 : n/2]) Divide-and-Conquer-Alignment(X[q + 1 : n],Y[n/2 + 1 : n]) Return P

% At the end, given the global list P, retrieve the alignment

% 104 Sequence Alignment: Running Time Analysis Warmup

% How long does the algorithm take?

% T(m, n) = max running time on strings of length at most m and n

% We increase time by T(m, n) was O(mn). Now? only a constant

% T(m, n) = T(q,n / 2)+T(m − q,n / 2)+O(mn)

% factor. But we reuse space during recursive calls.

% At first glance one can think : T(m, n) = O(mn log n).

% T(m, n) ≤ 2T(m, n / 2) + O(mn) ⇒ T(m, n) = O(mn logn)

% But this analysis is not tight, T(m, n) = O(mn)

% 106 Sequence Alignment: Running Time Analysis

% Theorem. Let T(m, n) = max running time of algorithm on strings of length m and n. T(m, n) = O(mn).

% T(m, n) = T(q,n / 2)+T(m − q,n / 2)+O(mn)

% Pf. (by induction on n, skip.)

% 

% 

% 

% O(mn) time to compute f( •, n/2) and g( •, n/2) and find index q. T(q, n/2) + T(m - q, n/2) time for two recursive calls.

% Choose constant c so that:

% T(m, 2) T(2, n) T(m, n)

% ≤

% ≤

% ≤

% cm cn cmn + T(q, n/2) + T(m− q, n/2)

%  Base cases: m = 2 or n = 2.

% 

% Inductive hypothesis: T(k,l) £ 2ckl, k≤m , l<n.

% €

% T (m, n)

% ≤ ≤ = =

% T (q, n / 2) + T (m − q, n / 2) + cmn 2cqn / 2 + 2c(m − q )n / 2 + cmn cqn + cmn − cqn + cmn 2cmn

% 108 Longest Common subsequence

% You can do the same trick as with the sequence alignment on min cost (Combine DP and D&C )

% 

% 

% 

% 

% 

% Compute the longest common subsequence in time O(mn) using O(m+n) space Compute the value of LCS(i,n/2) and LCS-reverse(i,n/2), for every i, using two space-efficient DP algorithms.

% Find the index q that maximizes the appropriate combination of LCS(q,n/2) and LCS-reverse(q,n/2) Recursively compute the longest common subsequence in the two subproblems: one for X[1…q] Y[1…n/2] and one for X[q+1,…m] Y[n/2+1 …n]

% Concatenate the two subsequences found by the two recursive calls

\subsection{Sequence Alignment in linear space}\label{sec:seqalign_lcs_linear_space}
There is a cool trick that allows linear space without compromising the time complexity asymptotically.
It combines Dynamic Programming with Divide and Conquer.

Can we avoid using quadratic space?

\textcolor{AccentBlue}{easy:} \textcolor{AccentRed}{optimal value}: \(O(m + n)\) space and \(O(mn)\) time.
\begin{itemize}
  \item use a \(2 \times n\) matrix. compute \(\operatorname{OPT}(i, \cdot)\) from \(\operatorname{OPT}(i-1, \cdot)\)
  \item but no longer a simple way to recover the alignment itself
\end{itemize}

\begin{theorem}[Hirschberg 1975]\label{thm:hirschberg}
Optimal alignment in \(O(m + n)\) space and \(O(mn)\) time. Clever combination of divide-and-conquer and dynamic programming.
\end{theorem}

A DP matrix can be seen as a directed acyclic graph (DAG).
The optimal alignment corresponds to the shortest path in the DAG from node \((0,0)\) to \((m,n)\).

\textcolor{AccentBlue}{Edit distance graph}
\begin{itemize}
  \item a node for every entry \(M[i,j]\)
  \item an edge from \(M[i-1,j-1]\), \(M[i-1,j]\), \(M[i,j-1]\) to \(M[i,j]\) with weights according the \nameref{def:cost_model}
\end{itemize}

\[
\begin{tikzpicture}[
  scale=1.5,
  font=\footnotesize,
  dpnode/.style={circle, minimum size=3mm, inner sep=0pt, draw=gray!60, fill=gray!20},
  startend/.style={dpnode, minimum size=6mm, draw=black, fill=none},
  focus/.style={dpnode, minimum size=6mm, draw=black, fill=red!20},
  ed/.style={-Latex, draw=gray!55, line width=0.6pt},
  edD/.style={ed, densely dotted}, % dashed "collapsed" connections
  edF/.style={ed, color=black}
]

% --- nodes (create ALL nodes once, with correct style/label)
\foreach \r in {0,1,2,3,4,5}{
  \foreach \c in {0,1,2,3,4,5,6,7}{
    \def\nodestyle{dpnode}
    \def\nodelabel{}%

    % special nodes (same names!)
    \ifnum\r=0 \ifnum\c=0 \def\nodestyle{startend}\def\nodelabel{$0$-$0$}\fi\fi
    \ifnum\r=3 \ifnum\c=4 \def\nodestyle{focus}\def\nodelabel{$i$-$j$}\fi\fi
    \ifnum\r=5 \ifnum\c=7 \def\nodestyle{startend}\def\nodelabel{$m$-$n$}\fi\fi

    \node[\nodestyle] (n\r\c) at (\c,-\r) {\nodelabel};
  }
}

% ---- horizontal edges (skip n33 -> n34)
\foreach \r in {0,1,2,3,4,5}{
  \foreach \c in {0,1,2,3,4,5,6}{
    \pgfmathtruncatemacro{\cp}{\c+1}

    % choose style
    \def\estyle{ed}
    \ifnum\c=2 \def\estyle{edD}\fi
    \ifnum\c=4 \def\estyle{edD}\fi

    % skip only if (r,c)=(3,3)
    \ifnum\r=3
      \ifnum\c=3
        % skip
      \else
        \draw[\estyle] (n\r\c) -- (n\r\cp);
      \fi
    \else
      \draw[\estyle] (n\r\c) -- (n\r\cp);
    \fi
  }
}

% ---- vertical edges (skip n24 -> n34)
\foreach \r in {0,1,2,3,4}{
  \pgfmathtruncatemacro{\rp}{\r+1}
  \foreach \c in {0,1,2,3,4,5,6,7}{

    % choose style
    \def\estyle{ed}
    \ifnum\r=1 \def\estyle{edD}\fi
    \ifnum\r=3 \def\estyle{edD}\fi

    % skip only if (r,c)=(2,4)
    \ifnum\r=2
      \ifnum\c=4
        % skip
      \else
        \draw[\estyle] (n\r\c) -- (n\rp\c);
      \fi
    \else
      \draw[\estyle] (n\r\c) -- (n\rp\c);
    \fi
  }
}

% ---- diagonal edges (skip n23 -> n34)
\foreach \r in {0,1,2,3,4}{
  \pgfmathtruncatemacro{\rp}{\r+1}
  \foreach \c in {0,1,2,3,4,5,6}{
    \pgfmathtruncatemacro{\cp}{\c+1}

    % choose style
    \def\estyle{ed}
    \ifnum\r=1 \def\estyle{edD}\fi
    \ifnum\r=3 \def\estyle{edD}\fi
    \ifnum\c=2 \def\estyle{edD}\fi
    \ifnum\c=4 \def\estyle{edD}\fi

    % skip only if (r,c)=(2,3)
    \ifnum\r=2
      \ifnum\c=3
        % skip
      \else
        \draw[\estyle] (n\r\c) -- (n\rp\cp);
      \fi
    \else
      \draw[\estyle] (n\r\c) -- (n\rp\cp);
    \fi
  }
}

% ---- highlighted incoming edges into (i,j) = n34
\draw[edF] (n24) -- node[midway, right] {$\delta$} (n34);                 % (i-1,j)
\draw[edF] (n33) -- node[midway, above] {$\delta$} (n34);                 % (i,j-1)
\draw[edF] (n23) -- node[pos=0.42, above, inner sep=0pt, outer sep=0pt, anchor=south west] {$\alpha_{x_i y_j}$} (n34); % (i-1,j-1)


\node[above=2.5mm] at (n00) {$\varepsilon$};
\node[left=2.5mm]  at (n00)  {$\varepsilon$};

\node[above=2.5mm] at (n01) {$ y_1$};
\node[above=2.5mm] at (n02) {$ y_2$};
\node[above=2.5mm] at (n03) {$ y_{j-1}$};
\node[above=2.5mm] at (n04) {$ y_j$};
\node[above=2.5mm] at (n05) {$ y_{n-2}$};
\node[above=2.5mm] at (n06) {$ y_{n-1}$};
\node[above=2.5mm] at (n07) {$ y_n$};

\node[left=2.5mm] at (n10) {$ x_1$};
\node[left=2.5mm] at (n20) {$ x_{i-1}$};
\node[left=2.5mm] at (n30) {$ x_i$};
\node[left=2.5mm] at (n40) {$ x_{m-1}$};
\node[left=2.5mm] at (n50) {$ x_m$};

% ---- pünktchen on axes at the collapsed gaps
\node[above=2.5mm] at ($(n02)!0.5!(n03)$) {$\cdots$};
\node[above=2.5mm] at ($(n04)!0.5!(n05)$) {$\cdots$};

\node[left=2.5mm]  at ($(n10)!0.5!(n20)$) {$\vdots$};
\node[left=2.5mm]  at ($(n30)!0.5!(n40)$) {$\vdots$};

\end{tikzpicture}
\]


















\subsection{Shortest Paths with Negative Weights}

% \nameref{alg:bellmanford} algorithm is a dynamic-programming, label-correcting method that supports arbitrary (i.e. including negative) weights and detects negative-weight cycles.  
% It relaxes all edges in up to \( n-1\) passes for \(O( n\times m)\) time and uses one extra pass to check for cycles.

% The Bellman-Ford equation:
% \begin{equation}\label{eq:bellmanford}
% \delta(s, v) = \min_{(u,v)\in E} \left(\delta(s, u) + w(u, v)\right)
% \end{equation}

% \begin{algorithm}[h]
% \caption{Bellman-Ford}\label{alg:bellmanford}
% \begin{algorithmic}[1]
% \Function{BellmanFord}{$G, w:E\to\R, s$}
%   \ForAll{$v\in V(G)$}
%     \State $\attrib{v}{\delta}\gets\infty$
%     \State $\attrib{v}{\pi}\gets\nil$
%   \EndFor
%   \State $\attrib{s}{\delta}\gets 0$
%   \For{$i=1$ to $|V(G)|-1$} 
%     \ForAll{$(u,v)\in E(G)$}
%       \If{$\attrib{u}{\delta}+w(u,v)<\attrib{v}{\delta}$} 
%         \State $\attrib{v}{\delta}\gets \attrib{u}{\delta}+w(u,v)$ \Comment{applying Equation~\eqref{eq:bellmanford}}
%         \State $\attrib{v}{\pi}\gets u$
%       \EndIf
%     \EndFor
%   \EndFor
%   \ForAll{$(u,v)\in E(G)$}
%     \If{$\attrib{u}{\delta}+w(u,v)<\attrib{v}{\delta}$}
%       \State \Return \fals \Comment{negative-weight cycle detected}
%     \EndIf
%   \EndFor
%   \State \Return \tru
% \EndFunction
% \end{algorithmic}
% \end{algorithm}



For graphs with possibly negative edge weights (but no negative-weight cycles reachable from the source),
\nameref{alg:dijkstra}'s greedy algorithm no longer works.
The \nameref{alg:bellmanford} algorithm is a dynamic-programming, label-correcting method that works for arbitrary real edge weights and detects negative-weight cycles.


\textcolor{AccentBlue}{DP viewpoint.}
Let \(G = (V,E)\) be a directed graph with edge weights \(w:E\to\R\), and let \(s\) be the source.
Define
\(
\operatorname{OPT}(i,v)
\)
to be the \hl{length of the shortest \(s\)-\(v\) path that uses \emph{at most} \(i\) edges}.
\begin{itemize}
  \item Base cases (\(i=0\)):
  \(
  \operatorname{OPT}(0,s) = 0
  \), 
  \(
  \operatorname{OPT}(0,v) = \infty
  \) for \(v \neq s\)
  \item For \(i \ge 1\) and any \(v \in V\),
  \[
  \operatorname{OPT}(i,v)
  =
  \min\Bigl\{
    \operatorname{OPT}(i-1,v),\;
    \min_{(u,v)\in E}\bigl(\operatorname{OPT}(i-1,u) + w(u,v)\bigr)
  \Bigr\}
  \]
\end{itemize}

\hl[2]{Any simple path in a graph on \(n\) vertices has at most \(n - 1\) edges}.
If there is no negative-weight cycle reachable from \(s\), the shortest paths are simple,
so the true shortest-path distances are given by \(\operatorname{OPT}(n-1,v)\).


Instead of explicitly storing the 2D table \(\operatorname{OPT}(i,v)\),
Bellman-Ford iteratively relaxes all edges in the graph.

\begin{algorithm}[h]
\caption{Bellman-Ford}\label{alg:bellmanford}
\begin{algorithmic}[1]
\Function{BellmanFord}{$G,w:E\to\R,s$}
  \ForAll{$v \in V(G)$}
    \State \(\attrib{v}{\delta} \gets \infty\)
    \State \(\attrib{v}{\pi} \gets \nil\) \Comment{predecessor on shortest path tree}
  \EndFor
  \State \(\attrib{s}{\delta} \gets 0\)
  \For{$i = 1 \TO |V(G)| - 1$} \Comment{at most \(|V|-1\) edges on a simple path}
    \ForAll{$(u,v) \in E(G)$}
      \If{\(\attrib{u}{\delta} + w(u,v) < \attrib{v}{\delta}\)}
        \State \(\attrib{v}{\delta} \gets \attrib{u}{\delta} + w(u,v)\)
        \State \(\attrib{v}{\pi} \gets u\)
      \EndIf
    \EndFor
  \EndFor
  \ForAll{$(u,v) \in E(G)$}
    \If{\(\attrib{u}{\delta} + w(u,v) < \attrib{v}{\delta}\)}
      \State \Return \fals \Comment{negative-weight cycle detected}
    \EndIf
  \EndFor
  \State \Return \tru \Comment{distances \(\attrib{v}{\delta}\) are correct}
\EndFunction
\end{algorithmic}
\end{algorithm}

\textcolor{AccentBlue}{Running time.}
The algorithm performs \(O(|V|)\) passes, each relaxing all \(|E|\) edges once.
Thus the running time is \(O(|V|\cdot|E|)\) and the space usage is \(O(|V|)\).
The \(\attribute{\pi}\) pointers define a shortest-path tree rooted at \(s\).



