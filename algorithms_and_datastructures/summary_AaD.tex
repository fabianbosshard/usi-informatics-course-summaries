% !TEX root = summary_AaD.tex
\input{preamble}


\title{Algorithms \& Data Structures}
\author{Fabian Bosshard}
\date{\today}


\begin{document}

\maketitle

\tableofcontents

\section{Introduction}

$T(n) :=$ number of \textbf{basic steps} needed to compute the result of a problem of size $n$

\begin{example}
\label{ex:pingala}
How many $1,2$-beats can one compose over a total of $n$ beats?

Idea: the number of $1,2$-beats over $n$ beats is the sum of the number of $1,2$-beats over $n-1$ beats and the number of $1,2$-beats over $n-2$ beats
\def\TA{\raise0.5ex\hbox{\vrule width1pt depth0pt height1ex\vrule width2em depth0pt height0.5ex}}
\def\TAA{\raise1ex\hbox{\color{gray}\vrule width2pt depth0.5ex height0.5ex\vrule width4em depth0pt height0.5ex}}
\begin{center}
    \vspace{0em}
    \begin{tabular}[b]{l}
        \TA\hspace{1em}\TA\TA\TA\TA \\
        \TA\hspace{1em}\TA\TA\TAA \\
        \TA\hspace{1em}\TA\TAA\TA \\
        \TA\hspace{1em}\TAA\TA\TA \\
        \TA\hspace{1em}\TAA\TAA \\
    
        \TAA\hspace{1em}\TA\TA\TA \\
        \TAA\hspace{1em}\TA\TAA \\
        \TAA\hspace{1em}\TAA\TA \\
      \end{tabular}
\end{center}


\begin{algorithm}[htb] % wihtout [H], it is not possible to place the algorithm in the minipage
    \caption{Pingala}
    \begin{algorithmic}[1]
    \Function{Pingala}{$n$} \Comment{Number of $1,2$-beats in $n$ beats}
        \If{$n \leq 2$}
            \State \Return $n$
        \EndIf
        \State \Return \Call{Pingala}{$n-1$} + \Call{Pingala}{$n-2$}
    \EndFunction
    \end{algorithmic}
\end{algorithm}


% pseudocode:
% if n <= 2
%     return n
% return Pngala(n-1) + Pingala(n-2)
\vspace{-0.1em}
\[
T(1) = 1, \quad T(2) = 1, \quad T(n) = T(n-1) + T(n-2) + 2
\]
\[
 T(n) \geq \underbrace{T(n-1)}_{\geq T(n-2)} + T(n-2)  \geq 2T(n-2)
\]
\[
T(n) \geq 2T(n-2) \geq 2(2T(n-4)) = 2^2T(n-4) \geq \ldots \geq 2^{n/2}T(0) = \sqrt{2}^n T(0)  
\qedhere
\]
\end{example}


\section{Basics}
\begin{definition}
We define the following families of functions:
\begin{itemize}[leftmargin=*]
    \item $O(g(n)) := \{ f(n) \mid \exists c > 0, n_0 \in \mathbb{N} \mid0 \leq f(n) \leq cg(n) \text{ for all } n \geq n_0 \}$
    \item $\Omega(g(n)) := \{ f(n) \mid \exists c > 0, n_0 \in \mathbb{N} \mid 0 \leq cg(n) \leq f(n) \text{ for all } n \geq n_0 \}$
    \item $\Theta(g(n)) := \{ f(n) \mid \exists c_1, c_2 > 0, n_0 \in \mathbb{N} \mid 0 \leq c_1g(n) \leq f(n) \leq c_2g(n) \; \forall n \geq n_0 \}$
    \item $o(g(n)) := \{ f(n) \mid \forall c > 0, \exists n_0 \in \mathbb{N} \mid 0 \leq f(n) < cg(n) \text{ for all } n \geq n_0 \}$
    \item $\omega(g(n)) := \{ f(n) \mid \forall c > 0, \exists n_0 \in \mathbb{N} \mid 0 \leq cg(n) < f(n) \text{ for all } n \geq n_0 \}$ \qedhere
\end{itemize}
\end{definition}



The notation `$f(n) =$' is used to denote that $f(n)$ is an element of the set of functions on the right-hand side.
\begin{example}
    Let $\pi(n)$ be the number of primes less than or equal to $n$. Then 
    \[
    \pi(n) = \Theta\left(\frac{n}{\log n}\right)
    \text{.} \qedhere
    \]
\end{example}

The $\Theta$-notation, $\Omega$-notation, and $O$-notation can be viewed as the ``asymptotic'' $=$, $\geq$, and $\leq$ relations for functions.
The $o$-notation and $\omega$-notation can be viewed as asymptotic $<$ and $>$.



\begin{theorem}
    \label{thm:O-and-Omega-imply-Theta}
    \leavevmode\setlength{\abovedisplayskip}{0pt}\vspace{-\baselineskip}
    \[
    f(n) = \Omega(g(n)) \wedge f(n) = O(g(n)) \Leftrightarrow f(n) = \Theta(g(n))
    \qedhere
    \]
\end{theorem}
The above theorem can be interpreted as saying
\[
f \geq g \wedge f \leq g \Leftrightarrow f = g
\] 

\begin{example}
    \label{ex:logarithm-factorial}
    $\log(n!) \in \Theta(n \log n)$. We can rewrite $\log(n!)$ as
    \begin{equation}
    \label{eq:logarithm-factorial}
    \log(n!) = \log\left(\prod_{i=1}^n i\right) = \sum_{i=1}^n \log i
    \end{equation}

    Clearly, $\log(n!) \in O(n \log n)$, since
    \(
     n \log n = \log(n^n) = \sum_{i=1}^n \log n \geq \sum_{i=1}^n \log i 
    \).

    One way to understand why $\log(n!) \in \Omega(n \log n)$ is to interpret \eqref{eq:logarithm-factorial} as a sum of areas, each rectangle has width $1$ and height $\log i$:
    \begin{center}

    \begin{tikzpicture}[xscale=1, yscale=1.6]
        %--------------------------------------------------
        % change this to any positive integer
        \def\n{8}
        \def\m{4}
        %--------------------------------------------------
      
        % helpful macro for the top of the y–axis
        \pgfmathsetmacro{\Ymax}{ln(\n)+0.5}
      
        % axes -------------------------------------------------
        \draw[->] (0,0) -- (\n+2,0) node[right] {$x$};
        \draw[->] (0,0) -- (0,\Ymax)  node[right] {};
      
      
        % rectangles  -----------------------------------------
        \foreach \i in {1,...,\n}{
            \draw[fill=gray!40] (\i-1,0) rectangle  (\i,{ln(\i)});
        }
      
        %  y = ln x  -------------------------------------------
        \draw[very thick, green!0!black, domain=1:\n+1.2, samples=140]
              plot (\x,{ln(\x)});
        \node[green!0!black, right] at (\n+1.2, {ln(\n+1.2)}) {$\log x$};
      
        % %  y = ln(x+1)  ----------------------------------------
        % \draw[very thick, green!0!black, domain=0:\n+2, samples=140]
        %       plot (\x,{ln(\x+1)});
        % \node[green!0!black, above=3pt] at (\n+2, {ln(\n+2)}) {$\log(x+1)$};
      
        %  horizontal red lines + red dots  --------------------
        \foreach \i in {1,...,\n}{
            % dotted helper line
            \draw[red, dotted, line width=0.6pt] (0,{ln(\i)}) -- (\i,{ln(\i)});
            \ifnum\i<\numexpr\m+1\relax
            % label on the left
            \node[left=3pt, red] at (0,{ln(\i)}) {$\log \i$};
            \fi
            % red point on the curve
            % \fill[red] (\i,{ln(\i)}) circle (1);
            \node [draw, circle, red, fill=red, inner sep=0pt, minimum size=4pt] at (\i,{ln(\i)}) {};
        }
        \node[left=3pt, red, yshift=1ex] at (0,{(ln(\n)+ln(\m))/2}) {$\vdots$};
        \node[left=3pt, red] at (0,{ln(\n)}) {$\log n$};

        % x–ticks 0,1,2, …, n  -------------------------------
        \foreach \x in {0,...,\m}{
            \node[below=3pt] at (\x,0) {$\x$};
        }
        % \draw (\n,0) -- ++(0,.12) node[below=4pt] {$n$};
        \node[below=3pt] at ({(\n+\m)/2},0) {$\ldots$};
        \node[below=3pt] at (\n,0) {$n$};

        % label for the area of the gray rectangles
        \draw[thin, black, {Circle[open,scale=.8]}-] (\m+1.6,{ln(\m+0.5)/2}) to[out=0,in=180] (\n+1.2,{ln(\m-0.5)}) node[right] {$\displaystyle \log(n!)$};

      \end{tikzpicture}
      \end{center}

    We can see that the area of the gray rectangles is bounded from below by the area under the curve $y = \log x$, i.e.
    \begin{equation}
    \label{eq:logarithm-factorial-lowerbound}
    \int_1^n \log x \, \dif x \leq \log(n!)
    \end{equation}
    Using integration by parts, we can express the left-hand side of \eqref{eq:logarithm-factorial-lowerbound} as
    \begin{align*}
        \int_1^n \log x \, \dif x &= \int_1^n \underset{\uparrow}{1} \cdot \underset{\downarrow}{\log x} \, \dif x = \left[ x \log x - \int x \cdot \frac{1}{x} \, \dif x \right]_1^n = \left[ x \log x - x \right]_1^n \\
        &= n \log n - n + 1 \in \Omega(n \log n)
    \end{align*}
    Therefore, $\log(n!) \in \Omega(n \log n) \wedge \log(n!) \in O(n \log n)$.
    Using Theorem \ref{thm:O-and-Omega-imply-Theta}, we conclude that $\log(n!) \in \Theta(n \log n)$.
\end{example}

\begin{table}[htb]
    % sources: https://en.wikipedia.org/wiki/Big_O_notation, https://en.wikipedia.org/wiki/Time_complexity
    \centering
    \begin{tabularx}{\columnwidth}{%
        |>{\RaggedRight\arraybackslash}p{0.28\columnwidth}%
        |>{\RaggedRight\arraybackslash}p{0.20\columnwidth}%
        |>{\small\RaggedRight\arraybackslash}X| }
      \hline
      \textbf{Notation} 
        & \textbf{Name} 
        & {\normalsize\textbf{Example}} \\
        % change to \small here
      \hline
      $O(1)$
        & constant
        & Finding median of \emph{sorted} array; computing $(-1)^n$; using a fixed-size lookup table. \\
      \hline
      $O(\alpha(n))$
        & inverse Ackermann
        & Amortized cost per operation in a disjoint-set data structure. \\
      \hline
        $O(\log^* n)$
            & iterated logarithmic
            & Distributed coloring of cycles (Cole-Vishkin algorithm). \\
        \hline
      $O(\log\log n)$
        & double logarithmic
        & Interpolation search on uniformly distributed data. \\
      \hline
      $O(\log n)$
        & logarithmic
        & Binary search; operations in balanced trees or a binomial heap. \\
      \hline
      $O(\log^c n)$, $c>1$
        & polylogarithmic
        & Matrix-chain ordering on a PRAM. \\
      \hline
      $O(n^c)$, $0<c<1$
        & fractional power
        & Searching in a $k$-d tree. \\
      \hline
        $O(\frac{n}{\log n})$
            & & $\#\mathrm{Primes} \leq n$ (Example \ref{ex:pingala}). \\
        \hline
      $O(n)$
        & linear
        & Scanning an unsorted list; ripple-carry addition of two $n$-bit integers. \\
      \hline
      $O(n\,\log^* n)$
        &
        & Seidel's polygon-triangulation algorithm. \\
      \hline
      $O(n\log n)=O(\log n!)$
        & linearithmic
        & Fastest comparison sorts; Fast Fourier transform. \\
      \hline
      $O(n^2)$
        & quadratic
        & Schoolbook multiplication; bubble/selection/insertion sort; worst-case quicksort; direct convolution. \\
      \hline
        $O(n^3)$
            & cubic
            & Naive $n\times n$ matrix multiplication; partial correlation. \\
        \hline
      $O(n^c)$, $c>1$
        & polynomial
        & TAG parsing; bipartite matching; determinant via LU. \\
      \hline
        $L_{n}[\alpha, c]$
        & sub-exponential
        & Factoring via number-field sieve. \\
      \hline
      $O(c^n)$, $c>1$
        & exponential
        & Exact TSP by DP; brute-force logical equivalence checking. \\
      \hline
      $O(n!)$
        & factorial
        & Brute-force TSP; enumerating permutations or partitions; determinant by Laplace expansion. \\
        % switch back to \normalsize here
      \hline
    \end{tabularx}
  \end{table}

        
When $f(n) = O(g(n))$, we say that $g(n)$ is an \textbf{upper bound} for $f(n)$, and that $g(n)$ \textbf{dominates} $f(n)$.

When $f(n) = \Omega(g(n))$, we say that $g(n)$ is a \textbf{lower bound} for $f(n)$.

When $f(n) = \Theta(g(n))$, we say that $g(n)$ is a \textbf{tight bound} for $f(n)$.

We use the $o$-notation to denote an upper bound that is not asymptotically tight, and the $\omega$-notation to denote a lower bound that is not asymptotically tight.
The following two implications hold:
\[
f(n) = o(g(n)) \Rightarrow \lim_{n \to \infty} \frac{f(n)}{g(n)} = 0
\qquad \quad
f(n) = \omega(g(n)) \Rightarrow \lim_{n \to \infty} \frac{f(n)}{g(n)} = \infty
\]



\subsection{Incremental Algorithms}
\label{sec:incremental_algorithms}
\begin{example}[Hand of cards]
    \label{ex:insertion_sort}
    % Sorting a hand of cards using insertion sort:
    % \begin{center}
    % \includegraphics[width=0.3\columnwidth]{images/sorting_cards_woutBG.pdf}
    % \end{center}
    \hyperref[sec:insertion_sort]{Insertion sort} (Algorithm \ref{alg:insertion_sort}) uses an algorithm design technique called \textbf{incremental} method: for each element $A[i]$, it inserts it into its proper place in the subarray $A[1:i]$, having already sorted $A[1:i-1]$.
    This is reminiscent of how one might sort a hand of cards, where you pick up a card and insert it into the correct position in the already sorted hand.
    
    At the start of each iteration of the for loop, the subarray $A[1:i-1]$ consists of the elements originally in $A[1:i-1]$, but in sorted order.
    This is a \hyperref[sec:loop_invariant]{loop invariant} (Section \ref{sec:loop_invariant}).
\end{example}

\subsection{Loop Invariant}
\label{sec:loop_invariant}
When using a \textbf{loop invariante}, 3 things need to be shown:
\begin{enumerate}
\item \textbf{Initialization:} It is true prior to the first iteration of the loop.
\item \textbf{Maintenance:} If it is true before an iteration of the loop, it remains true before the next iteration.
\item \textbf{Termination:} When the loop terminates, the invariant gives a useful property that helps show that the algorithm is correct.
\end{enumerate}


A loop-invariant proof is a form of \textbf{mathematical induction}, where to prove that a property holds, you prove a base case and an inductive step.
Here, showing that the invariant holds before the first iteration corresponds to the base case, and showing that the invariant holds from iteration to iteration corresponds to the inductive step.
The third property is perhaps the most important one, since you are using the loop invariant to show correctness. 
Typically, you use the loop invariant along with the condition that caused the loop to terminate. 
Mathematical induction typically applies the inductive step infinitely, but in a loop invariant the ``induction'' stops when the loop terminates.



\subsection{Correctness}

You are given a problem $P$ and an algorithm $A$. 
$P$ formally defines a \textbf{correctness} condition.
Assume, for simplicity, that $A$ consists of one loop.
\begin{enumerate}
    \item Formulate an invariant $C$
    \item \textbf{Initialization}: prove that $C$ holds right before the first execution of the first instruction of the loop
    \item \textbf{Management}: prove that if $C$ holds right before the first instruction of the loop, then it holds also at the end of the loop
    \item \textbf{Termination}: prove that the loop terminates, with some exit condition $X$
    \item Prove that $X\wedge C \Rightarrow P$, which means that $A$ is correct
\end{enumerate}


\subsection{Divide-and-Conquer Algorithms}
\label{sec:divide_and_conquer}

Many useful algorithms are \textbf{recursive}:
they \textbf{recurse} (call themselves) one or more times to handle closely related subproblems.
These algorithms typically follow the \textbf{divide-and-conquer} method:
they break the problem into several subproblems that are similar to the original problem but smaller in size,
solve the subproblems recursively, and then combine these solutions to create a solution to the original problem.

In the divide-and-conquer method, if the problem is small enough (the \textbf{base case}), you just solve it directly without recursing.
Otherwise (the \textbf{recursive case}), you perform three characteristic steps:
\begin{enumerate}
    \item \textbf{Divide} the problem into one or more subproblems that are smaller instances of the same problem.
    \item \textbf{Conquer} the subproblems by solving them recursively.
    \item \textbf{Combine} the subproblem solutions to form a solution to the original problem.
\end{enumerate}


\begin{center}
    \begin{tikzpicture}[
        every node/.style={draw, rounded corners=2pt, align=center, minimum width=2.4cm, minimum height=1.0cm},
        arrow/.style={->, thick}
    ]
    
    %--- NODES ---
    % Top: Main Problem
    \node (main) {\footnotesize Main Problem};
    
    % Bottom: Final solution
    \node (final) [below =3.2cm of main] {\footnotesize Solution of\\\footnotesize Main Problem};
    
    \def\distance{3.4cm}
    % Compute relative positions for subproblems dynamically
    \node (sub1) at ($(main)!0.3!(final) + (-\distance,0)$) {\footnotesize Sub-Problem};
    \node (sub2) at ($(main)!0.3!(final) + (\distance,0)$) {\footnotesize Sub-Problem};

    % Compute relative positions for solutions of subproblems dynamically
    \node (sol1) at ($(main)!0.7!(final) + (-\distance,0)$) {\footnotesize Solution of\\\footnotesize Sub-Problem};
    \node (sol2) at ($(main)!0.7!(final) + (\distance,0)$) {\footnotesize Solution of\\\footnotesize Sub-Problem};
    
    %--- ARROWS ---
    % From main problem to sub-problems
    \draw[arrow] (main) -- (sub1);
    \draw[arrow] (main) -- (sub2);
    
    % From sub-problems to their solutions
    \draw[arrow] (sub1) -- node[midway, left, xshift=0.34cm, draw=none]
       {\tiny Solve\\[-5pt] \tiny Sub-Problem} (sol1);
    \draw[arrow] (sub2) -- node[midway, right, xshift=-0.34cm, draw=none]
       {\tiny Solve\\[-5pt] \tiny Sub-Problem} (sol2);
    
    % From solutions to final solution
    \draw[arrow] (sol1) -- (final);
    \draw[arrow] (sol2) -- (final);
    
    %--- BOLD TEXT IN THE MIDDLE ---
    % We place three labels along the vertical between 'main' and 'final'.
    \node[font=\bfseries, draw=none] at ($(main)!0.20!(final)$) {Divide};
    \node[font=\bfseries, draw=none] at ($(main)!0.50!(final)$) {Conquer};
    \node[font=\bfseries, draw=none] at ($(main)!0.80!(final)$) {Combine};
    
    \end{tikzpicture}
\end{center}


When an algorithm contains a recursive call, we can often describe its running time with a \textbf{recurrence relation}, which expresses the overall running time of a problem of size $n$ in terms of the running time of the same algorithm on smaller inputs.


% \begin{digression}[Master Theorem]
% \label{digression:master_theorem}
% Let $T(n)$ be the worst case running time on an input of size $n$.
% If the problem is small enough, say $n \leq n_0$, for some constant $n_0$, the straightforward solution takes constant time $\Theta(1)$.
% Suppose that the division of the problem yields $a$ subproblems, each of size $n/b$. 
% For \hyperref[sec:merge_sort]{merge sort} (Algorithm \ref{alg:merge_sort}), $a = b = 2$, but there are other divide-and-conquer algorithms in which $a \neq b$.
% If it takes $D(n)$ time to divide the problem into subproblems and $C(n)$ time to combine the solutions to the subproblems into the solution to the original problem, we get the recurrence
% \begin{align}
% \label{eq:recurrence_relation}
% T(n) &=
% \begin{cases}
%     \Theta(1) & \text{if } n \leq n_0 \\
%     D(n) + aT(n/b) + C(n) & \text{if } n > n_0
% \end{cases}
% \end{align}

% \begin{theorem}[Master Theorem]
% \label{thm:master_theorem}
% Let $a>0$ and $b>1$ be constants, and let $f(n)$ be a driving function that is defined and nonnegative on all sufficiently large reals. Define the recurrence $T(n)$ on $n \in \mathbb{N}$ by
% \[
% T(n)=a T(n / b)+f(n)
% \]
% where $a T(n / b)$ actually means $a^{\prime} T(\lfloor n / b\rfloor)+a^{\prime \prime} T(\lceil n / b\rceil)$ for some constants $a^{\prime} \geq 0$ and $a^{\prime \prime} \geq 0$ satisfying $a=a^{\prime}+a^{\prime \prime}$. Then the asymptotic behavior of $T(n)$ can be characterized as follows:
% \begin{enumerate}
%     \item If there exists a constant $\epsilon>0$ such that $f(n)=O\left(n^{\log _b a-\epsilon}\right)$, then $T(n)=$ $\Theta\left(n^{\log _b a}\right)$.
%     \item If there exists a constant $k \geq 0$ such that $f(n)=\Theta\left(n^{\log _b a} \lg ^k n\right)$, then $T(n)=$ $\Theta\left(n^{\log _b a} \lg ^{k+1} n\right)$.
%     \item If there exists a constant $\epsilon>0$ such that $f(n)=\Omega\left(n^{\log _b a+\epsilon}\right)$, and if $f(n)$ additionally satisfies the regularity condition $a f(n / b) \leq c f(n)$ for some constant $c<1$ and all sufficiently large $n$, then $T(n)=\Theta(f(n))$. \qedhere
% \end{enumerate}
% \end{theorem}
% \end{digression}

\subsection{Binary Search}
\label{subsec:binary_search}

% \begin{figure}[htb]
%   % image from: https://cpc-utec.github.io/blog/web/class-11.html
%   \centering
%   \begin{subfigure}[b]{0.49\columnwidth}
%     \includegraphics[width=\linewidth, height=0.5\linewidth]{images/linear_s.png}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[b]{0.49\columnwidth}
%     \includegraphics[width=\linewidth, height=0.5\linewidth]{images/binary_s.png}
%   \end{subfigure}
% \end{figure}


Algorithm~\ref{alg:binary_search} is an efficient method for finding an element \(x\) in a sorted array \(A\). 
By repeatedly halving the search interval, it reduces the problem size exponentially: at each step, it compares \(x\) to the middle element of the current interval and discards the half in which \(x\) cannot lie. 
This yields a worst-case running time of \(O(\log n)\), a dramatic improvement over a linear search's \(O(n)\) behavior.


\begin{algorithm}[htb]
    \caption{Binary Search}
    \label{alg:binary_search}
    \begin{algorithmic}[1]
        \Function{BinarySearch}{$A, x$}
            \State $l \gets 1$ \Comment{leftmost index}
            \State $r \gets \LEN(A)$ \Comment{rightmost index}
            \While{$l \leq r$}
                \State $m \gets \lfloor (l + r) / 2 \rfloor$ \Comment{midpoint of $A[l:r]$}
                \If{$A[m] < x$}
                    \State $l \gets m + 1$ \Comment{search in right half}
                \ElsIf{$A[m] > x$}
                    \State $r \gets m - 1$ \Comment{search in left half}
                \Else
                    \State \Return $m$ \Comment{found $x$ at index $m$}
                \EndIf
            \EndWhile
            \State \Return $0$ \Comment{not found: $0$ is not a valid index}
        \EndFunction
    \end{algorithmic}
\end{algorithm}


\section{Sorting}

\subsection{Insertion Sort}
\label{sec:insertion_sort}
Was already used to sort cards in Section \ref{sec:incremental_algorithms}, Example \ref{ex:insertion_sort}.
\begin{algorithm}[htb]
    \caption{Insertion Sort}
    \label{alg:insertion_sort}
    \begin{algorithmic}[1]
    \Function{InsertionSort}{$A$}
        \For{$i = 2 \TO \LEN(A)$}
            \State $j \gets i$
            \While{$j > 1 \AND A[j-1] > A[j]$}
                \State swap $A[j]$ and $A[j-1]$
                \State $j \gets j - 1$
            \EndWhile
        \EndFor
    \EndFunction
\end{algorithmic}
\end{algorithm}


\subsection{Merge Sort}
\label{sec:merge_sort}


\begin{algorithm}[htb]
    %\scriptsize
    \caption{Merge Sort}
    \label{alg:merge_sort}
    \begin{algorithmic}[1]
\Function{MergeSort}{$A$}
    \If{$\operatorname{len}(A) \leq 1$} \Comment{base case (array is trivially sorted)}
        \State \Return $A$
    \EndIf
    \State $m = \lfloor \operatorname{len}(A) / 2 \rfloor$ \Comment{midpoint of $A$} \label{alg:merge_sort:midpoint}
    \State $A_L \gets$ \Call{MergeSort}{$A[1:m]$} \Comment{recursively sort $A[1:m]$} \label{alg:merge_sort:recurse_left}
    \State $A_R \gets$ \Call{MergeSort}{$A[m\!+\!1:|A|]$} \Comment{recursively sort $A[m\!+\!1:|A|]$} \label{alg:merge_sort:recurse_right}
    \State \Return \Call{Merge}{$A_L, A_R$} \Comment{merge the two sorted arrays}
\EndFunction
    \end{algorithmic}
\end{algorithm}


\begin{algorithm}[htb]
    %\scriptsize
    \caption{Merge}
    \label{alg:merge}
    \begin{algorithmic}[1]
\Function{Merge}{$A, B$}
    \State $i,j \gets 1$
    \State $C \gets []$
    \While{$i \leq \LEN(A) \OR j \leq \LEN(B)$}
        \If{$i \leq \LEN(A) \AND (j > \LEN(B) \OR A[i] < B[j])$}
            \State append $A[i]$ to $C$
            \State $i \gets i + 1$
        \Else
            \State append $B[j]$ to $C$
            \State $j \gets j + 1$
        \EndIf
    \EndWhile
    \State \Return $C$
\EndFunction
\end{algorithmic}
\end{algorithm}


We describe the running time of merge sort (Algorithm \ref{alg:merge_sort}) as follows:
\begin{enumerate}
    \item \textbf{Divide:} compute the middle of the array (Algorithm \ref{alg:merge_sort}, Line \ref{alg:merge_sort:midpoint}), which takes constant time, $D(n) = \Theta(1)$
    \item \textbf{Conquer:} recursively solve two subproblems (Algorithm \ref{alg:merge_sort}, Line \ref{alg:merge_sort:recurse_left}, \ref{alg:merge_sort:recurse_right}), each of size $n/2$, contributes $2T(n/2)$ %to the total running time (ignoring the floors and ceilings)
    \item \textbf{Combine:} merge (Algorithm \ref{alg:merge}) the two sorted subarrays, which takes $\Theta(n)$ time, $C(n) = \Theta(n)$
\end{enumerate}

Using the so called `master theorem':
\[
T(n) = 2T(n/2) + \Theta(n) \xRightarrow{\text{\hyperref[thm:master_theorem]{master theorem}}} T(n) = \Theta(n \log_2 n)
\]


Intuitively we can also understand why that is the case without the master theorem. Assume for simplicity that $n$ is an exact power of 2 and that the implicit base case is $n = 1$:
\begin{align*}
    T(n) &= 
    \begin{cases}
        c_1 & \text{if } n = 1 \\
        2T(n/2) + c_2n & \text{if } n > 1
    \end{cases}
\end{align*}
where $c_1 > 0$ represents the time to solve the base case ($n = 1$) and $c_2 > 0$ is the time per element of the divide and combine steps.
\begin{center}
\begin{tikzpicture}[
        %>=stealth,
        level distance=1.2cm,
        sibling distance=2.5cm,
        every node/.style={draw=none},
        font=\small,
        scale=0.85,
    ]
    
    %--- Top (root) node ---
    \node (top) at (0,0) {\(c_2 \cdot n\)};
    
    %--- Second level (2 children) ---
    \node (l1) at (-2,-1) {\(c_2 \cdot n/2\)};
    \node (r1) at ( 2,-1) {\(c_2 \cdot n/2\)};
    \draw (top) -- (l1);
    \draw (top) -- (r1);
    
    %--- Third level (4 children) ---
    \node (l2) at (-3,-2) {\(c_2 \cdot n/4\)};
    \node (l3) at (-1,-2) {\(c_2 \cdot n/4\)};
    \node (r2) at ( 1,-2) {\(c_2 \cdot n/4\)};
    \node (r3) at ( 3,-2) {\(c_2 \cdot n/4\)};
    \draw (l1) -- (l2);
    \draw (l1) -- (l3);
    \draw (r1) -- (r2);
    \draw (r1) -- (r3);
    
    %--- Dotted lines indicating more subdivision below ---
    \draw (l2) -- ($(l2)!0.7!($(l2)+(-0.5,-0.75)$)$); \draw[dotted] ($(l2)!0.7!($(l2)+(-0.5,-0.75)$)$) -- ($(l2)+(-0.5,-0.75)$);
    \draw (l2) -- ($(l2)!0.7!($(l2)+(0.5,-0.75)$)$); \draw[dotted] ($(l2)!0.7!($(l2)+(0.5,-0.75)$)$) -- ($(l2)+(0.5,-0.75)$);
    \draw (l3) -- ($(l3)!0.7!($(l3)+(-0.5,-0.75)$)$); \draw[dotted] ($(l3)!0.7!($(l3)+(-0.5,-0.75)$)$) -- ($(l3)+(-0.5,-0.75)$);
    \draw (l3) -- ($(l3)!0.7!($(l3)+(0.5,-0.75)$)$); \draw[dotted] ($(l3)!0.7!($(l3)+(0.5,-0.75)$)$) -- ($(l3)+(0.5,-0.75)$);
    \draw (r2) -- ($(r2)!0.7!($(r2)+(-0.5,-0.75)$)$); \draw[dotted] ($(r2)!0.7!($(r2)+(-0.5,-0.75)$)$) -- ($(r2)+(-0.5,-0.75)$);
    \draw (r2) -- ($(r2)!0.7!($(r2)+(0.5,-0.75)$)$); \draw[dotted] ($(r2)!0.7!($(r2)+(0.5,-0.75)$)$) -- ($(r2)+(0.5,-0.75)$);
    \draw (r3) -- ($(r3)!0.7!($(r3)+(-0.5,-0.75)$)$); \draw[dotted] ($(r3)!0.7!($(r3)+(-0.5,-0.75)$)$) -- ($(r3)+(-0.5,-0.75)$);
    \draw (r3) -- ($(r3)!0.7!($(r3)+(0.5,-0.75)$)$); \draw[dotted] ($(r3)!0.7!($(r3)+(0.5,-0.75)$)$) -- ($(r3)+(0.5,-0.75)$);
    
    
    %--- Bottom row (n leaves, each with cost c1) ---
    \node at (-3.75,-3.5) {\(c_1\)};
    \node at (-3.25,-3.5) {\(c_1\)};
    \node at (-2.75,-3.5) {\(c_1\)};
    \node at (-2.25,-3.5) {\(\dots\)};
    % ...
    \node at ( 2.25,-3.5) {\(\dots\)};
    \node at ( 2.75,-3.5) {\(c_1\)};
    \node at ( 3.25,-3.5) {\(c_1\)};
    \node at ( 3.75,-3.5) {\(c_1\)};
    
    %--- Brace indicating n leaves ---
    \draw[decorate, decoration={brace, amplitude=5pt, mirror}, thick]
      (-4,-4) -- (4,-4)
      node[midway, below=6pt] {\(n\)};
    
    %--- Vertical arrow for log n + 1---
    \draw[<->, thick] (-4.5,0) -- (-4.5,-3.5)
      node[midway, left] {\(\log_2 n + 1\)};
    
    %--- Dotted arrows on the right showing cost per level = c2 * n ---
    \draw[dotted, ->, thick] (1, 0) -- (6.5, 0)   node[right] {\(c_2 \cdot n\)};
    \draw[dotted, ->, thick] (3,-1) -- (6.5,-1)  node[right] {\(c_2 \cdot n\)};
    \draw[dotted, ->, thick] (4,-2) -- (6.5,-2)  node[right] {\(c_2 \cdot n\)};
    \node[right] at (6.5, -2.75) {$\vdots$};
    \draw[dotted, ->, thick] (4.55,-3.5) -- (6.5,-3.5) node[right] {\(c_1 \cdot n\)};
    
    \end{tikzpicture}
\end{center}
\begin{align*}
    T(n) &= 2T(n/2) + c_2n \quad = 2(2T((n/2)/2) + c_2(n/2)) + c_2n \\
    &= 2^2T(n/2^2) + 2c_2n \quad = 2^2(2T((n/2^2)/2) + c_2(n/2^2)) + 2c_2n \\
    &= 2^3T(n/2^3) + 3c_2n \quad = 2^3(2T((n/2^3)/2) + c_2(n/2^3)) + 3c_2n \\
    & \vdots \\
    &= 2^{\log_2(n)}T(n/2^{\log_2(n)}) + \log_2(n)c_2n \\
    &= n T(1) + \log_2(n)c_2n \\
    &= c_1 n + c_2 n \log_2n \\
    &= \Theta(n \log n)
\end{align*}





\subsection{Quick Sort (with Lomuto Partitioning)}

\definecolor{mygruen}{rgb}{0.95,0.95,0.95}
\definecolor{myblue}{rgb}{0.2,0.8,1.0}         % previously dunkelgruen
\definecolor{myorange}{rgb}{1,0.65,0}      % previously hellgruen

\newcommand{\mygruen}{\color{mygruen}}
\newcommand{\myblue}{\color{myblue}}
\newcommand{\myorange}{\color{myorange}}

\begin{figure}[htb]
    \centering
    \begin{tikzpicture}[scale = 1.2]
      \draw[fill=myorange!30] (0,0) rectangle (2, 0.5);
      \draw[fill=myblue!30] (2,0) rectangle (5, 0.5);
      \draw[fill=myorange!30] (7.5,0) rectangle (8, 0.5);
      \fill[fill=myorange!30] (5,0) rectangle (7.5, 0.25);
      \fill[fill=myblue!30] (5,0.25) rectangle (7.5, 0.5);
      \draw (7.5, 0) -- (7.5, 0.5);
      \draw (2, 0) -- (2, 0.5);
      \draw (5, 0) -- (5, 0.5);
      \node at (7.75, 0.25) {$v$};
      \node at (1, 0.25) {$\leq v$};
      \node at (3.5, 0.25) {$> v$};
      \node at (6.25, 0.25) {? $\cdots$ ?};
      % \node at (1.75, -0.25) {\small$\uparrow$};
      % \node at (1.75, -0.65) {${i}$};
      % \node at (5.25, -0.25) {\small$\uparrow$};
      % \node at (5.25, -0.65) {${j}$};
      % \node at (0.25, -0.25) {\small$\uparrow$};
      % \node at (0.25, -0.65) {${p}$};
      % \node at (7.75, -0.25) {\small$\uparrow$};
      % \node at (7.75, -0.65) {${r}$};
      \node at (1.75, 0.75) {${i}$};
      \node at (5.25, 0.75) {${j}$};
      \node at (0.25, 0.75) {${l}$};
      \node at (7.75, 0.75) {${r}$};
      
      \draw[thick] (0,0) rectangle (8, 0.5);
        %\draw [step = .25, blue, opacity=0.3] (0, -.5) grid (\columnwidth, 0.5);
    \end{tikzpicture}
    % \vspace*{-2.5em}
    % \caption{Lomuto invariant: $A[1..\texttt{i} - 1]$ consists of elements smaller than $p$, 
    % $A[\texttt{i}..\texttt{j}-1]$ consists of elements at least as large as $p$; $A[\texttt{j}..\textit{n}-1]$ has not been looked at, which is depicted by filling this part of the array with both colors.} 
    \label{fig:lomuto:invariant}
\end{figure}

Lomuto partitioning (Algorithm \ref{alg:lomuto_partition}) divides an array $A$ into two subarrays $A[l:q-1]$ and $A[q+1:r]$ such that all elements in $A[l:q-1]$ are less than or equal to $A[q]$ and all elements in $A[q+1:r]$ are greater than $A[q]$.

\begin{algorithm}[h!]
    \caption{Quick Sort}
    \label{alg:quick_sort}
\begin{algorithmic}[1]
    \Function{QuickSort}{$A, l, r$}
        \If{$l < r$}
            \State $q = \Call{Partition}{A, l, r}$
            \State \Call{QuickSort}{$A, l, q - 1$}
            \State \Call{QuickSort}{$A, q + 1, r$}
        \EndIf
    \EndFunction
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[h!]
    \caption{Lomuto Partitioning}
    \label{alg:lomuto_partition}
\begin{algorithmic}[1]
    \Function{Partition}{$A, l, r$}
        \State $v = A[r]$ \Comment{pick last element as pivot}
        \State $i = l - 1$ \Comment{highest index into the less-than-or-equal--partition}
        \For{$j = l \TO r$}
            \If{$A[j] \leq v$}
                \State $i = i + 1$
                \State swap $A[i]$ and $A[j]$
            \EndIf
        \EndFor
        \State \Return $i$ \Comment{index of pivot}
    \EndFunction
\end{algorithmic}
\end{algorithm}
    




\subsection{Heap Sort}
\label{subsec:heap_sort}
\begin{definition}[Heap]
A \emph{binary heap} is a nearly-complete binary tree stored in an array \(A[1:n]\) satisfying the \emph{max-heap property}:
\[
\forall i>1:\quad A[\mathrm{Parent}(i)] \ge A[i]
\]
The relationship between the indices of a binary heap is as follows:
\[
\mathrm{Parent}(i)=\lfloor i/2\rfloor \qquad
\mathrm{Left}(i)=2i \qquad
\mathrm{Right}(i)=2i+1 \qedhere
\]
\end{definition}
Furthermore, an $n$-element heap has height $h=\lfloor\log_2 n\rfloor$ and at most $\lceil n/2^{\tilde{h}+1}\rceil$ nodes at any given height $\tilde{h}$, where $\tilde{h}$ is defined as the longest path from the current node to a leaf node, measured in number of edges.


\definecolor{arraycolor}{rgb}{0.70,0.99,0.70}
\newcommand{\arraycolor}{\color{arraycolor}}


\begin{center}
\begin{tikzpicture}[
  arr/.style={->},
  level distance=1.2cm,
  sibling distance=2.5cm,
  % -------- main tree nodes ---------
  every node/.style={
    draw,circle,
    minimum size = 1.3em,
    inner sep    = 0pt
  },
  % -------- label nodes -------------
  every label/.style={
    draw=none,          % no outline
    shape=rectangle,    % *not* a circle
    inner sep=0pt       % tight around the text
  },
  font=\footnotesize,
  scale=0.9,
]
%--- Top (root) node ---
\node[fill=arraycolor] (top) at (0,1) [label=below:{$1$}] {};
\node[fill=arraycolor] (l1)  at (-2,-1) [label=below:{$2$}] {};
\node[fill=arraycolor] (r1)  at ( 2,-1) [label=below:{$3$}] {};
% arrows between levels
\draw[arr] (top) -- (l1);
\draw[arr] (top) -- (r1);

%--- Third level (4 children) ---
\node[fill=arraycolor] (l2) at (-3,-2) [label=below:{$4$}] {};
\node[fill=arraycolor] (l3) at (-1,-2) [label=below:{$5$}] {};
\node[fill=arraycolor] (r2) at ( 1,-2) [label=below:{$6$}] {};
\node[fill=arraycolor] (r3) at ( 3,-2) [label=below:{$7$}] {};
\draw[arr] (l1) -- (l2);
\draw[arr] (l1) -- (l3);
\draw[arr] (r1) -- (r2);
\draw[arr] (r1) -- (r3);

%--- dots in between ---
\node[draw=none] at (-3,-3) {$\dots$};
\node[draw=none] at (-1,-3) {$\dots$};
\node[draw=none] at (1,-3) {$\dots$};
\node[draw=none] at (3,-3) {$\dots$};

%--- Dotted arrows indicating more subdivision below ---
\newcommand{\splitratio}{0.6}
\newcommand{\subdx}{0.5}
\newcommand{\subdy}{0.5}
\foreach \nodecoord in {l2,l3,r2,r3} {
  \draw[] (\nodecoord) -- ($ (\nodecoord) !\splitratio! ($(\nodecoord)+(-\subdx,-\subdy)$) $); % left edge
  \draw[dotted] ($ (\nodecoord) !\splitratio! ($(\nodecoord)+(-\subdx,-\subdy)$) $ ) -- ($(\nodecoord)+(-\subdx,-\subdy)$); % continuation of left edge as dotted line
  \draw[] (\nodecoord) -- ($ (\nodecoord) !\splitratio! ($(\nodecoord)+(\subdx,-\subdy)$) $); % right edge
  \draw[dotted] ($ (\nodecoord) !\splitratio! ($(\nodecoord)+(\subdx,-\subdy)$) $ ) -- ($(\nodecoord)+(\subdx,-\subdy)$); % continuation of right edge as dotted line
}

%--- Bottom two rows ---
\node (2_power_hminus2) at (-4,-3) [draw=none] {};
% \node (2_power_hminus1) at (-4.75,-3.75) [label={[yshift=-0pt]below:{$2^{h \scalebox{0.5}[1]{$-$} 1}$}}] {}; % this label needs to be shifted down...
\node[fill=arraycolor] (2_power_hminus1) at (-4.75,-3.75) [label=below:{$2^{h\!-\!1}$}] {};
\node[fill=arraycolor] (2_power_h) at (-5.5,-4.5) [label=below:{$2^h$}] {};
\node[fill=arraycolor] (2powerh_plus_1) at (-4,-4.5) [label=below:{$2^h\!+\!1$}] {};

\draw[dotted] (2_power_hminus2) -- ($(2_power_hminus2)!0.5!(2_power_hminus1)$);
\draw[arr] ($(2_power_hminus2)!0.5!(2_power_hminus1)$) -- (2_power_hminus1);
\draw[arr] (2_power_hminus1) -- (2_power_h);
\draw[arr] (2_power_hminus1) -- (2powerh_plus_1);

\node[draw=none] at (-3.75,-3.75) {$\dots$};
\node[draw=none] at (-2.25,-3.75) {$\dots$};
\node[draw=none] at (-3,-4.5) {$\dots$};


\node[fill=arraycolor] (heapsize_minus1) at (-2, -4.5) [label=below:{$n\!-\!1$}] {};
\node[fill=arraycolor] (heapsize) at (-0.5, -4.5) [label=below:{$n$}] {};
\node[fill=arraycolor] (heapsize_parent) at (-1.25, - 3.75) [label={[yshift=-0pt]below:{$\left\lfloor \! \frac{n}{2} \! \right\rfloor$}}] {};
\node (heapsize_pparent) at (-2, -3) [draw=none] {};

\draw[dotted] (heapsize_pparent) -- ($(heapsize_pparent)!0.5!(heapsize_parent)$);
\draw[arr] ($(heapsize_pparent)!0.5!(heapsize_parent)$) -- (heapsize_parent);
\draw[arr] (heapsize_parent) -- (heapsize);
\draw[arr] (heapsize_parent) -- (heapsize_minus1);

\node[draw=none] at (-0.25,-3.75) {$\dots$};
\node[draw=none] at (0.75,-3.75) {$\dots$};
\node[draw=none] at (1.75,-3.75) {$\dots$};
\node[draw=none] at (2.75,-3.75) {$\dots$};
\node[draw=none] at (3.75,-3.75) {$\dots$};
\node[fill=arraycolor] (2powerh_minus_1) at (4.75,-3.75) [label=below:{$2^h\!-\!1$}] {};
\node (2powerh_minus_1_parent) at (4,-3) [draw=none] {};
\draw[dotted] (2powerh_minus_1_parent) -- ($(2powerh_minus_1_parent)!0.5!(2powerh_minus_1)$);
\draw[arr] ($(2powerh_minus_1_parent)!0.5!(2powerh_minus_1)$) -- (2powerh_minus_1);

% \node (heapsize_parent_plus_1) at (0.25, -3.75) [label=below:{$\left\lfloor \! \frac{n}{2} \! \right\rfloor \! + \! 1$}] {};
% \node (heapsize_parent_plus_1_parent) at (1, -3) [draw=none] {};
% \draw[dotted] (heapsize_parent_plus_1_parent) -- ($(heapsize_parent_plus_1_parent)!0.5!(heapsize_parent_plus_1)$);
% \draw[arr] ($(heapsize_parent_plus_1_parent)!0.5!(heapsize_parent_plus_1)$) -- (heapsize_parent_plus_1);

% %--- \tilde{h} on left side ---
% \newcommand{\linestart}{-5.5}
% \newcommand{\labelshift}{-3.5em}
% \draw[dashed, thin, draw=gray] (-0.5, 1) -- (\linestart, 1)node[left, draw=none, outer sep=5ex] {\(\tilde{h} = h\)};
% \draw[dashed, thin, draw=gray] (\linestart, -1) node[right, draw=none, outer sep=5ex, xshift=\labelshift] {\(\tilde{h} = h\!-\!1\)} -- (-2.5, -1);
% \draw[dashed, thin, draw=gray] (\linestart, -2) node[right, draw=none, outer sep=5ex, xshift=\labelshift] {\(\tilde{h} = h\!-\!2\)} -- (-3.5, -2);
% \draw[draw=none] (\linestart, -2.75) node[right, draw=none, outer sep=5ex, xshift=\labelshift] {\(\vdots\)} -- (-4.25, -2.75);
% \draw[draw=none] (\linestart, -3.75) node[right, draw=none, outer sep=5ex, xshift=\labelshift] {\(\tilde{h} = 1\)} -- (-5.25, -3.75);
% \draw[draw=none] (\linestart, -4.5) node[right, draw=none, outer sep=5ex, xshift=\labelshift] {\(\tilde{h} = 0\)} -- (-6, -4.5);

%--- \tilde{h} on left side via east‐anchor + x‐shift trick ---
\newcommand{\linestart}{-6.8}
\node[draw=none,anchor=west,inner sep=0] (lbl0) at (\linestart,1) {$h = \lfloor \log_2 n \rfloor$};
\draw[dashed,thin,gray] ($(lbl0.east)+(0.8em,0)$) -- (-0.5,1);
\node[draw=none,anchor=west,inner sep=0] (lbl1) at (\linestart,-1) {$\tilde h = h\!-\!1$};
\draw[dashed,thin,gray] ($(lbl1.east)+(0.8em,0)$) -- (-2.5,-1);
\node[draw=none,anchor=west,inner sep=0] (lbl2) at (\linestart,-2) {$\tilde h = h\!-\!2$};
\draw[dashed,thin,gray] ($(lbl2.east)+(0.8em,0)$) -- (-3.5,-2);
\node[draw=none,anchor=west,inner sep=0] (lbldots) at (\linestart,-2.75) {$\vdots$};
\node[draw=none,anchor=west,inner sep=0] (lbl3) at (\linestart,-3.75) {$\tilde h = 1$};
% \draw[dashed,thin,gray] ($(lbl3.east)+(0.8em,0)$) -- (-5.25,-3.75);
\node[draw=none,anchor=west,inner sep=0] (lbl4) at (\linestart,-4.5) {$\tilde h = 0$};
% \draw[dashed,thin,gray] ($(lbl4.east)+(0.8em,0)$) -- (-6,-4.5);


%--- Vertical arrow and vertical (90 degree rotated) label log n on right side ---
\draw[|-|] (6,1) -- (6,-4.5)
node[draw=none, midway, xshift=-0.8em](labelposition) {};
\node[draw=none, rotate=-90] at (labelposition) {$h=\lfloor \log n \rfloor$};

\end{tikzpicture}
\end{center}




\begin{center}

\begin{tikzpicture}[scale=1.2,every node/.style={font=\footnotesize}]
  % -----------------------------------------------------------
  % generic cell helper:  \heapcell{<label>}{<index-in-row>}{<node-name>}
  % -----------------------------------------------------------
  \def\cw{0.5} % cell width
  \newcommand{\heapcell}[3]{%
      \pgfmathsetmacro\x{#2*\cw}%
      \draw[fill=arraycolor] (\x,0) rectangle ++(\cw,0.5);               % rectangle
      \node at (\x+0.5*\cw,0.7) {\footnotesize$#1$};           % index label
      \coordinate (#3) at (\x+0.5*\cw,0);               % anchor for arrows
  }
  
  % array structure of heap
  \heapcell{1}{0}{p1}
  \heapcell{2}{1}{p2}
  \heapcell{3}{2}{p3}
  \heapcell{4}{3}{p4}
  \heapcell{5}{4}{p5}
  \heapcell{6}{5}{p6}
  \heapcell{7}{6}{p7}
  \node at (7.75*\cw,0.25) {$\dots$};
  \heapcell{2^{h\!-\!1}}{8.5}{phm}
  \node at (10.25*\cw,0.25) {$\dots$};
  \heapcell{\left\lfloor \! \frac{n}{2} \! \right\rfloor}{11}{pnpar}
  \node at (12.75*\cw,0.25) {$\dots$};
  \heapcell{2^h\!-\!1}{13.5}{p2h}            
  \heapcell{2^h}{14.5}{plh}             % left  child of 2^{h-1}
  \heapcell{2^h\!+\!1}{15.5}{prh}           % right child of 2^{h-1}
  \node at (17.25*\cw,0.25) {$\dots$};
  \heapcell{n\!-\!1}{18}{pnmone}          % left  child of floor(n/2)
  \heapcell{n}{19}{pn}                % right child of floor(n/2)

   % array rectangle bounding box (needs to be drawn last)
  \draw[thick] (0,0) rectangle ++(20*\cw,0.5);
  
  % -----------------------------------------------------------
  % arrows
  \foreach \p/\l/\r in {p1/p2/p3, p2/p4/p5, p3/p6/p7}{
      \draw[->] (\p)  .. controls +(0,-0.40) and +(0,-0.40) .. (\l);
      \draw[->] (\p)  .. controls +(0,-0.60) and +(0,-0.60) .. (\r);
  }
  % from 2^{h-1} to 2^{h} and 2^{h}+1
  \draw[->] (phm) .. controls +(0,-0.40) and +(0,-0.40) .. (plh);
  \draw[->] (phm) .. controls +(0,-0.60) and +(0,-0.60) .. (prh);
  % from floor(n/2) to n-1 and n
  \draw[->] (pnpar) .. controls +(0,-0.40) and +(0,-0.40) .. (pnmone);
  \draw[->] (pnpar) .. controls +(0,-0.60) and +(0,-0.60) .. (pn);
\end{tikzpicture}
  \end{center}


\begin{algorithm}[htb]
  \caption{Max-Heapify}
  \label{alg:heapify}
  \begin{algorithmic}[1]
    \Function{MaxHeapify}{$A,i$}
      \State \(l \gets \mathrm{Left}(i)\)
      \State \(r \gets \mathrm{Right}(i)\)
      \State \(m \gets i\) \Comment{index of largest element among \(\{A[i], A[l], A[r]\}\)}
      \If{\(l \le \attrib{A}{heap\mbox{-}size} \AND A[l] > A[m] \)}
        \State \(m \gets l\)
      \EndIf
      \If{\(r \le \attrib{A}{heap\mbox{-}size} \AND A[r] > A[m] \)}
        \State \(m \gets r\)
      \EndIf
      \If{\(m \ne i\)}
        \State swap \(A[i]\) and \(A[m]\)
        \State \Call{MaxHeapify}{$A, m$}
      \EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[htb]
  \caption{Build-Max-Heap}
  \label{alg:build_heap}
  \begin{algorithmic}[1]
    \Function{BuildMaxHeap}{$A$}
      \State \(\attrib{A}{heap\mbox{-}size} \gets |A|\)
      \For{\(i=\lfloor|A|/2\rfloor \TO 1\)} \Comment{elements after \(\lfloor|A|/2\rfloor\) are leaves}
        \State \Call{MaxHeapify}{$A,i$}
      \EndFor
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[htb]
  \caption{Heap Sort}
  \label{alg:heap_sort}
  \begin{algorithmic}[1]
    \Function{HeapSort}{$A$}
      \State \Call{BuildMaxHeap}{$A$} \label{alg:heap_sort:build}
      \For{\(i=|A| \DOWNTO 2\)} \label{alg:heap_sort:start_for}
        \State swap \(A[1]\) and \(A[i]\)
        \State \(\attrib{A}{heap\mbox{-}size}\gets \attrib{A}{heap\mbox{-}size}-1\)
        \State \Call{MaxHeapify}{$A,1$}
      \EndFor \label{alg:heap_sort:end_for}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:heap_sort} sorts an array \(A\) \textcolor{red}{in place} by first building a max-heap from the input array and then repeatedly extracting the maximum element (the root of the heap) and placing it at the end of the array.
The complexities of Algorithms \ref{alg:heapify}, \ref{alg:build_heap}, \ref{alg:heap_sort} are:
\[
T_{\textsc{\hyperref[alg:heapify]{MaxHeapify}}}(n)=\Theta(\log n)
\qquad
T_{\textsc{\hyperref[alg:build_heap]{BuildMaxHeap}}}(n)=\Theta(n)
\]
\[
T_{\textsc{\hyperref[alg:heap_sort]{HeapSort}}}(n)=\Theta(n\log n)
\]

The complexity of Algorithm~\ref{alg:heapify} is determined by the height $\tilde{h}$ of the node to be heapified, which is given by $\tilde{h}=\lfloor\log i\rfloor$ for a node at index \(i\).

Analyzing the complexity of Algorithm~\ref{alg:build_heap} is more involved.
A simple upper bound on the running time is $O(n\lg n)$, since each call to \textsc{\hyperref[alg:heapify]{MaxHeapify}} costs $O(\log n)$ and \textsc{\hyperref[alg:build_heap]{BuildMaxHeap}} makes $O(n)$ such calls.
This upper bound is correct but not asymptotically tight.

We can derive a tighter bound by recalling that the time for \textsc{\hyperref[alg:heapify]{MaxHeapify}} to run at a node $i$ depends on the height $\tilde h$ of that node in the tree, and that the height of most nodes is small.

Calling \textsc{\hyperref[alg:heapify]{MaxHeapify}} on a node of height $\tilde{h}$ costs $c \, \tilde{h}$, and there are at most $\lceil n/2^{\tilde{h}+1}\rceil$ nodes at that height.  
We can sum over all heights, starting from the leaves (with height $0$) up to the root (with height $\lfloor\log n\rfloor$);
\begin{align}
T(n)
&=\sum_{\tilde{h}=0}^{\lfloor\log n\rfloor}\left\lceil \frac{n}{2^{\tilde{h}+1}}\right\rceil c \, \tilde{h}
\leq c \, n \sum_{\tilde{h}=0}^{\lfloor\log n\rfloor}\frac{n}{2^{\tilde{h}}} \tilde{h}
\leq c \, n \sum_{\tilde{h}=0}^{\infty}\frac{\tilde{h}}{2^{\tilde{h}}}
\label{eq:buildheap-sum}
\end{align}
Recall the geometric series:
\[
  \sum_{\tilde{h}=0}^\infty x^{\tilde{h}} = \frac1{1-x}, \quad |x|<1
\]
Differentiating both sides with respect to \(x\) gives:
\[
  \frac{\dif}{\dif x}\!\left(\sum_{\tilde{h}=0}^\infty x^{\tilde{h}}\right)
  = \frac{\dif}{\dif x}\!\left(\frac1{1-x}\right)
  \quad\Longrightarrow\quad
  \sum_{\tilde{h}=0}^\infty \tilde{h}\,x^{\tilde{h}-1} = \frac1{(1-x)^2}
\]
Multiply through by \(x\) to shift the exponent back:
\begin{equation}
\label{eq:geomseries-derivative}
\sum_{\tilde{h}=0}^\infty \tilde{h}\,x^{\tilde{h}}
= x \sum_{\tilde{h}=0}^\infty \tilde{h}\,x^{\tilde{h}-1}
= \frac{x}{(1-x)^2},\quad |x|<1,
\end{equation}
Setting $x=\tfrac12$ in \eqref{eq:geomseries-derivative} and substituting into \eqref{eq:buildheap-sum} gives
\begin{equation}
T(n) \leq c \, n \sum_{\tilde{h}=0}^{\infty}\tilde{h}\left(\frac{1}{2}\right)^{\tilde{h}} = c \, n \cdot \frac{\frac{1}{2}}{\left(1-\frac{1}{2}\right)^2} = O(n)
\end{equation}
Thus \textsc{\hyperref[alg:build_heap]{BuildMaxHeap}} builds a max-heap from an array in $O(n)$ time.


For Algorithm~\ref{alg:heap_sort}, we have a similar situation, but different from \textsc{\hyperref[alg:build_heap]{BuildMaxHeap}}, where the majority of the calls to \textsc{\hyperref[alg:heapify]{MaxHeapify}} are done on nodes at the bottom of the heap (where the height is small), in \textsc{\hyperref[alg:heap_sort]{HeapSort}}, the calls to \textsc{\hyperref[alg:heapify]{MaxHeapify}} are always done on the root of the heap, and the height is large for the majority of the calls.

After \textsc{\hyperref[alg:build_heap]{BuildMaxHeap}} has finished (line \ref{alg:heap_sort:build}), the array $A$ is a valid max-heap of size $n$.  
The \textsc{\hyperref[alg:heap_sort]{HeapSort}} loop (lines \ref{alg:heap_sort:start_for}-\ref{alg:heap_sort:end_for}) then performs exactly $n-1$ iterations.
At the $k^{\text{th}}$ iteration the heap contains $n-k+1$ elements, so the call to \textsc{\hyperref[alg:heapify]{MaxHeapify}} costs $\Theta\!\bigl(\log(n-k+1)\bigr)$.
The total time spent in the loop is therefore
\[
  \sum_{k=1}^{n-1} \Theta \left(\log(n-k+1)\right)
  \;=\;
  \Theta \left(\sum_{l=1}^{n-1}\log l \right)
  \;=\;
  \Theta \left(\log(n!)\right)
  \stackrel{\text{Ex. \ref{ex:logarithm-factorial}}}{=}
\Theta(n\log n)
\]
where the change of index $l=n-k+1$ rewrites the sum in increasing order.



\subsection{k-Smallest Element}
\label{subsec:quickselect}
Algorithm~\ref{alg:quickselect} is a randomized “divide-and-conquer” method for finding the $k$-th smallest element in an unsorted array in expected $O(n)$ time.  
At each step it chooses a pivot uniformly at random, partitions the input into three subsets -- those less than, equal to, and greater than the pivot -- and then recurses only on the subset that must contain the desired element.
\begin{algorithm}[htb]
    % def quickselect(A, k):
    % pivot = A[randint(0, len(A) - 1)]
    % L, M, R = [], [], []
    % for a in A: 
    %     if a < pivot:
    %         L.append(a)
    %     elif a == pivot:
    %         M.append(a)
    %     else:
    %         R.append(a)
    % if k <= len(L):
    %     return quickselect(L, k)
    % elif k <= len(L) + len(M):
    %     return pivot
    % else:
    %     return quickselect(R, k - (len(L) + len(M)))
\caption{Quick Select}
\label{alg:quickselect}
\begin{algorithmic}[1]
\Function{QuickSelect}{$A, k$} \Comment{$A$ is an unordered multiset (`bag') of elements}
\State $v \gets A[\operatorname{randint}(1,|A|)]$ \Comment{pick a random pivot}
\State $A_L, A_M, A_R \gets \{\}_m$ \Comment{three empty multisets}
\ForAll{$a \in A$}
    \If{$a < v$}
        \State add $a$ to $A_L$
    \ElsIf{$a = v$}
        \State add $a$ to $A_M$
    \Else
        \State add $a$ to $A_R$
    \EndIf
\EndFor
% \State $A_L \gets \langle\,x \in A \mid x < v\,\rangle$
% \State $A_M \gets \langle\,x \in A \mid x = v\,\rangle$
% \State $A_R \gets \langle\,x \in A \mid x > v\,\rangle$
\If{$k \le |A_L|$}
\State \Return \Call{QuickSelect}{$A_L, k$}
\ElsIf{$k \le |A_L| + |A_M|$}
\State \Return $v$
\Else
\State \Return \Call{QuickSelect}{$A_R,k - (|A_L| + |A_M|)$}
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

Partitioning around the pivot takes $\Theta(n)$ time, and since the pivot is random the expected size of the recursive call is at most a constant fraction of $n$, yielding an overall expected running time of $O(n)$.

\subsection{Overview of Sorting Algorithms}

% \begin{center}
% \begin{tabular}{lccc c}
%     \toprule
%     \textbf{Algorithm} & \multicolumn{3}{c}{\textbf{Complexity}} & \textbf{In place?} \\
%                         & \emph{worst}      & \emph{average}       & \emph{best}  & \\
%     \midrule
%     \textsc{Insertion-Sort} & $\Theta(n^2)$    & $\Theta(n^2)$        & $\Theta(n)$   & \ding{52} \\
%     \textsc{Selection-Sort} & $\Theta(n^2)$    & $\Theta(n^2)$        & $\Theta(n^2)$ & \ding{52} \\
%     \textsc{Merge-Sort}     & $\Theta(n\log n)$ & $\Theta(n\log n)$     & $\Theta(n\log n)$ & \ding{56} \\
%     \textsc{Quick-Sort}     & $\Theta(n^2)$    & $\Theta(n\log n)$     & $\Theta(n\log n)$ & \ding{52} \\
%     \textsc{Heap-Sort}      & $\Theta(n\log n)$ & $\Theta(n\log n)$     & $\Theta(n\log n)$ & \ding{52} \\
%     \bottomrule
% \end{tabular}
% \end{center}



\newcommand{\mixcolor}{70}

\definecolor{Red}{rgb}{1.0, 0.0, 0.0}
\definecolor{Orange}{rgb}{1.0, 0.5, 0.0}
\definecolor{Yellow}{rgb}{1.0, 1.0, 0.0}
\definecolor{Green}{rgb}{0.0, 1.0, 0.0}
\definecolor{BlueGreen}{rgb}{0.0, 1.0, 0.5}
\definecolor{Blue}{rgb}{0.0, 1.0, 1.0}


\begin{center}
    \begin{tabular}{l c c c c c}
        \toprule
        \textbf{Algorithm} & \multicolumn{3}{c}{\textbf{Time Complexity}} & \textbf{in place?} \\%& \textbf{Space}  \\
                            & \emph{worst}      & \emph{average}       & \emph{best}  & \\%& \emph{worst} \\
        \midrule
        \textsc{InsertionSort} & \cellcolor{Red!\mixcolor}$\Theta(n^2)$    & \cellcolor{Red!\mixcolor}$\Theta(n^2)$        & \cellcolor{Green!\mixcolor}$\Theta(n)$   & \ding{52} \\%& \cellcolor{Blue!\mixcolor}$\mathcal{O}(1)$ \\
        \textsc{SelectionSort} & \cellcolor{Red!\mixcolor}$\Theta(n^2)$    & \cellcolor{Red!\mixcolor}$\Theta(n^2)$        & \cellcolor{Red!\mixcolor}$\Theta(n^2)$ & \ding{52} \\%& \cellcolor{Blue!\mixcolor}$\mathcal{O}(1)$ \\
        \textsc{MergeSort}     & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$ & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$     & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$ & \ding{56} \\%& \cellcolor{Green!\mixcolor}$\mathcal{O}(n)$ \\
        \textsc{QuickSort}     & \cellcolor{Red!\mixcolor}$\Theta(n^2)$    & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$     & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$ & \ding{52} \\%& \cellcolor{BlueGreen!\mixcolor}$\mathcal{O}(\log n)$ \\
        \textsc{HeapSort}      & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$ & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$     & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$ & \ding{52} \\%& \cellcolor{Blue!\mixcolor}$\mathcal{O}(1)$ \\
        \bottomrule
     \end{tabular}
\end{center}


\subsection{Application}
When an array is sorted, many operations and queries (e.g. the one in Example \ref{ex:caterpillar}) can be performed much more efficiently than in an unsorted array.

\begin{example}[Caterpillar Method]
\label{ex:caterpillar}
\definecolor{svgBlue}       {HTML}{0F75BD}
\definecolor{svgRed}        {HTML}{EB1C24}
\definecolor{svgGreenDark}  {HTML}{0E9348}
\definecolor{svgGreenLight} {HTML}{8FC43A}
\definecolor{svgYellow}     {HTML}{FFFF00}
% Checking if there a sorted array contains two elements $A[i]$ and $A[j]$ such that $A[i] + q = A[j]$, can be done in linear running time using the \includegraphics[height=2ex]{images/Hungrycaterpillar.pdf} method (sometimes also called \emph{two-pointer} or \emph{sliding-window} method).
Checking if there a sorted array contains two elements $A[i]$ and $A[j]$ such that $A[i] + q = A[j]$, can be done in linear running time using the 
\begin{tikzpicture}[x=1mm,y=1mm,yscale=-1,scale=0.1]
  \path[fill=svgBlue] svg{m 157.18913 191.53421 c -0.65294 -0.39134 -1.41147 -1.16769 -1.81701 -1.85969 -0.59204 -1.01024 -0.67936 -1.42014 -0.58385 -2.74057 0.28372 -3.92242 4.20975 -6.0369 7.42406 -3.99845 1.65793 1.05143 2.05077 1.90213 2.05077 4.44099 0 2.45873 -0.25511 2.92729 -2.20157 4.04364 -1.77317 1.01696 -3.30611 1.05285 -4.8724 0.11408 z};
  \path[fill=svgRed] svg{m 162.16094 202.28463 c -2.31199 -0.80479 -5.51218 -3.34634 -7.18339 -5.70493 -3.3035 -4.66236 -3.51652 -11.58765 -0.5211 -16.94113 1.35536 -2.42231 4.21876 -5.04676 6.76393 -6.19948 1.86325 -0.84386 2.57212 -0.97367 5.319 -0.97393 2.42425 -2.3e-4 3.56364 0.16482 4.84708 0.70214 5.02426 2.10344 8.42886 6.56804 9.31109 12.20999 1.10931 7.09396 -3.21685 14.79046 -9.51276 16.92381 -2.6958 0.91348 -6.37153 0.90677 -9.02385 -0.0165 z m 8.25693 -4.80789 c 2.36514 -1.63378 1.37295 -2.92944 -1.08602 -1.41818 -2.06023 1.26619 -3.50533 1.15136 -6.43916 -0.51164 -0.73798 -0.41832 -0.89462 -0.41747 -1.18574 0.006 -0.48927 0.71217 0.16562 1.59506 1.76792 2.38344 2.0172 0.99255 5.13974 0.78569 6.943 -0.45995 z m -8.46621 -5.94253 c 1.57645 -0.87053 2.30023 -1.93159 2.55869 -3.75121 0.45522 -3.20479 -2.68621 -6.25609 -5.5574 -5.39791 -3.92371 1.17277 -4.86789 6.05337 -1.6771 8.66919 1.5928 1.30578 2.93149 1.44318 4.67581 0.47993 z m 12.92053 0.14281 c 1.77134 -0.78046 2.61059 -2.05403 2.76403 -4.19442 0.16774 -2.34026 -0.70928 -3.93178 -2.67604 -4.85616 -1.20596 -0.56677 -1.5875 -0.60079 -2.8612 -0.25518 -4.07594 1.10596 -4.71289 7.09715 -0.97404 9.16176 1.44063 0.79555 2.20211 0.82482 3.74725 0.144 z};
  \path[fill=black] svg{m 152.97534 209.11734 c -2.58672 -0.33038 -3.06549 -1.12043 -2.9364 -4.84548 l 0.1064 -3.07129 1.21176 -0.1058 1.21178 -0.1058 3.6e-4 2.13184 3.7e-4 2.13187 2.00221 0.017 c 3.22142 0.0274 5.16964 1.57253 3.68178 2.92001 -0.92038 0.83354 -3.0646 1.21037 -5.27832 0.92763 z M 62.15223 208.7513 c -1.146996 -0.61809 -1.29313 -0.97701 -1.299274 -3.19118 -0.0056 -1.87813 0.07652 -2.0902 0.943956 -2.44814 1.295202 -0.53445 1.287451 -0.53893 1.287451 0.74309 v 1.13497 l 2.250435 0.18481 c 3.63979 0.2989 5.09691 1.67883 3.31496 3.13933 -0.9403 0.77067 -5.33048 1.06602 -6.497528 0.43712 z m 10.626288 -1.8632 c -1.61115 -0.58775 -1.86425 -1.15626 -1.86425 -4.18729 v -2.9537 l 1.11855 -0.35315 1.11856 -0.35314 v 2.00849 2.00847 l 1.95747 0.001 c 5.43489 0.004 5.94372 3.73129 0.53818 3.94275 -1.27008 0.0497 -2.56091 -0.001 -2.86851 -0.11371 z m 68.172402 -0.46094 c -1.55643 -0.74466 -2.92323 -2.16671 -2.92323 -3.04138 0 -0.32667 0.87327 -1.97621 1.94062 -3.66564 l 1.94063 -3.07169 0.73652 1.05349 0.73652 1.05351 -0.98396 1.58527 -0.98395 1.58527 1.93923 1.15683 c 1.94737 1.16167 2.8259 2.44329 2.3862 3.48104 -0.32558 0.76838 -3.06002 0.69032 -4.78858 -0.1367 z m -9.69636 -3.86462 c -1.54806 -0.95268 -2.92104 -2.77809 -2.92104 -3.88359 0 -0.46288 0.85684 -1.55456 2.22151 -2.83036 l 2.22152 -2.07686 0.83922 0.7553 0.83923 0.75531 -1.27088 1.1875 -1.27088 1.1875 1.36754 1.26835 c 1.6764 1.55482 2.42386 3.24327 1.75762 3.97031 -0.682 0.74422 -2.2584 0.6053 -3.78384 -0.33346 z m -50.746042 -3.64127 c -2.80105 -3.16931 -3.22887 -3.81905 -3.01116 -4.57303 0.42317 -1.46552 1.78251 -0.71903 4.09885 2.25087 0.054 0.0693 0.88937 -0.34858 1.85626 -0.9286 2.08798 -1.25254 4.42092 -1.45924 4.78378 -0.42384 0.49634 1.41627 -1.41078 3.61311 -3.98056 4.5853 -2.0089 0.75999 -2.3422 0.67899 -3.74717 -0.9107 z m 43.939112 -5.44051 c -1.34318 -1.14726 -2.45261 -3.04618 -2.45261 -4.19793 0 -0.69946 0.79378 -1.41055 3.54211 -3.17309 1.76551 -1.13225 1.94197 -1.17538 3.33163 -0.81417 l 1.46737 0.38139 -2.25985 1.52088 -2.25984 1.52087 1.06947 1.16019 c 1.14932 1.24682 1.66952 2.89653 1.27147 4.03231 -0.39269 1.12051 -2.12993 0.91893 -3.70975 -0.43045 z m -33.684722 -1.5902 c -2.00487 -1.44994 -2.28209 -1.78076 -1.95547 -2.33348 0.60398 -1.02205 0.91214 -1.0421 2.35642 -0.15326 l 1.35873 0.83616 1.80426 -1.69528 c 1.88862 -1.77454 3.27494 -2.17476 4.1658 -1.20261 0.982975 1.07266 -0.68107 4.47935 -2.74465 5.61898 -1.70901 0.9438 -2.4072 0.79387 -4.98509 -1.07051 z m 72.265932 -21.60387 c -0.49215 -1.30922 -1.66606 -3.62651 -2.60868 -5.14953 -2.4138 -3.90007 -3.41024 -6.13364 -3.64145 -8.16247 -0.26752 -2.34752 0.29026 -3.29198 1.86194 -3.15269 1.46299 0.12965 2.58934 1.54416 3.60799 4.53103 1.20385 3.52985 2.72173 14.31404 2.01473 14.31404 -0.18684 0 -0.74238 -1.07117 -1.23453 -2.38038 z m 6.55805 2.12063 c -0.54796 -0.80297 1.40138 -12.30745 2.61189 -15.4147 1.11712 -2.86751 3.59436 -4.09387 4.82727 -2.38975 0.85516 1.18199 0.34083 2.84039 -2.82711 9.11584 -1.40632 2.78581 -2.85222 5.93885 -3.21312 7.00674 -0.61942 1.83291 -0.99099 2.27964 -1.39893 1.68187 z};
  \path[fill=svgGreenDark] svg{m 53.178419 203.12867 c -2.309776 -1.0424 -3.966718 -2.6754 -5.272052 -5.19583 -0.852006 -1.64512 -0.969704 -2.30336 -0.968746 -5.4182 7.46e-4 -2.91784 0.143541 -3.82512 0.805686 -5.12444 1.007891 -1.97774 3.569898 -4.52319 5.406898 -5.37193 2.384813 -1.10185 2.504316 -0.99651 2.27698 2.00726 -0.238887 3.15633 0.270624 5.43402 1.898528 8.48712 1.371817 2.57279 4.561752 5.62097 6.858296 6.5535 0.869869 0.35323 1.581579 0.75944 1.581579 0.90271 0 0.65872 -2.063219 2.36772 -3.81048 3.15628 -2.690429 1.2142 -6.091082 1.21556 -8.776689 0.004 z m 92.936891 -2.46894 c -3.39904 -1.41212 -5.04705 -4.09346 -5.31749 -8.65161 -0.20518 -3.45855 0.39509 -6.15539 2.10447 -9.45488 2.89056 -5.57958 9.03898 -9.51403 13.74293 -8.7943 2.60031 0.39788 2.66667 0.63514 0.70826 2.5313 -5.59084 5.41314 -6.55695 14.92796 -2.1257 20.93495 1.29547 1.75616 1.19618 1.95873 -1.57288 3.20843 -2.33442 1.05353 -5.33161 1.14343 -7.53959 0.22611 z m -71.130132 -7.77335 c -5.09804 -1.97207 -8.91885 -5.57567 -10.34477 -9.7567 -1.307998 -3.83523 -0.711355 -7.08411 1.68765 -9.18995 1.16472 -1.02238 1.2983 -1.06445 1.50789 -0.47506 0.78926 2.21951 1.73179 3.58296 3.71502 5.37415 3.48939 3.15149 7.31584 4.54719 12.48686 4.5546 1.53935 0.003 3.37387 -0.14534 4.07666 -0.3279 0.7028 -0.18254 1.33991 -0.27568 1.41578 -0.20698 0.0758 0.0687 0.1467 1.16777 0.15743 2.44236 0.0241 2.85653 -0.78381 4.80418 -2.58776 6.23866 -1.88586 1.49956 -3.56872 2.04341 -6.78651 2.19318 -2.44904 0.11397 -3.12186 0.008 -5.32825 -0.84636 z m 52.860352 -7.62911 c -2.87697 -0.91481 -5.03351 -4.28036 -5.03351 -7.85531 0 -1.9487 1.56286 -6.10663 3.11883 -8.29756 1.37058 -1.92992 4.14367 -4.48231 6.33503 -5.83089 6.03783 -3.71571 12.80346 -3.51204 15.77002 0.47475 1.05514 1.41798 1.70938 4.19906 1.41129 5.99926 l -0.23386 1.41232 -3.04043 0.1523 c -2.4959 0.12507 -3.47051 0.35065 -5.44182 1.25986 -4.42075 2.03894 -8.42187 6.58487 -9.8996 11.24752 -0.33844 1.0679 -0.81316 1.93496 -1.05495 1.9268 -0.24176 -0.008 -1.11072 -0.22821 -1.931 -0.48905 z M 80.221698 174.83918 c -6.06516 -1.872 -10.33049 -6.1746 -10.94791 -11.04354 -0.31016 -2.44598 0.8029 -4.91801 2.97123 -6.59892 l 1.52658 -1.1834 0.44992 1.5973 c 1.17667 4.17735 5.09944 8.97869 9.13151 11.17666 1.10149 0.60045 3.13728 1.3453 4.52399 1.65521 2.12791 0.47557 2.83578 0.49975 4.53698 0.15496 1.10863 -0.2247 2.45423 -0.72094 2.99023 -1.10278 0.53598 -0.38183 1.04269 -0.63252 1.12597 -0.55709 0.0833 0.0754 -0.24876 0.91944 -0.7379 1.87558 -2.16692 4.2358 -9.09063 6.02605 -15.5706 4.02602 z m 31.345832 -6.64416 c -3.20176 -1.53187 -4.88522 -4.73878 -4.84409 -9.22755 0.0472 -5.15277 2.55444 -10.30959 6.70433 -13.78913 7.48562 -6.27649 15.40641 -4.18799 17.00263 4.48312 l 0.22338 1.21333 -2.34106 0.90108 c -6.42008 2.47114 -11.04471 8.31638 -11.71018 14.80087 l -0.2361 2.30068 -1.7175 -0.0149 c -0.94839 -0.008 -2.32827 -0.30714 -3.08141 -0.66747 z m -16.763352 -2.1475 c -2.20523 -0.7137 -3.93548 -1.83417 -5.87387 -3.8038 -7.15303 -7.26834 -7.18378 -19.73184 -0.0568 -22.99723 3.07303 -1.40797 7.02216 -0.85044 9.97688 1.40851 l 1.206002 0.92201 -0.933312 1.24067 c -1.38833 1.84555 -3.0214 5.95917 -3.54451 8.92843 -0.86913 4.93332 0.16142 10.57414 2.42234 13.25867 0.50392 0.59833 0.81163 1.18263 0.68376 1.29841 -0.38239 0.34632 -2.43916 0.21082 -3.88057 -0.25567 z};
  \path[fill=svgGreenLight] svg{m 45.359131 202.6977 c -3.294933 -1.38158 -5.20006 -4.48 -5.212216 -8.47702 -0.007 -2.08279 0.157828 -2.80011 0.924414 -4.03903 1.13705 -1.83767 3.025286 -3.05214 4.745404 -3.05214 1.258378 0 1.274372 0.0181 0.970199 1.09744 -0.170057 0.6036 -0.309243 2.541 -0.309243 4.30535 0 2.80692 0.130647 3.46117 1.044806 5.23395 0.574679 1.11434 1.653453 2.59224 2.397293 3.28423 l 1.352411 1.25818 -1.359942 0.51462 c -1.762065 0.66678 -2.726894 0.64017 -4.553126 -0.12558 z m 19.015476 -4.04542 c -9.12575 -3.95957 -12.230232 -16.79128 -5.352409 -22.123 1.53317 -1.18853 3.398215 -1.88803 5.044208 -1.89184 0.775722 -0.001 0.786492 0.0414 0.252757 1.01029 -1.565906 2.84283 -0.705212 7.62933 2.003295 11.14042 1.88768 2.44706 5.25875 4.85389 8.32394 5.94302 l 2.49647 0.88704 -0.41849 1.061 c -0.23016 0.58355 -1.17352 1.749 -2.09633 2.58988 -2.81955 2.56922 -6.40516 3.05292 -10.253441 1.38319 z m 72.130493 -2.8544 c -3.63894 -0.91169 -5.86541 -3.71583 -6.23171 -7.84845 -0.4519 -5.09858 3.97703 -12.29038 9.28026 -15.06952 4.68292 -2.45404 9.3092 -2.52169 12.15186 -0.17768 l 0.82915 0.68368 -1.90453 0.79913 c -4.07745 1.71087 -8.40746 7.04175 -9.81754 12.08681 -0.62829 2.24807 -0.72323 6.06964 -0.20135 8.10818 0.18885 0.73767 0.31066 1.36992 0.27074 1.405 -0.38963 0.34228 -3.03103 0.35004 -4.37688 0.0128 z M 79.198888 182.41954 c -4.96159 -1.2143 -8.67654 -3.90959 -10.64826 -7.72558 -0.92538 -1.79091 -1.07255 -2.42287 -0.90547 -3.88873 0.1862 -1.6339 1.26162 -4.27386 1.741 -4.27386 0.11712 0 0.4526 0.49384 0.74574 1.09743 0.29314 0.60361 1.49875 1.97445 2.6792 3.04635 5.99439 5.4433 16.17929 6.51135 21.08617 2.21123 l 1.18586 -1.03922 0.25432 1.22761 c 0.8158 3.93837 -2.23029 8.0837 -6.8716 9.35133 -2.56884 0.70159 -6.38451 0.6989 -9.26696 -0.006 z m 40.983252 -8.11713 c -2.83148 -1.8576 -4.10451 -5.24048 -3.45142 -9.17156 0.90738 -5.46129 5.0649 -10.62906 10.55239 -13.11651 3.2678 -1.48125 5.37329 -1.72037 8.26308 -0.93839 3.16608 0.85675 5.82247 4.46965 5.82247 7.91903 0 1.05744 -0.0741 1.12058 -1.31677 1.12058 -1.8718 0 -5.52172 1.18759 -7.9198 2.57689 -4.16681 2.414 -8.18671 7.20369 -9.08004 10.81883 -0.24458 0.9898 -0.64451 1.79963 -0.88868 1.79963 -0.24422 0 -1.13575 -0.45383 -1.98123 -1.0085 z m -33.436372 -4.53009 c -5.43371 -1.56041 -10.13765 -5.96028 -12.12924 -11.34514 -0.64757 -1.75093 -0.77922 -2.74128 -0.65093 -4.89628 0.29322 -4.92454 2.75475 -7.53964 7.56022 -8.03183 l 1.86428 -0.19095 -0.11851 3.16975 c -0.20427 5.46592 1.62735 10.00549 5.5816 13.8338 2.50131 2.42163 5.32071 3.88203 7.49455 3.88203 h 1.43929 l -0.61576 0.94403 c -1.70755 2.61795 -6.37568 3.7976 -10.42555 2.63459 z m 14.502102 -2.5157 c -2.013482 -0.85298 -2.997732 -1.82651 -4.265732 -4.2193 -2.7323 -5.15604 -2.3228 -12.15855 1.08917 -18.62364 3.988112 -7.55672 11.303672 -10.35266 15.761452 -6.02386 1.11774 1.08538 2.55128 3.14683 2.5534 3.67176 0 0.1921 -0.47523 0.60428 -1.05778 0.91593 -3.21601 1.72049 -6.64948 6.022 -8.07971 10.12224 -1.6187 4.64068 -1.09164 10.54045 1.14126 12.775 0.5849 0.58533 0.5412 0.64845 -0.81896 1.18334 -2.23111 0.87736 -4.54896 0.95013 -6.3231 0.19853 z};
  \path[fill=svgBlue] svg{m 170.63883 191.67702 c -3.3344 -2.53777 -2.39326 -7.45897 1.67222 -8.74401 4.313 -1.36327 8.16626 3.80777 5.72402 7.68159 -1.5299 2.42671 -4.95688 2.91897 -7.39624 1.06242 z};
  \path[fill=svgYellow] svg{m 159.00066 180.68498 c -1.73762 0.2329 -3.52017 1.9332 -4.15438 3.9617 -1.93334 6.18748 4.17728 11.41752 8.24309 7.05589 2.01369 -2.16033 2.33252 -5.93765 0.72019 -8.53574 -1.11593 -1.79827 -2.92915 -2.73437 -4.8089 -2.48185 m 13.72912 0.0343 c -4.32849 0.93564 -5.80559 7.56268 -2.46731 11.07237 3.31797 3.48832 8.4807 0.55579 8.48805 -4.82121 0.007 -3.90291 -2.90496 -6.92495 -6.02074 -6.25116 m -11.91158 1.96839 c 2.12471 0.75795 3.1901 4.07658 2.072 6.45468 -1.44298 3.06881 -5.04047 3.09642 -6.42381 0.0494 -1.65339 -3.64193 1.04938 -7.68202 4.35181 -6.50411 m 14.10412 0.0815 c 1.92986 0.88445 2.87946 3.68291 2.04479 6.02672 -1.34361 3.77289 -5.89501 3.44691 -6.89042 -0.49343 -0.90725 -3.59206 1.95363 -6.85862 4.84563 -5.53329};
  \path[fill=black] svg{m 163.74844 198.05128 c -1.44602 -0.71168 -2.35134 -1.80511 -1.96423 -2.37239 0.34368 -0.50361 0.76995 -0.40708 2.26783 0.51349 1.8136 1.11463 3.6586 1.10304 5.50715 -0.0346 1.0614 -0.65319 1.53608 -0.78082 1.84922 -0.49722 0.29948 0.27121 0.30756 0.54483 0.028 0.94623 -1.36659 1.96211 -5.18022 2.67862 -7.68796 1.44445 z};
  \end{tikzpicture}
method (sometimes also called \emph{two-pointer} or \emph{sliding-window} method).

\begin{center}
\begin{tikzpicture}[xscale=0.3, yscale=0.3]

  % data & bounds
  \def\A{3,5,10,11,13,16,20,21,24}
  \def\q{4}
  \def\xmin{2}   % = min(A)-1
  \def\xmax{25}  % = max(A)+1
  \def\ymin{2}
  \def\ymax{25}

  %--- grid at each A[k] ---
  \foreach \a in \A {
    \draw[gray!30, thin] (\a,\ymin) -- (\a,\ymax);
    \draw[gray!30, thin] (\xmin,\a) -- (\xmax,\a);
  }

  %--- axes ---
  \draw[->] (\xmin,\ymin) -- (\xmax+0.5,\ymin) node[below right] {$x$};
  \draw[->] (\xmin,\ymin) -- (\xmin,\ymax+0.5) node[above left]  {$y$};

  %--- ticks: first, last, and A[i] / A[j] ---
  \node[below] at (3,\ymin)  {\small $A[1]$};
  \node[below] at (13.5,\ymin) {\small $\ldots$};
  \node[below] at (24,\ymin) {\small $A[n]$};

  \node[left] at (\xmin,3)  {\small $A[1]$};
  \node[left] at (\xmin,13.5) {\small $\vdots$};
  \node[left] at (\xmin,24) {\small $A[n]$};

  \node[below] at (16,\ymin) {\small\textcolor{Orange}{$A[i]$}};
  \node[left] at (\xmin,20) {\small\textcolor{Orange}{$A[j]$}};

  % Orange dotted line from (16,\ymin) to (16,20) and from (\xmin,20) to (16,20)
  \draw[Orange, dotted, thick] (16,\ymin) -- (16,20);
  \draw[Orange, dotted, thick] (\xmin,20) -- (16,20);

  %--- dashed line y = x + q ---
  \draw[Magenta, dashed, thick, domain=\xmin:{\ymax -\q}] 
        plot (\x,{\x+\q}) node[above right, Magenta] {$y = x + q$};

  %--- caterpillar arrows ---
  \draw[->, black, thick] (3,3)   -- (3,5);
  \draw[->, black, thick] (3,5)   -- (3,10);
  \draw[->, black, thick] (3,10)  -- (5,10);
  \draw[->, black, thick] (5,10)  -- (10,10);
  \draw[->, black, thick] (10,10) -- (10,11);
  \draw[->, black, thick] (10,11) -- (10,13);
  \draw[->, black, thick] (10,13) -- (10,16);
  \draw[->, black, thick] (10,16) -- (11,16);
  \draw[->, black, thick] (11,16) -- (13,16);
  \draw[->, black, thick] (13,16) -- (13,20);
  \draw[->, black, thick] (13,20) -- (16,20);

  %--- final red dot ---
  \fill[Orange] (16,20) circle (6pt);

\end{tikzpicture}
\end{center}

\begin{algorithm}[htb]
  \caption{Checking for two elements with difference $q$}
  \label{alg:caterpillar}
\begin{algorithmic}[1]
  \Function{Caterpillar}{$A, q$}
    \State $l, r \gets 1$ \label{line:init-indices}
    \While{$r \leq n$}
      \If{$A[r] < A[l] + q$}
        \State $r \gets r + 1$
      \ElsIf{$A[r] > A[l] + q$}
        \State $l \gets l + 1$
      \Else
        \State \Return \tru
      \EndIf
    \EndWhile
    \State \Return \fals
  \EndFunction
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:caterpillar} maintains two indices \(l\) and \(r\), both starting at the left end of \(A\) (Line \ref{line:init-indices}).
At each step, it compares \(A[r]\) and \(A[l]\) and checks if the difference \(A[r] - A[l]\) is less than, greater than, or equal to \(q\):

\begin{itemize}[before={\parskip = 0em}, nosep]
  \item If \(A[r] - A[l] < q\), move \(r\) one step to the right (to increase the difference).
  \item If \(A[r] - A[l] > q\), move \(l\) one step to the right (to decrease the difference).
  \item If \(A[r] - A[l] = q\), we found a valid pair and stop. \qedhere
\end{itemize}
\end{example}


\section{Data Structures}
\label{sec:data_structures}

Operations on a dynamic set can be grouped into two categories, \textit{queries}, which return information about the set, and \textit{modifying operations}, which change the set:
\begin{description}[style=nextline, font={$\; \bullet$~\normalfont}]
\item[\textsc{Search$(S, k)$}]
\textcolor{orange}{query} that, given a set $S$ and a key value $k$, returns a pointer $x$ to an element in $S$ such that $x.\text{key} = k$, or \textsc{nil} if no such element belongs to $S$
\item[\textsc{Insert$(S, x)$}]
\textcolor{orange}{modifying operation} that adds the element pointed to by $x$ to the set $S$ (we usually assume that any attributes in element $x$ needed by the set implementation have already been initialized)
\item[\textsc{Delete$(S, x)$}]
\textcolor{orange}{modifying operation} that, given a pointer $x$ to an element in the set $S$, removes $x$ from $S$ (note that this operation takes a pointer to an element $x$, \underline{not} a key value\textcolor{red}{!})
\item[\textsc{Minimum$(S)$} and \textsc{Maximum$(S)$}]
\textcolor{orange}{queries} on a totally ordered set $S$ that return a pointer to the element of $S$ with the smallest (for \textsc{Minimum}) or largest (for \textsc{Maximum}) key
\item[\textsc{Successor$(S, x)$}]
\textcolor{orange}{query} that, given an element $x$ whose key is from a totally ordered set $S$, returns a pointer to the next larger element in $S$, or \textsc{nil} if $x$ is the maximum element
\item[\textsc{Predecessor$(S, x)$}]
\textcolor{orange}{query} that, given an element $x$ whose key is from a totally ordered set $S$, returns a pointer to the next smaller element in $S$, or \textsc{nil} if $x$ is the minimum element
\end{description}

% stack and queue are dynamic sets in which the element removed is prespecified:
% \begin{description}[before={\parskip=0pt}, nosep, leftmargin=1em, labelindent=1em, font={\normalfont}]
% \item[\textbf{Stack}:] element deleted is the one most recently inserted (\textbf{LIFO}: last-in, first-out)
% \item[\textbf{Queue}:] element deleted is the one that has been in the set the longest (\textbf{FIFO}: first-in, first-out)
% \end{description}


%--------------------------------------------------------------------
\subsection{Stacks}
\label{subsec:stacks}

\begin{definition}[Stack]
A \emph{stack} is a LIFO container that
supports constant-time insertion and deletion at one end.
\end{definition}

\begin{figure}[htb]
  \centering
  \begin{tikzpicture}[
    % font=\footnotesize,
    scale=1.2]
    \draw[fill=arraycolor] (0,0) rectangle (3,0.5);
    \draw[]   (3,0) rectangle (5,0.5);
    \draw[thick] (0,0) rectangle (5,0.5);
    % \foreach \x in {1,2,3,4} \draw (\x,0)--(\x,0.5);
    draw (3,0) -- (3,0.5);
    \node at (4,0.25) {free};
    \node[left, outer sep=0.5ex] at (0,0.25) {$S$};
    \node[above] at (0.25,0.5) {$1$};
    \draw[dotted] (2.5,0.5) -- (2.5,0);
    \node[above] at (2.75,0.5) {$\attrib{S}{top}$};
    % \draw[<-] (2.75,0.55) -- ++(0,0.2) node[above] {$\attrib{S}{top}$};
    \node[above] at (4.75,0.5) {$N$};
  \end{tikzpicture}
  %\vspace*{-1.0em}
  %\caption{Stack after three \textsc{Push} operations.}
  \label{fig:stack}
\end{figure}

Interface (Algorithm \ref{alg:stack_ops}):
\begin{itemize}[before={\parskip=0pt},nosep]
  \item \textsc{StackEmpty}\((S)\): returns \tru{} iff the stack \(S\) contains no elements.
  \item \textsc{Push}\((S,x)\): places element \(x\) on the top of the stack \(S\).
  \item \textsc{Pop}\((S)\): removes and returns the top element of the stack \(S\).
\end{itemize}

The stack has attributes \(\attrib{S}{top}\), indexing the most recently inserted element, and \(\attrib{S}{length}\), equaling the size \(N\) of the array.



\begin{algorithm}[htb]
  \caption{Stack Operations (array-based)}
  \label{alg:stack_ops}
  \begin{algorithmic}[1]
    \Function{StackEmpty}{$S$}
      \State \Return $\bigl(\attrib{S}{top}=0\bigr)$
    \EndFunction
    \Function{Push}{$S,x$}
      \If{$\attrib{S}{top}=\attrib{S}{length}$}
        \State \textbf{error} ``overflow''
      \EndIf
      \State $\attrib{S}{top}\gets\attrib{S}{top}+1$
      \State $S[\attrib{S}{top}] \gets x$
    \EndFunction
    \Function{Pop}{$S$}
      \If{\Call{StackEmpty}{$S$}}
        \State \textbf{error} ``underflow''
      \EndIf
      \State $\attrib{S}{top}\gets\attrib{S}{top}-1$
      \State \Return $S[\attrib{S}{top}+1]$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

%--------------------------------------------------------------------
\subsection{Queues}
\label{subsec:queues}

\begin{definition}[Queue]
A \emph{queue} is a FIFO container with constant-time insertion at the tail and deletion at the head.
\end{definition}


Interface (Algorithms~\ref{alg:enqueue} and \ref{alg:dequeue}):
\begin{itemize}[before={\parskip=0pt},nosep]
  \item \textsc{Enqueue}\((Q,x)\): insert \(x\) at the tail of \(Q\).
  \item \textsc{Dequeue}\((Q)\): remove and return the head element of \(Q\).
\end{itemize}

The classic fixed-length \emph{circular-array} implementation keeps two indices \(\attrib{Q}{head}\) and \(\attrib{Q}{tail}\) (\(\attrib{Q}{head}\) points to the first element, \(\attrib{Q}{tail}\) to the first free slot).  


\begin{figure}[htb]
  \centering
  \begin{tikzpicture}[
    % font=\footnotesize,
    scale=1.2]
    \draw[fill=arraycolor] (2,0) rectangle (5.5,0.5);
    \draw[thick] (0,0) rectangle (8,0.5);
    draw (2,0) -- (2,0.5);
    \node at (1,0.25) {free};
    \node at (6.75,0.25) {free};
    \node[left, outer sep=0.5ex] at (0,0.25) {$Q$};
    \node[above] at (0.25,0.5) {$1$};
        % ----- head / tail arrows -----
        % \draw[<-] (2.25,0.55) -- ++(0,0.2) node[above] {$\attrib{Q}{head}$};
        % \draw[<-] (5.75,0.55) -- ++(0,0.2) node[above] {$\attrib{Q}{tail}$};
    \node[above] at (2.25,0.5) {$\attrib{Q}{head}$};
    \draw[dotted] (2.5,0.5) -- (2.5,0);
    \node[above] at (5.75,0.5) {$\attrib{Q}{tail}$};
    \draw[dotted] (6,0.5) -- (6,0);
    \node[above] at (7.75,0.5) {$N$};
  \end{tikzpicture}
  \label{fig:queue}
\end{figure}

\begin{algorithm}[htb]
  \caption{Enqueue (circular array)}
  \label{alg:enqueue}
  \begin{algorithmic}[1]
    \Function{Enqueue}{$Q,x$}
      \If{$\attrib{Q}{queue\mbox{-}full}$}
        \State \textbf{error} ``overflow''
      \EndIf
      \State $Q[\attrib{Q}{tail}] \gets x$
      \State $\attrib{Q}{tail} \gets (\attrib{Q}{tail} \bmod \attrib{Q}{length}) + 1$
      \State $\attrib{Q}{queue\mbox{-}empty} \gets \fals$
      \If{$\attrib{Q}{tail} = \attrib{Q}{head}$}
        \State $\attrib{Q}{queue\mbox{-}full} \gets \tru$
      \EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[htb]
  \caption{Dequeue (circular array)}
  \label{alg:dequeue}
  \begin{algorithmic}[1]
    \Function{Dequeue}{$Q$}
      \If{$\attrib{Q}{queue\mbox{-}empty}$}
        \State \textbf{error} ``underflow''
      \EndIf
      \State $x \gets Q[\attrib{Q}{head}]$
      \State $\attrib{Q}{head} \gets (\attrib{Q}{head} \bmod \attrib{Q}{length}) + 1$
      \State $\attrib{Q}{queue\mbox{-}full} \gets \fals$
      \If{$\attrib{Q}{tail} = \attrib{Q}{head}$}
        \State $\attrib{Q}{queue\mbox{-}empty} \gets \tru$
      \EndIf
      \State \Return $x$
    \EndFunction
  \end{algorithmic}
\end{algorithm}


%--------------------------------------------------------------------
\subsection{Linked Lists}
\label{subsec:linked_lists}

\begin{definition}[Doubly-Linked List]
  A \emph{doubly-linked list} \(L\) consists of nodes \(x\) that store a key \(\attrib{x}{key}\) and two links \(\attrib{x}{prev}\), \(\attrib{x}{next}\).
  A special sentinel node \(\attrib{L}{nil}\) simplifies boundary cases because the list is empty iff \(\attrib{\attrib{L}{nil}}{next} = \attrib{L}{nil}\).
  \end{definition}


\begin{figure}[htb]
  \centering

  \begin{tikzpicture}[scale=0.65,every node/.style={font=\normalsize}]
    \def\cw{1} % cell width
    
    \newcommand{\cell}[5]{%
        \pgfmathsetmacro\x{#2*\cw}%
        \pgfmathsetmacro\y{#3*\cw}%
        \draw[fill=#5] (\x-0.5*\cw,\y-0.5*\cw) rectangle ++(\cw,\cw);
        \node at (\x,\y) {#1};
        \coordinate (#4) at (\x,\y); % anchor for arrows
    }
  
    % sentinel
    \cell{}{0}{1}{nil_next}{cyan!25}
    \cell{\textsc{nil}}{0}{0}{nil_key}{cyan!25}
    \cell{}{0}{-1}{nil_prev}{cyan!25}
    
    % first element
    \cell{}{3}{1}{x1_next}{arraycolor}
    \cell{$k_1$}{3}{0}{x1_key}{arraycolor}
    \cell{}{3}{-1}{x1_prev}{arraycolor}
  
    % second element
    \cell{}{6}{1}{x2_next}{arraycolor}
    \cell{$k_2$}{6}{0}{x2_key}{arraycolor}
    \cell{}{6}{-1}{x2_prev}{arraycolor}
    
    % elements in between
    \node[] at (9*\cw,0) (n2) {$\hdots$};
    
    % last element
    \cell{}{12}{1}{xn_next}{arraycolor}
    \cell{$k_n$}{12}{0}{xn_key}{arraycolor}
    \cell{}{12}{-1}{xn_prev}{arraycolor}
  
    % pointer to sentinel
    \coordinate (nil_label_coord) at ($(nil_prev) + (-1*\cw,0*\cw)$);
    \node[left=0em of nil_label_coord] (nil_label) {$\attrib{L}{nil}$};
    \draw[->] (nil_label) -- ($(nil_prev) + (-0.5*\cw,0*\cw)$);
    
    % dots and arrows  
    % next
    \draw[->] (nil_next) node[circle, fill=black, inner sep=0pt, minimum size=2pt] {} -- ($(x1_next) - (0.5*\cw,0)$);
    \draw[->] (x1_next) node[circle, fill=black, inner sep=0pt, minimum size=2pt] {} -- ($(x2_next) - (0.5*\cw,0)$);
    % \draw[-] (x2_next) node[circle, fill=black, inner sep=0pt, minimum size=2pt] {} -- ($(x2_next)!0.2!($(xn_next) - (0.5*\cw,0)$)$);
    % \draw[dotted] ($(x2_next)!0.2!($(xn_next) - (0.5*\cw,0)$)$) -- ($(x2_next)!0.3!($(xn_next) - (0.5*\cw,0)$)$);
    % \draw[dotted] ($(x2_next)!0.7!($(xn_next) - (0.5*\cw,0)$)$) -- ($(x2_next)!0.8!($(xn_next) - (0.5*\cw,0)$)$);
    % \draw[->] ($(x2_next)!0.8!($(xn_next) - (0.5*\cw,0)$)$) -- ($(xn_next) - (0.5*\cw,0)$);
    \draw[-] (x2_next) node[circle, fill=black, inner sep=0pt, minimum size=2pt] {} -- ($(x2_next)!0.25!(xn_next)$);
    \draw[dotted] ($(x2_next)!0.25!(xn_next)$) -- ($(x2_next)!0.35!(xn_next)$);
    \draw[dotted] ($(x2_next)!0.65!(xn_next)$) -- ($(x2_next)!0.75!(xn_next)$);
    \draw[->] ($(x2_next)!0.75!(xn_next)$) -- ($(xn_next) - (0.5*\cw,0)$);
    \draw[->,rounded corners=4pt] (xn_next) node[circle, fill=black, inner sep=0pt, minimum size=2pt] {} -- +(0,1) -- ($(nil_next) + (-1*\cw,1)$) -- ($(nil_next) + (-1*\cw,0)$) -- ($(nil_next) - (0.5*\cw,0)$); % I do not want a bezier, but straight line going through the control points (maybe with a little bit of curvature to make it look nice)
    % prev
    \draw[->] (x1_prev) node[circle, fill=black, inner sep=0pt, minimum size=2pt] {} -- ($(nil_prev) + (0.5*\cw,0)$);
    \draw[->] (x2_prev) node[circle, fill=black, inner sep=0pt, minimum size=2pt] {} -- ($(x1_prev) + (0.5*\cw,0)$);
    % \draw[-]  (xn_prev) node[circle, fill=black, inner sep=0pt, minimum size=2pt] {} -- ($(xn_prev)!0.2!($(x2_prev) + (0.5*\cw,0)$)$);
    % \draw[dotted] ($(xn_prev)!0.2!($(x2_prev) + (0.5*\cw,0)$)$) -- ($(xn_prev)!0.3!($(x2_prev) + (0.5*\cw,0)$)$);
    % \draw[dotted] ($(xn_prev)!0.7!($(x2_prev) + (0.5*\cw,0)$)$) -- ($(xn_prev)!0.8!($(x2_prev) + (0.5*\cw,0)$)$);
    % \draw[->] ($(xn_prev)!0.8!($(x2_prev) + (0.5*\cw,0)$)$) -- ($(x2_prev) + (0.5*\cw,0)$);
    \draw[-] (xn_prev) node[circle, fill=black, inner sep=0pt, minimum size=2pt] {} -- ($(xn_prev)!0.25!(x2_prev)$);
    \draw[dotted] ($(xn_prev)!0.25!(x2_prev)$) -- ($(xn_prev)!0.35!(x2_prev)$);
    \draw[dotted] ($(xn_prev)!0.65!(x2_prev)$) -- ($(xn_prev)!0.75!(x2_prev)$);
    \draw[->] ($(xn_prev)!0.75!(x2_prev)$) -- ($(x2_prev) + (0.5*\cw,0)$);
    \draw[->,rounded corners=4pt] (nil_prev) node[circle, fill=black, inner sep=0pt, minimum size=2pt] {}
              -- +(0,-1)                             % drop a little
              -- ($(xn_prev) + (1.0*\cw,-1)$)       % travel under the diagram
              -- ($(xn_prev) + (1.0*\cw,0)$)        % come up again
              -- ($(xn_prev) + (0.5*\cw,0)$);       % arrow-head at last element
  
  \end{tikzpicture}
  
  
\end{figure}

\begin{algorithm}[htb]
  \caption{Doubly-Linked List Operations (with sentinel)}
  \label{alg:list_ops}
  \begin{algorithmic}[1]
    \Function{ListInit}{$L$}
      \State $\attrib{\attrib{L}{nil}}{prev} \gets \attrib{L}{nil}$
      \State $\attrib{\attrib{L}{nil}}{next} \gets \attrib{L}{nil}$
    \EndFunction
    \Function{ListInsert}{$L,x$} \Comment{insert at front}
      \State $\attrib{x}{next} \gets \attrib{\attrib{L}{nil}}{next}$
      \State $\attrib{\attrib{\attrib{L}{nil}}{next}}{prev} \gets x$
      \State $\attrib{\attrib{L}{nil}}{next} \gets x$
      \State $\attrib{x}{prev} \gets \attrib{L}{nil}$
    \EndFunction
    \Function{ListDelete}{$x$}
      \State $\attrib{\attrib{x}{prev}}{next} \gets \attrib{x}{next}$
      \State $\attrib{\attrib{x}{next}}{prev} \gets \attrib{x}{prev}$
    \EndFunction
    \Function{ListSearch}{$L,k$}
      \State $x \gets \attrib{\attrib{L}{nil}}{next}$
      \While{$x \ne \attrib{L}{nil} \AND \attrib{x}{key} \ne k$}
        \State $x \gets \attrib{x}{next}$
      \EndWhile
      \State \Return $x$
    \EndFunction
  \end{algorithmic}
\end{algorithm}



Algorithm~\ref{alg:list_ops} shows the basic operations on a doubly-linked list.
\textsc{ListInsert} and \textsc{ListDelete} take \(O(1)\) time,
whereas \textsc{ListSearch} takes \(\Theta(n)\) time in the worst case,
with \(n\) the current length of \(L\).


%--------------------------------------------------------------------
\subsection{Dictionaries}
\begin{definition}[Dictionary]
\label{def:dictionary}
A \emph{dictionary} is an abstract data structure that represents a set of elements (or keys). 
It is a \emph{dynamic set} that supports the following operations:
\begin{itemize}[before={\parskip=0pt}, nosep]
\item \textsc{Insert}: insert element into the set
\item \textsc{Delete}: delete element from the set
\item \textsc{Search}: test memebership of an element in the set \qedhere
\end{itemize}
\end{definition}

\subsection{Direct-Address Tables}
\label{subsec:direct_address}
\begin{definition}[Direct-Address Table]
A \emph{direct-address table} implements a dictionary.
Suppose the key universe is the set \(U=\{1,\dots,M\}\).
A \emph{direct-address table} is an array \(T[1:M]\) where slot \(T[k]\) stores a Boolean indicating membership of key \(k\) in the represented set.
\end{definition}
\begin{algorithm}[htb]
  \caption{Direct-Address Table Operations}
  \label{alg:direct_address}
  \begin{algorithmic}[1]
    \Function{DirectAddressInsert}{$T,k$}  \State $T[k] \gets \tru$  \EndFunction
    \Function{DirectAddressDelete}{$T,k$}  \State $T[k] \gets \fals$ \EndFunction
    \Function{DirectAddressSearch}{$T,k$}  \State \Return $T[k]$     \EndFunction
  \end{algorithmic}
\end{algorithm}
All direct-address table operations (Algorithm \ref{alg:direct_address}) cost \(O(1)\) time, but the table occupies \(\Theta(|U|)\) space, which is prohibitive when the universe is large and the actual set is sparse.
i.e., direct-address tables usually waste a lot of space.


%--------------------------------------------------------------------
\subsection{Hash Tables}
\label{subsec:hash_tables}

To reduce the space overhead we use a smaller table \(T\) with \(|T| \ll |U|\) and map each key \(k\in U\) to a position in \(T\) using a
\emph{hash function} \(h:U\rightarrow\{1,\dots,|T|\}\).

\begin{definition}[Load Factor]
For a table of size \(|T|\) that currently stores \(n\) keys, the
\emph{load factor} is \(\alpha = \frac{n}{|T|}\).
\end{definition}

\subsubsection{Chaining}
Each slot \(T[i]\) stores a linked list of keys that hash to \(i\).

\begin{figure}[htb]
\centering
% stolen from https://tex.stackexchange.com/questions/636419/draw-hash-table, who stole it from CLRS
\begin{tikzpicture}[scale=0.8,
node distance = 7mm and 4mm,
  start chain = going right, 
    arr/.style = {semithick, ->},
    dot/.style = {circle, fill, inner sep=1.2pt,label={[fill=none]left:#1}},
every label/.append style = {font=\footnotesize, fill=white, align=center,
                              fill opacity=0.5, text opacity=1, 
                              inner sep=1pt},
      E/.style = {ellipse, draw, fill=#1},
  mpnh/.style = {rectangle split, rectangle split horizontal, 
                  rectangle split parts=3, draw, fill=arraycolor,
                  inner sep=2pt,
                  on chain},
  mpnv/.style = {rectangle split, rectangle split parts=10,
      rectangle split part fill={myblue!30,myorange!30,myblue!30,myblue!30,myblue!30,
                                myorange!30,myblue!30,myorange!30,myorange!30,myblue!30},
      draw, minimum height=2ex},
    sym/.style = {yshift=-1mm},
    syp/.style = {yshift=+1mm},
                        ]
\node[mpnv, label=$T$] (H) 
    {\nodepart{one}     $\diagup$
      \nodepart{two}     \vphantom{$\diagup$} 
      \nodepart{three}   $\diagup$
      \nodepart{four}    $\diagup$
      \nodepart{five}    $\diagup$
      \nodepart{six}     \vphantom{$\diagup$} 
      \nodepart{seven}   $\diagup$
      \nodepart{eight}   \vphantom{$\diagup$} 
      \nodepart{nine}    \vphantom{$\diagup$} 
      \nodepart{ten}     $\diagup$
    };
%
\node[mpnh, right=of H.two east] (A1) 
    {\nodepart{one}  $\diagup$
    \nodepart{two}  $k_1$
    \nodepart{three}    \hphantom{$\diagup$}  
    };
\node[mpnh] (A2)
    {\nodepart{one}      \hphantom{$\diagup$}
    \nodepart{two}      $k_4$
    \nodepart{three}    $\diagup$
    };
%
\node[mpnh, right=of H.six east] (B1)
    {\nodepart{one}      $\diagup$
    \nodepart{two}      $k_5$
    \nodepart{three}    \hphantom{$\diagup$}
    };
\node[mpnh] (B2)
    {\nodepart{one}      \hphantom{$\diagup$}
    \nodepart{two}      $k_2$
    \nodepart{three}    \hphantom{$\diagup$}
    };
\node[mpnh] (B3)
    {\nodepart{one}      \hphantom{$\diagup$}
    \nodepart{two}      $k_7$
    \nodepart{three}    $\diagup$ 
    };
%
\node[mpnh, right=of H.eight east] (C1)
    {\nodepart{one}      $\diagup$
    \nodepart{two}      $k_3$
    \nodepart{three}    $\diagup$
    };
%
\node[mpnh, right=of H.nine east] (D1)
    {\nodepart{one}  $\diagup$
    \nodepart{two}  $k_8$
    \nodepart{three}    \hphantom{$\diagup$}
    };
\node[mpnh] (D2)
    {\nodepart{one}      \hphantom{$\diagup$}
    \nodepart{two}      $k_6$
    \nodepart{three}    $\diagup$
    };
%% arrows (right)
\draw[arr]  (H |- H.two east)   edge (A1)
            (H |- H.six east)   edge (B1)
            (H |- H.eight east) edge (C1)
            (H |- H.nine east)   to (D1)
            ;
\draw[arr, transform canvas={yshift=1mm}]  
            (A1.three north |- A1.east)  edge (A2)
            (B1.three north |- B1.east)  edge (B2)
            (B2.three north |- B2.east)  edge (B3)
            (D1.three north |- D2)   to   (D2)
            ;
\draw[arr, transform canvas={yshift=-1mm}]
            (A2.one north |- A2)  edge  (A1)
            (B2.one north |- B2)  edge  (B1)
            (B3.one north |- B3)  edge  (B2)
            (D2.one north |- D2)   to   (D1)
            ;
%% dots, ellipses
\pgfmathsetseed{3}
Explicitly sets the seed for
\foreach \i in {1,2,...,8}
    \node (k\i) [
      dot=$k_{\i}$,
      label={[fill=none]left:$k_\i$} 
      ] at (-33mm +40*rand,0.9*rand) {};


\scoped[on background layer]
{
\draw[fill=myblue!30]  (-4,0.4) ellipse (3 and 2.4);
\path (-4,1.6)
  node[label={
     [fill=none,                % no white box
      text opacity=1,
      align=center,
      inner sep=1pt]
     above:{$U$\\(universe of keys)}  % position: text
  }]
  {};

\draw[fill=myorange!30]   (-4,0) ellipse (2.4 and 1.6);
\path   (-6.2,0) node[label={[fill=none]right:$K$\\ (actual\\ keys)}] {};

\draw[arr]  (k1)    edge ([syp] H.two west)
            (k4)    edge ([sym] H.two west)

            (k2)    edge ([syp] H.six west)
            (k5)    edge (H.six west)
            (k7)    edge ([sym] H.six west)
            
            (k3)    edge (H.eight west)
            
            (k8)    edge ([syp] H.nine west)
            (k6)    edge ([sym] H.nine west)
            ;
}
\end{tikzpicture}
\end{figure}

\begin{algorithm}[htb]
  \caption{Chained Hash Operations}
  \label{alg:chained_hash}
  \begin{algorithmic}[1]
    \Function{ChainedHashInsert}{$T,k$}
      \State \Call{ListInsert}{$T[h(k)],k$}
    \EndFunction
    \Function{ChainedHashSearch}{$T,k$}
      \State \Return \Call{ListSearch}{$T[h(k)],k$}
    \EndFunction
    \Function{ChainedHashDelete}{$T,k$}
      \State $x \gets$ \Call{ChainedHashSearch}{$T,k$}
      \If{$x \ne \textsc{nil}$} \State \Call{ListDelete}{$x$} \EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}

We assume \emph{uniform hashing} i.e.
\[
\Prob[h(k) = i] = \frac{1}{|T|} \quad \forall i \in \{1,\dots,|T|\}
\]
So, given \(n\) distinct keys, the expected length \(n_i\) of the linked list at position \(i\) is
\[
\Exp[n_i] = \frac{n}{|T|} = \alpha
\]

If we further assume \(h(k)\) can be computed in \(O(1)\), the expected running time of \textsc{ChainedHashSearch} is
\[\boxed{
\Theta(1+\alpha)
}\]

\subsubsection{Open Addressing}

Instead of using linked lists, keys are stored directly in the array. 
On a collision we \emph{probe} other slots in \(T\) using a permutation \(h(k,1),\dots,h(k,|T|)\). 
So \(h(k,i)\) is a function of both \(k\) and \(i\), where \(i\) is the probe number.

\(h(k,\cdot)\) must be a \emph{permutation} of \(\{1,\dots,|T|\}\), i.e., \(h(k,1), \dots, h(k,|T|)\) must cover all slots in \(T\) exactly once.


We assume \emph{independent uniform permutation hashing}: the probe sequence of each key is equally likely to be any of the \textcolor{red}{\(|T|!\)} permutations of \(\{1,\dots,|T|\}\).
Independent uniform permutation hashing generalizes the notion of independent uniform hashing introduced earlier to a hash function that produces not just a single slot number, but a whole probe sequence. 
True independent uniform permutation hashing is difficult to implement, however, and in practice suitable approximations (such as double hashing,  Example~\ref{ex:double_hashing}) are used.

Neither double hashing nor its special case, linear probing, meets the assumption of independent uniform permutation hashing. 
Double hashing cannot generate more than \textcolor{red}{\(|T|^2\)} different probe sequences.
Nonetheless, double hashing has a large number of possible probe sequences and seems to give good results. 
Linear probing is even more restricted, capable of generating only \textcolor{red}{\(|T|\)} different probe sequences.

\begin{example}[Double Hashing]
  \label{ex:double_hashing}
  Double hashing offers one of the best methods available for open addressing because the permutations produced have many of the characteristics of randomly chosen permutations. 
  Double hashing uses a hash function of the form
  \begin{equation}
    \label{eq:double_hashing}
  h(k,i) = (h_1(k) + i \cdot h_2(k)) \bmod |T|
  \end{equation}
  where \(h_1\) and \(h_2\) are two different hash functions.
  The second hash function \(h_2(k)\) must be \emph{relatively prime} to \(|T|\) (i.e., \(\gcd(h_2(k),|T|)=1\)) to ensure that all slots in \(T\) are probed.
  A convenient way to achieve this is to let \(|T|\) be an exact power of \(2\) and to design \(h_2(k)\) so that it always produces an odd number.
  Another way is to let \(|T|\) be prime and to design \(h_2(k)\) so that it always returns a positive integer less than \(|T|\).
  In~\eqref{eq:double_hashing} we can also set \(h_2(k) = 1\) for all \(k\), in which case we get \emph{linear probing}, a special case of double hashing.
\end{example}

\begin{algorithm}[htb]
  \caption{Open-Address Hash Insert (generic probing)}
  \label{alg:open_address_insert}
  \begin{algorithmic}[1]
    \Function{HashInsert}{$T,k$}
      \For{$i=1 \TO |T|$}
        \State $j \gets h(k,i)$
        \If{$T[j]=\textsc{nil}$}
          \State $T[j] \gets k$
          \State \Return $j$
        \EndIf
      \EndFor
      \State \textbf{error} ``overflow''
    \EndFunction
  \end{algorithmic}
\end{algorithm}

To analyze the time complexity of Algorithm~\ref{alg:open_address_insert}, we assume independent uniform permutation hashing for the hash function.
We also assume that at least one slot is empty, i.e., \(\alpha < 1\).
Because deleting from an open-address hash table does not really free up a slot, we assume as well that no deletions occur.

If we denote by \(X\) the number of probes performed until an empty slot is found, then on each probe we hit an occupied slot with probability \(\alpha = n/|T|\) and an empty slot with probability \(1-\alpha\).  
Hence
\[
\Prob[X=i]= \alpha^{i-1}(1-\alpha), \quad i=1,2,\dots
\]
so that \(X\) is geometrically distributed with success probability \(1-\alpha\).
Hence
\[
\Exp[X]=\sum_{i=1}^{\infty} i\alpha^{i-1}(1-\alpha)=\frac{1}{1-\alpha}
\]
Thus an insertion (or an unsuccessful search) requires on average \(\tfrac{1}{1-\alpha}\) probes; this grows rapidly as the load factor \(\alpha\) approaches \(1\).

% \[
% \Prob[X>i] = \alpha^{i}, \quad 0 \leq i < |T|
% \]
% and the expectation is the tail-sum of this geometric distribution:
% \[
% \Exp[X]
%    =
%    \sum_{i=0}^{\infty}\Prob[X>i]
%    \leq
%    \sum_{i=0}^{\infty}\alpha^{i}
%    =
%    \frac{1}{1-\alpha}
% \]
% Thus an insertion (or any \emph{unsuccessful} search) in an open-address hash table takes
% \[
% \boxed{\Exp[X]=\dfrac{1}{1-\alpha}}
% \]
% probes on average, which grows sharply as the table fills up and \(\alpha\) approaches 1.


% %--------------------------------------------------------------------
% \subsection{Complexity Summary}
% \label{subsec:ds_complexity}

% \begin{center}
% \begin{tabularx}{\linewidth}{l *{3}{>{\centering\arraybackslash}X} c}
% \toprule
% \textbf{Structure / Operation} &
% \textbf{Insert} & \textbf{Delete} & \textbf{Search} &
% \textbf{Notes}\\
% \midrule
% Stack & \(O(1)\) & \(O(1)\) & \(O(1)\) & array implementation \\
% Queue & \(O(1)\) & \(O(1)\) & \(O(1)\) & circular array \\
% Linked List & \(O(1)\) & \(O(1)\) & \(\Theta(n)\) & sentinel node \\
% Direct-Address & \(O(1)\) & \(O(1)\) & \(O(1)\) & space \(\Theta(|U|)\) \\
% Hash (chaining) & \(O(1)\) & \(O(1)\) & \(\Theta(1+\alpha)\) & \(\alpha = n/m\) \\
% Hash (open addr.) & \(O\!\bigl(\tfrac{1}{1-\alpha}\bigr)\) &
% \(O\!\bigl(\tfrac{1}{1-\alpha}\bigr)\) &
% \(O\!\bigl(\tfrac{1}{1-\alpha}\bigr)\) &
% \(\alpha<1\) \\
% \bottomrule
% \end{tabularx}
% \end{center}

% \begin{example}
% Hash tables with chaining are usually preferable for applications that
% require a dynamically growing dictionary, whereas open addressing is
% often faster in memory-constrained settings as long as a good probe
% sequence is employed.
% \end{example}


%--------------------------------------------------------------------
\subsection{Binary Search Trees}
\label{subsec:bst}

\begin{definition}[Binary Search Tree]
A \emph{binary search tree} implements a dynamic set. It stores keys from a totally ordered domain in nodes linked by \emph{left} and \emph{right} child pointers such that for every node \(x\)
\[
y\in\text{left-subtree}(x) \Leftrightarrow  \attrib{y}{key}\le\attrib{x}{key}
,\quad
z\in\text{right-subtree}(x)\Leftrightarrow  \attrib{z}{key}\ge\attrib{x}{key}
\]
\end{definition}

% Interface:
% \begin{itemize}[before={\parskip=0pt},nosep]
%   \item \textsc{TreeInsert}\((T,k)\), \textsc{TreeDelete}\((T,k)\), \textsc{TreeSearch}\((T,k)\)
%   \item Traversals: \textsc{InOrder}-, \textsc{Preorder}-, \textsc{Postorder}-, \textsc{ReverseOrderTreeWalk}
%   \item Extremal queries: \textsc{TreeMinimum}, \textsc{TreeMaximum}
%   \item Iteration: \textsc{TreeSuccessor}, \textsc{TreePredecessor}
% \end{itemize}




\begin{example}[Lower Bound]
  Let $t$ be the root of a binary search tree that represents a set $S$ of numbers. 
  % So, all the values in the tree are distinct. 
  The size of the tree is $|S|=n$. 
  The height of the tree is $h$. Algorithm~\ref{alg:bst-lower-bound} returns the node containing the least element $y \in S$ such that $x \leq y$, or \textsc{nil} if no such element exists.
  
  \begin{algorithm}[htb]
    \caption{Lower Bound for Binary Search Tree}
    \label{alg:bst-lower-bound}
    \begin{algorithmic}[1]
      \Function{LowerBound}{$t, x$}
      \If{$t = \textsc{nil}$} \Comment{base case}
      \State \Return \textsc{nil}
      \EndIf
      % \If{$t.\text{key} = x$}
      % \State \Return $t$
      % \ElsIf{$t.\text{key} < x$}
      \If{$t.\text{key} < x$}
      \State \Return \Call{LowerBound}{$t.\text{right}, x$}
      \Else
      \State $y \gets $ \Call{LowerBound}{$t.\text{left}, x$}
      \If{$y \neq \textsc{nil}$}
      \State \Return $y$
      \Else
      \State \Return $t$
      \EndIf
      \EndIf
      \EndFunction
    \end{algorithmic}
  \end{algorithm}
  The complexity is $O(h)$, where $h$ is the height of the tree. 
  \end{example}
  

%--------------------------------------------------------------------
\subsubsection{Traversals}
\begin{algorithm}[htb]
  \caption{Inorder Tree Walk (recursive)}
  \label{alg:bst_walk}
  \begin{algorithmic}[1]
    \Function{TreeWalk}{$x$}
      \If{$x \ne \textsc{nil}$}
        \State \Call{TreeWalk}{$\attrib{x}{left}$} \label{alg:bst_walk:line:call_left}
        \State print \(\attrib{x}{key}\) \label{alg:bst_walk:line:print}
        \State \Call{TreeWalk}{$\attrib{x}{right}$} \label{alg:bst_walk:line:call_right}
      \EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:bst_walk} shows the \textsc{InOrderTreeWalk} algorithm.
Three other variants can be obtained from Algorithm~\ref{alg:bst_walk} by swapping the order of the recursive calls and the print statement (lines \ref{alg:bst_walk:line:call_left}, \ref{alg:bst_walk:line:print}, and \ref{alg:bst_walk:line:call_right}).
For \textsc{PreOrderTreeWalk}, we swap lines \ref{alg:bst_walk:line:call_left} and \ref{alg:bst_walk:line:print}.
For \textsc{PostOrderTreeWalk}, we swap lines \ref{alg:bst_walk:line:print} and \ref{alg:bst_walk:line:call_right}.
For \textsc{ReverseOrderTreeWalk}, we swap lines \ref{alg:bst_walk:line:call_left} and \ref{alg:bst_walk:line:call_right}.

The general recurrence is 
\[
T(n) = T(n_L) + T(n - n_L - 1) + \Theta(1)
\]
and all four variants run in \(\Theta(n)\) time. 
This can be proven using the \emph{substitution method}.
Can we do better? No, because the length of the output is \(\Theta(n)\).

%--------------------------------------------------------------------
\subsubsection{Basic Queries}

\begin{algorithm}[htb]
  \caption{Searching a Binary Search Tree for a Key}
  \label{alg:bst_search}
  \begin{algorithmic}[1]
    \Function{TreeSearch}{$x,k$}
      \If{$x=\textsc{nil} \OR k=\attrib{x}{key}$} 
        \State \Return $x$ 
      \EndIf
      \If{$k<\attrib{x}{key}$}
        \State \Return \Call{TreeSearch}{$\attrib{x}{left},k$}
      \Else
        \State \Return \Call{TreeSearch}{$\attrib{x}{right},k$}
      \EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}


\begin{algorithm}[htb]
  \caption{Minimum of a Binary Search Tree}
  \label{alg:bst_min}
  \begin{algorithmic}[1]
    \Function{TreeMinimum}{$x$}
      \While{$\attrib{x}{left}\ne\textsc{nil}$} 
      \State $x\gets\attrib{x}{left}$ 
      \EndWhile
      \State \Return $x$
    \EndFunction
    % \Function{TreeMaximum}{$x$}
    %   \While{$\attrib{x}{right}\ne\textsc{nil}$} $x\gets\attrib{x}{right}$ \EndWhile
    %   \State \Return $x$
    % \EndFunction
  \end{algorithmic}
\end{algorithm}
To find the maximum, replace all occurences of \attribute{left} with \attribute{right} in Algorithm~\ref{alg:bst_min}.

\begin{definition}[Successor]
  The \emph{successor} of a node \(x\) is the minimum of the right subtree of \(x\) if it exists, otherwise it is the lowest ancestor \(a\) of \(x\) such that \(x\) falls in the left subtree of \(a\).
\end{definition}

\begin{algorithm}[htb]
  \caption{Successor of a Node in a Binary Search Tree}
  \label{alg:bst_successor}
  \begin{algorithmic}[1]
    \Function{TreeSuccessor}{$x$}
      \If{$\attrib{x}{right}\ne\textsc{nil}$}
        \State \Return \Call{TreeMinimum}{$\attrib{x}{right}$} \Comment{leftmost node in right subtree}
      \EndIf
      \While{$\attrib{x}{parent} \ne \textsc{nil} \AND \attrib{\attrib{x}{parent}}{right} = x$}
        \State $x\gets \attrib{x}{parent}$ 
      \EndWhile
      \State \Return $\attrib{x}{parent}$ \Comment{lowest ancestor of \(x\) whose left child is an ancestor of \(x\)}
    \EndFunction
  \end{algorithmic}
\end{algorithm}
To find the predecessor, replace all occurences of \attribute{right} with \attribute{left} and use \textsc{TreeMaximum} instead of \textsc{TreeMinimum} in Algorithm~\ref{alg:bst_successor}.



%--------------------------------------------------------------------
\subsubsection{Updates}

\begin{algorithm}[htb]
  \caption{Inserting a Node into a Binary Search Tree}
  \label{alg:bst_insert}
  \begin{algorithmic}[1]
    \Function{TreeInsert}{$T,z$}
      \State $y, x \gets \textsc{nil}, \attrib{T}{root}$
      \While{$x\ne\textsc{nil}$}
        \State $y\gets x$
        \If{$\attrib{z}{key} < \attrib{x}{key}$}
          \State $x\gets\attrib{x}{left}$
        \Else
          \State $x\gets\attrib{x}{right}$
        \EndIf
      \EndWhile
      \State $\attrib{z}{parent}\gets y$
      \If{$y=\textsc{nil}$}
        \State $\attrib{T}{root}\gets z$
      \ElsIf{$\attrib{z}{key}<\attrib{y}{key}$}
        \State $\attrib{y}{left}\gets z$
      \Else
        \State $\attrib{y}{right}\gets z$
      \EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}


Deletion distinguishes three cases:
\begin{itemize}[before={\parskip=0pt},nosep]
  \item \(z\) is a leaf node, i.e. it has no children:
    \begin{itemize}[before={\parskip=0pt},nosep]
      \item simply remove the node \(z\)
    \end{itemize}
  \item \(z\) has one child:
    \begin{itemize}[before={\parskip=0pt},nosep]
      \item remove \(z\)
      \item connect its child to its parent
    \end{itemize}
  \item \(z\) has two children:
    \begin{itemize}[before={\parskip=0pt},nosep]
      \item find the successor \(y\) of \(z\) (since \(z\) has two children, \(y\) is guaranteed to be the minimum of the right subtree of \(z\) and thus have at most one child)
      \item copy the key of \(y\) into \(z\)
      \item delete \(y\), which is a leaf or has one right child, i.e. connect the child of \(y\) to the parent of \(y\)
    \end{itemize}
\end{itemize}

\begin{algorithm}[htb]
\caption{Deleting a Node from a Binary Search Tree}
\label{alg:bst_delete}
\begin{algorithmic}[1]
  \Function{TreeDelete}{$T,z$}
  \LComment{\(z\) has two children: find successor, copy its key, then delete successor}
    \If{$\attrib{z}{left}\neq\textsc{nil} \AND \attrib{z}{right}\neq\textsc{nil}$}
      \State $s \gets \Call{TreeMinimum}{\attrib{z}{right}}$
      \State $\attrib{z}{key}\gets \attrib{s}{key}$ \Comment{replace the key in \(z\) with the one from the successor}
      \State \Return \Call{TreeDelete}{$T,s$} 
    \EndIf
    \LComment{\(z\) has at most one child: pick it (could be \(\textsc{nil}\))} 
    \If{$\attrib{z}{left}\neq\textsc{nil}$} %\Comment{\(z\) has one child}
      \State $c\gets \attrib{z}{left}$
    \Else
      \State $c\gets \attrib{z}{right}$
    \EndIf
    \If{$c\neq\textsc{nil}$} \Comment{if child exists, update its parent pointer}
      \State $\attrib{c}{parent}\gets \attrib{z}{parent}$
    \EndIf
    \If{$\attrib{z}{parent}=\textsc{nil}$} \Comment{if \(z\) was the root, make child the new root}
      \State $\attrib{T}{root}\gets c$
    \Else \Comment{otherwise, bypass \(z\) by connecting its child to its parent}
      \If{$z = \attrib{\attrib{z}{parent}}{left}$}
        \State $\attrib{\attrib{z}{parent}}{left}\gets c$
      \Else
        \State $\attrib{\attrib{z}{parent}}{right}\gets c$
      \EndIf
    \EndIf
    \State \Return $T$
  \EndFunction
\end{algorithmic}
% to be continued ...
  % \begin{algorithmic}[1]
  %   \Function{TreeDelete}{$T,z$}
  %     \LComment{\(z\) has two children: find successor, copy its key, then delete successor}
  %     \If{$\attrib{z}{left}\neq\textsc{nil}\;\AND\;\attrib{z}{right}\neq\textsc{nil}$}
  %       \State $s\gets \Call{TreeMinimum}{\attrib{z}{right}}$
  %       \LComment{instead of removing the node, replace \(z\)'s key with successor's}
  %       \State $\attrib{z}{key}\gets \attrib{s}{key}$
  %       \State \Return \Call{TreeDelete}{$T,s$}
  %     \EndIf
  %     \LComment{\(z\) has at most one child: pick it (could be \(\textsc{nil}\))} 
  %     \If{$\attrib{z}{left}\neq\textsc{nil}$}
  %       \State $c\gets \attrib{z}{left}$
  %     \Else
  %       \State $c\gets \attrib{z}{right}$
  %     \EndIf
  %     \LComment{if child exists, update its parent pointer}
  %     \If{$c\neq\textsc{nil}$}
  %       \State $\attrib{c}{parent}\gets \attrib{z}{parent}$
  %     \EndIf
  %     \LComment{if \(z\) was the root, make child the new root}
  %     \If{$\attrib{z}{parent}=\textsc{nil}$}
  %       \State $\attrib{T}{root}\gets c$
  %     \Else
  %       \LComment{otherwise, bypass \(z\) by updating the correct child link of \(z\)'s parent}
  %       \If{$z = \attrib{(\attrib{z}{parent})}{left}$}
  %         \State $\attrib{(\attrib{z}{parent})}{left}\gets c$
  %       \Else
  %         \State $\attrib{(\attrib{z}{parent})}{right}\gets c$
  %       \EndIf
  %     \EndIf
  %     \State \Return $T$
  %   \EndFunction
  % \end{algorithmic}
\end{algorithm}

\FloatBarrier

Insertion, search and deletion operations have complexity \(\Theta(h)\) where \(h\) is the height of the tree.
In the average case, the height is \(O(\log n)\) (i.e. with a random insertion order).
In some particular cases, the height can be \(O(n)\) (i.e. with ordered sequence).
The problem is that the `worst case' is not that uncommon.
One way to avoid this is to instead of inserting \(A = [a_1, a_2, a_3, \ldots, a_n]\) in order, insert a random permutation of \(A\).
The problem is that \(A\) is usually not known in advance.
It is the application that calls the insertion procedure.
But we can also obtain a random permutation of \(A\) by using a randomized insertion algorithm (see \ref{subsubsec:bst_rand_insert}).

% %--------------------------------------------------------------------
\subsubsection{Randomized Insertion}
\label{subsubsec:bst_rand_insert}
In order to avoid the linear-height worst case one can insert into a tree using Algorithm~\ref{alg:bst_rand_insert}.
The idea behind the function \textsc{TreeRandomizedInsert} is as follows. 
We insert a new node \(z\) into the tree \(t\) as the new root of \(t\) with probability \(1/(\attrib{t}{size}+1)\).
If \(z\) is not inserted as the new root, we recursively insert it into the appropriate subtree of \(t\).
The additional attribute \(\attrib{t}{size}\) represents the number of nodes in the subtree rooted at \(t\).

\begin{algorithm}[htb]
  \caption{Randomized Insertion into a Binary Search Tree}
  \label{alg:bst_rand_insert}
  \begin{algorithmic}[1]
    \Function{TreeRandomizedInsert}{$t,z$}
      \If {$t=\textsc{nil}$}
        \State \Return $z$
      \EndIf
      \State $r \gets$ \Call{randint}{$1,\attrib{t}{size}+1$} 
      \If{$r=1$} \Comment{$\Prob(r=1)=\Prob(z\text{ is inserted as the new root of }t) = \frac{1}{\attrib{t}{size}+1}$}
        \State $\attrib{z}{size}\gets\attrib{t}{size}+1$
        \State \Return \Call{TreeRootInsert}{$t,z$} \Comment{see \ref{subsubsec:bst_root_insert}}
      \EndIf
      \If{$\attrib{z}{key}<\attrib{t}{key}$}
        \State $\attrib{t}{left}\gets$ \Call{TreeRandomizedInsert}{$\attrib{t}{left},z$}
      \Else
        \State $\attrib{t}{right}\gets$ \Call{TreeRandomizedInsert}{$\attrib{t}{right},z$}
      \EndIf
      \State $\attrib{t}{size}\gets\attrib{t}{size}+1$
      \State \Return $t$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

With Algorithm~\ref{alg:bst_rand_insert} every insertion order is equally likely; the expected height of the tree is \(O(\log n)\) and all operations run in expected \(O(\log n)\) time.


%--------------------------------------------------------------------
\subsubsection{Rotations}
\begin{definition}[Rotation]
A \emph{rotation} is a local restructuring of a BST that exchanges the relative position of a node \(x\) and one of its children while preserving the in-order sequence of keys.
% \begin{itemize}[noitemsep]
%   \item \emph{Left rotation} at \(x\): \(x\) moves to the left of its right child.
%   \item \emph{Right rotation} at \(x\): \(x\) moves to the right of its left child.
% \end{itemize}
Rotations do \emph{not} change the in-order ordering of the keys and they are the basic tool used by self-balancing trees.
\end{definition}


\begin{center}
\begin{tikzpicture}[%
shorten >=0pt,
font=\footnotesize,
every path/.style={line cap=round},
root/.style  ={circle,fill=black,minimum size=2pt,inner sep=0pt},
node/.style  ={circle,draw,minimum size=16,inner sep=0pt, outer sep=0pt},
thickedge/.style   ={thick,->},
lab/.style   ={anchor=west,font=\footnotesize},
rng/.style   ={font=\footnotesize},
scale=0.7
]
\newcommand{\xdistance}{1.8}
\newcommand{\ydistance}{1.4}
\newcommand{\trianglenode}[5]{%
\node[isosceles triangle,
isosceles triangle apex angle=47,
      shape border rotate=90,
      minimum width=8mm,
      minimum height=8mm,
      draw,
      fill=#4!25,
      inner sep=0pt,
      outer sep=0pt,
      below,
      font=\footnotesize] (#1) at (#5) {};
\node at ($(#1.apex)!1.1!(#1.base)$) {$#2$};
  \node[below] at ($(#1.lower side)$) {$#3$};
}

% left  tree
\begin{scope}[shift={(0,0)}, local bounding box=T2, scale=0.75]

\node[root] (x2) {};
\node[lab,left=0pt of x2] {$x$};

\node[node,below right=1mm and 6mm of x2] (b2) {$b$};
\node[node,below left=6mm and 10mm of b2] (a2) {$a$};

\node[root,above left=1mm and 6mm of a2] (l) {};
\node[lab,left=0pt of l] {$l$};

\draw[->,bend left=20] (x2) to (b2);
\draw[->,bend left=20] (l) to (a2);
\draw[thickedge] (b2) to (a2);

\coordinate (alpha2) at ($(a2) + (-\xdistance,-\ydistance)$);
\coordinate (beta2)  at ($(a2) + (\xdistance,-\ydistance)$);
\coordinate (gamma2) at ($(b2) + (\xdistance,-\ydistance)$);

\draw[thick] (a2) -- (alpha2) (a2) -- (beta2) (b2) -- (gamma2);

\trianglenode{alpha2}{\alpha}{k\le a}{cyan}{alpha2}
\trianglenode{beta2}{\beta}{a\le k\le b}{yellow}{beta2}
\trianglenode{gamma2}{\gamma}{k\ge b}{purple}{gamma2}

\end{scope}

% right tree
\begin{scope}[shift={(8,0)}, local bounding box=T1, scale=0.75]

\node[root] (x1) {};
\node[lab,left=0pt of x1] {$x$};

\node[node,below right=1mm and 6mm of x1] (a1) {$a$};
\node[node,below right=6mm and 10mm of a1] (b1) {$b$};

\node[root,above right=1mm and 6mm of b1] (r) {};
\node[lab,right=0pt of r] {$r$};

\draw[->,bend left=20] (x1) to (a1);
\draw[->,bend right=20] (r) to (b1);
\draw[thickedge] (a1) to (b1);

\coordinate (alpha1) at ($(a1) + (-\xdistance,-\ydistance)$);
\coordinate (beta1)  at ($(b1) + (-\xdistance,-\ydistance)$);
\coordinate (gamma1) at ($(b1) + (\xdistance,-\ydistance)$);

\draw[thick] (a1) -- (alpha1) (b1) -- (beta1) (b1) -- (gamma1);

\trianglenode{alpha1}{\alpha}{k\le a}{cyan}{alpha1}
\trianglenode{beta1}{\beta}{a\le k\le b}{yellow}{beta1}
\trianglenode{gamma1}{\gamma}{k\ge b}{purple}{gamma1}
\end{scope}

% arrows between the two trees
\draw[draw=black,very thick,->,bend left=10, transform canvas={yshift=-4pt}] ($(T1.west)+(-0.5,0)$) to node[midway,below] {\textsc{\hyperref[alg:line:leftrotate]{LeftRotate}}} ($(T2.east)+(0.5,0)$);
\draw[draw=black,very thick,->,bend left=10, transform canvas={yshift=4pt}] ($(T2.east)+(0.5,0)$) to node[midway,above]  {\textsc{\hyperref[alg:line:rightrotate]{RightRotate}}} ($(T1.west)+(-0.5,0)$);

\end{tikzpicture}
\end{center}
  
\begin{algorithm}[htb]
  \caption{Rotations in a Binary Search Tree}
  \label{alg:bst_rotations}
  \begin{algorithmic}[1]
    \Function{RightRotate}{$x$} \label{alg:line:rightrotate}
      \State $l \gets \attrib{x}{left}$
      \State $\attrib{x}{left} \gets \attrib{l}{right}$
      \State $\attrib{l}{right} \gets x$
      \State \Return $l$
    \EndFunction
    \Function{LeftRotate}{$x$} \label{alg:line:leftrotate}
      \State $r \gets \attrib{x}{right}$
      \State $\attrib{x}{right} \gets \attrib{r}{left}$
      \State $\attrib{r}{left} \gets x$
      \State \Return $r$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Rotations are very useful operations.
For example, we can use it to perform

\subsubsection{Root Insertion}
\label{subsubsec:bst_root_insert}
\begin{algorithm}[htb]
  \caption{Root Insertion into a Binary Search Tree}
  \label{alg:bst_root_insert}
  \begin{algorithmic}[1]
    \Function{TreeRootInsert}{$x,z$}
      \If{$x=\textsc{nil}$}
        \State \Return $z$
      \EndIf
      \If{$\attrib{z}{key}<\attrib{x}{key}$}
        \State $\attrib{x}{left}\gets$ \Call{TreeRootInsert}{$\attrib{x}{left},z$}
        \State \Return \Call{RightRotate}{$x$}
      \Else
        \State $\attrib{x}{right}\gets$ \Call{TreeRootInsert}{$\attrib{x}{right},z$}
        \State \Return \Call{LeftRotate}{$x$}
      \EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}


General strategies to deal with complexity in the worst case:
\begin{itemize}[before={\parskip=0pt},nosep]
  \item \emph{Randomization}: can make the scenario $h = n$ highly unlikely
  \item \emph{Amortized Maintenance}: relatively expensive but `amortized' operations
  \item \emph{Self-Balancing}: e.g. Red-Black trees (see \ref{subsec:rb_tree})
\end{itemize}



\subsection{Red-Black Trees}
\label{subsec:rb_tree}
A \emph{red-black tree} is a binary search tree with one extra bit of storage per node:
its \attribute{color}, which can be either $\red$ or $\black$.
By constraining the node colors on any simple path from the root to a leaf, red-black trees ensure that no such path is more than twice as long as any other, so that the tree is approximately balanced.
\begin{definition}[Red-Black Tree]
  \label{def:rb_tree_properties}
  A red-black tree is a binary search tree that satisfies the following \emph{red-black properties}:
  \begin{enumerate}[before={\parskip=0pt},nosep]
    \item Every node is either red or black. \label{def:rb_tree_properties:color}
    \item The root is black. \label{def:rb_tree_properties:root_isblack}
    \item Every leaf ($\nil$) is black. \label{def:rb_tree_properties:leafs_areblack}
    \item If a node is red, then both its children are black. \label{def:rb_tree_properties:redparent_blackchildren}
    \item For each node, all simple paths from the node to descendant leaves contain the same number of black nodes.\label{def:rb_tree_properties:same_blackheight} \qedhere
  \end{enumerate}
\end{definition}
\begin{definition}[Black-Height]
The number of black nodes on any simple path from, but \underline{not} including, a node $x$ down to, and including, a leaf is called the \emph{black-height} of the node $x$:
\[
\blackheight(x) \coloneqq \numof \text{black nodes on path from (excluding) } x \text{ to (including) leafs}
\]
By property \ref{def:rb_tree_properties:same_blackheight}, the notion of black-height is well defined, since all descending paths have the same number of black nodes.
\end{definition}

\begin{lemma}[Height of a Red-Black Tree]
  \label{lem:rb_tree_height}
  The height \(h(x)\) of a red-black tree with \(n = \operatorname{size}(x)\) internal nodes is at most \(2\log(n+1)\).
  % A red-black tree with \(n=\operatorname{size}(x)\) internal nodes has height \(h(x)\) that is at most \(2\log(n+1)\).
\end{lemma}
\begin{proof}
  First, we prove (by induction) that the subtree rooted at any node \(x\) contains at least 
  \begin{equation}
    \label{eq:rb_tree_height:num_nodes_from_blackheight}
    2^{\operatorname{bh}(x)} - 1
  \end{equation}
  internal nodes.
  \begin{enumerate}[partopsep=0em, topsep=0em, label=(\roman*)]
    \item base case: \(x\) is a leaf, so \(\operatorname{size}(x)=0\) and \(\operatorname{bh}(x)=0\), fulfilling \eqref{eq:rb_tree_height:num_nodes_from_blackheight} \textcolor{Green}{\ding{52}}
    \item induction step: consider \(x, y_1, y_2\) such that \(x = \attrib{y_1}{parent} = \attrib{y_2}{parent}\)
    \[
      \operatorname{size}(x) = \operatorname{size}(y_1) + \operatorname{size}(y_2) + 1 \geq (2^{\operatorname{bh}(y_1)} - 1) + (2^{\operatorname{bh}(y_2)} - 1) + 1
    \]
    By rule \ref{def:rb_tree_properties:same_blackheight}, both children must have the same black-height.
    Let \(\operatorname{bh}(y) = \operatorname{bh}(y_1) = \operatorname{bh}(y_2)\). Then 
    \[
      \operatorname{size}(x) \geq 2 \cdot (2^{\operatorname{bh}(y)} - 1) + 1 = 2^{\operatorname{bh}(y) + 1} - 1
    \]
    The black-height of a child \(y\) differs at most by one from the black-height of the parent \(x\), i.e. \(\operatorname{bh}(y)  \in \{\operatorname{bh}(x), \operatorname{bh}(x) - 1\}\).
    In both cases, we have \(\operatorname{size}(x) \geq 2^{\operatorname{bh}(x)} - 1\), fulfilling \eqref{eq:rb_tree_height:num_nodes_from_blackheight} \textcolor{Green}{\ding{52}}
\end{enumerate}
By property \ref{def:rb_tree_properties:redparent_blackchildren}, the black-height of a node \(x\) is at least half the height of the tree, i.e. \(\operatorname{bh}(x) \geq \frac{h(x)}{2}\).
Therefore,
\[
 n = \operatorname{size}(x) \geq 2^{\operatorname{bh}(x)} - 1 \geq 2^{\frac{h(x)}{2}} - 1
\]
which can be rearranged to
\[
  h(x) \leq 2\log(n+1) \qedhere
\]
\end{proof}

  % \begin{proof}
    % Let $h$ be the (ordinary) height of the red-black tree and
    % let $\operatorname{bh}(x)$ denote the black-height of a node $x$.
    % We proceed in two steps.
    % \begin{enumerate}[leftmargin=*]
    %   \item 
    %   \label{step:rb_tree_height:num_nodes_from_blackheight}
    %   A subtree with black-height $k$ stores at least $2^{k}-1$ internal nodes.
    %   We prove the claim by induction on $k$.
    %   \begin{itemize}[leftmargin=*, partopsep=0em, topsep=0em]
    %   \item Base case: $k=0$.
    %     A node of black-height 0 is a $\nil$ leaf (Property \ref{def:rb_tree_properties:leafs_areblack}).
    %     It contributes no internal nodes, so the bound $2^{0}-1=0$ is tight.
    %   \item Induction step: $k\ge 1$.
    %     Assume every node of black-height $k-1$ roots a subtree
    %     with at least $2^{k-1}-1$ internal nodes.  
    %     Let $x$ be a node with $\operatorname{bh}(x)=k$.
    %     \begin{enumerate}[label=(\alph*), leftmargin=*, partopsep=0em, topsep=0em]
    %       \item Either $x$ is black.  
    %             Then \(\operatorname{bh}(\text{left}(x))=\operatorname{bh}(\text{right}(x))=k-1\).
    %       \item Or $x$ is red.  
    %             Property \ref{def:rb_tree_properties:redparent_blackchildren} forces both children to be black, so again each child has black-height $k-1$.
    %     \end{enumerate}
    %     Hence {\em both} children satisfy the inductive hypothesis.  
    %     Counting $x$ itself we get
    %     \[
    %       \# \text{internal nodes in }T(x)
    %       \;\;\ge\;\;
    %       1 + 2\bigl(2^{k-1}-1\bigr)
    %       \;=\;
    %       2^{k}-1
    %     \]
    %   \end{itemize}
    %   Thus every node of black-height $k$ heads a subtree with at least $2^{k}-1$ internal nodes.

    %   \item
    %   \label{step:rb_tree_height:height_from_blackheight}
    %   Now we want to relate the black-height to ordinary height.
    %   Walk from the root to any leaf.  
    %   Consecutive red nodes are forbidden (Property \ref{def:rb_tree_properties:redparent_blackchildren}),
    %   so along every root-leaf path \emph{at most every other} node can be red.
    %   Therefore
    %   \[
    %         \operatorname{bh}(\text{root}) \;\ge\; \frac{h}{2}
    %   \]
    %   \end{enumerate}
    
    %   Let $n$ be the total number of internal nodes.
    %   By Step \ref{step:rb_tree_height:num_nodes_from_blackheight} (applied to the root),
    %   \[
    %     n \;\;\ge\;\; 2^{\operatorname{bh}(\text{root})}-1
    %     \quad\Longrightarrow\quad
    %     \operatorname{bh}(\text{root}) \;\le\; \log_2(n+1)
    %   \]
    %   Combining with the inequality from Step \ref{step:rb_tree_height:height_from_blackheight} we get
    %   \[
    %       h \;\le\; 2\,\operatorname{bh}(\text{root})
    %       \;\le\; 2\log_2(n+1)
    %   \]
    %   Hence a red-black tree with $n$ internal nodes has height at most $2\log_2(n+1)$. 
    % \end{proof}






\section{Graphs}\label{sec:graphs}
\subsection{Representations of Graphs}
There are two standard ways to represent a graph: as a collection of adjacency lists or as an adjacency matrix.
Because the adjacency-list representation provides a compact way to represent \emph{sparse} graphs (those for which \(|E| \ll |V|^2\)), it is usually the method of choice.
The adjacency-matrix representation might be preferred when the graph is \emph{dense} (i.e., \(|E| \approx |V|^2\)), or you need to be able to tell quickly wether there is an edge connecting two given vertices.

The space required for the adjacency-list representation is \(\Theta(|V| + |E|)\), and finding each edge in the graph also takes \(\Theta(|V| + |E|)\) time, since each of the \(|V|\) adjacency lists must be examined.

The space required for the adjacency-matrix representation is \(\Theta(|V|^2)\), and finding each edge in the graph takes \(\Theta(|V|^2)\) time, since the entire adjacency matrix must be examined.
For an undirected graph, the adjacency matrix is symmetric, i.e. $\matr{A} = \matr{A}^T$.

The complexity for different operations is summarized in the following table.


% \newcommand{\mixcolor}{70}

% \definecolor{Red}{rgb}{1.0, 0.0, 0.0}
% \definecolor{Orange}{rgb}{1.0, 0.5, 0.0}
% \definecolor{Yellow}{rgb}{1.0, 1.0, 0.0}
% \definecolor{Green}{rgb}{0.0, 1.0, 0.0}
% \definecolor{BlueGreen}{rgb}{0.0, 1.0, 0.5}
% \definecolor{Blue}{rgb}{0.0, 1.0, 1.0}


% \begin{center}
%     \begin{tabular}{l c c c c c}
%         \toprule
%         \textbf{Algorithm} & \multicolumn{3}{c}{\textbf{Time Complexity}} & \textbf{in place?} \\%& \textbf{Space}  \\
%                             & \emph{worst}      & \emph{average}       & \emph{best}  & \\%& \emph{worst} \\
%         \midrule
%         \textsc{InsertionSort} & \cellcolor{Red!\mixcolor}$\Theta(n^2)$    & \cellcolor{Red!\mixcolor}$\Theta(n^2)$        & \cellcolor{Green!\mixcolor}$\Theta(n)$   & \ding{52} \\%& \cellcolor{Blue!\mixcolor}$\mathcal{O}(1)$ \\
%         \textsc{SelectionSort} & \cellcolor{Red!\mixcolor}$\Theta(n^2)$    & \cellcolor{Red!\mixcolor}$\Theta(n^2)$        & \cellcolor{Red!\mixcolor}$\Theta(n^2)$ & \ding{52} \\%& \cellcolor{Blue!\mixcolor}$\mathcal{O}(1)$ \\
%         \textsc{MergeSort}     & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$ & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$     & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$ & \ding{56} \\%& \cellcolor{Green!\mixcolor}$\mathcal{O}(n)$ \\
%         \textsc{QuickSort}     & \cellcolor{Red!\mixcolor}$\Theta(n^2)$    & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$     & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$ & \ding{52} \\%& \cellcolor{BlueGreen!\mixcolor}$\mathcal{O}(\log n)$ \\
%         \textsc{HeapSort}      & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$ & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$     & \cellcolor{Yellow!\mixcolor}$\Theta(n\log n)$ & \ding{52} \\%& \cellcolor{Blue!\mixcolor}$\mathcal{O}(1)$ \\
%         \bottomrule
%      \end{tabular}
% \end{center}

% \begin{center}
%     \begin{tabular}{l c c }
%         \toprule
%         \textbf{Algorithm} & \multicolumn{2}{c}{\textbf{Representation}} \\
%                             & Adjadency List      & Adjacency Matrix  \\  
%         \midrule
%         accessing vertex $u$    & \cellcolor{Green!\mixcolor}\(O(1)\)\\ optimal                             & \cellcolor{Green!\mixcolor}\(O(1)\) \\ optimal                  \\
%         iteration through $V$   & \cellcolor{Green!\mixcolor}\(\Theta(|V|)\) \\ optimal                     & \cellcolor{Green!\mixcolor}\(\Theta(|V|)\) \\ optimal           \\
%         iteration through $E$   & \cellcolor{Yellow!\mixcolor}\(\Theta(|V| + |E|)\) okay (not optimal) \\   & \cellcolor{Red!\mixcolor}\(\Theta(|V|^2)\) \\ possibly very bad \\
%         checking $(u,v)\in E$   & \cellcolor{Red!\mixcolor}\(O(|V|)\) \\ bad                                & \cellcolor{Green!\mixcolor}\(O(1)\) \\ optimal                  \\   
%         space complexity & \cellcolor{Green!\mixcolor}\(\Theta(|V| + |E|)\) \\ optimal                      & \cellcolor{Red!\mixcolor}\(\Theta(|V|^2)\) \\ possibly very bad \\
%         \bottomrule
%      \end{tabular}
% \end{center}

\newcolumntype{C}{>{\centering\arraybackslash}m{35mm}}
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}
\begin{table}[h]
\centering
\begin{tabular}{l C @{\hspace{10pt}} C}
\toprule
\textbf{Task}                                             & \multicolumn{2}{c}{\textbf{Representation}} \\
&                             {Adjacency List} &                                                        {Adjacency Matrix}\\
\midrule
accessing vertex $u$ &        \cellcolor{Blue!\mixcolor}\makecell{$O(1)$\\optimal} &                          \cellcolor{Blue!\mixcolor}\makecell{$O(1)$\\optimal}                    \\
\addlinespace[2pt]
iteration through $V$ &       \cellcolor{Green!\mixcolor}\makecell{$\Theta(|V|)$\\optimal} &                  \cellcolor{Green!\mixcolor}\makecell{$\Theta(|V|)$\\optimal}             \\
\addlinespace[2pt]
iteration through $E$ &       \cellcolor{Yellow!\mixcolor}\makecell{$\Theta(|V|+|E|)$\\okay (not optimal)} &  \cellcolor{Red!\mixcolor}\makecell{$\Theta(|V|^{2})$\\possibly very bad} \\
\addlinespace[2pt]
checking $(u,v)\in E$ &       \cellcolor{Red!\mixcolor}\makecell{$O(|V|)$\\bad} &                             \cellcolor{Blue!\mixcolor}\makecell{$O(1)$\\optimal}                    \\
\addlinespace[2pt]
space complexity &            \cellcolor{Green!\mixcolor}\makecell{$\Theta(|V|+|E|)$\\optimal} &              \cellcolor{Red!\mixcolor}\makecell{$\Theta(|V|^{2})$\\possibly very bad} \\
\bottomrule
\end{tabular}
\end{table}



For a graph \(G=(V,E)\) with source \(s\) we define the predecessor subgraph of \(G\) as \(G_\pi=(V_\pi,E_\pi)\) where
\[
V_\pi = \{v\in V\mid v.\pi \neq \nil \} \cup \{s\}
\]
and
\[
E_\pi = \{(v.\pi,v)\mid v\in V_\pi \setminus \{s\}\}
\]

For depth-first search we define the predecessor subgraph as \(G_\pi=(V,E_\pi)\), where
\[
E_\pi = \{(v.\pi,v)\mid v\in V \AND v.\pi \neq \nil\}
\]
It is a forest.

\subsubsection{Graph Traversals} \label{sec:graph-traversals}

\begin{algorithm}[H]
\caption{Breadth-First Search}\label{alg:bfs}
\begin{algorithmic}[1]
\Function{BFS}{$G=(V,E),\,s$} \Comment{\(s\) is the source}
  \ForAll{$u\in V \setminus \{s\}$}
    \State $\attrib{u}{color},\attrib{u}{distance},\attrib{u}{\pi}\gets \white,\infty,\nil$
  \EndFor
  % \State Initialize $\attrib{v}{color}\gets\white$, $\attrib{v}{distance}\gets\infty$, $\attrib{v}{\pi}\gets\nil$ for all $v\in V$
  \State $\attrib{s}{color},\attrib{s}{distance},\attrib{s}{\pi}\gets \gray,0,\nil$
  \State $Q\gets\emptyset$ \Comment{queue for vertices to visit}
  \State \Call{Enqueue}{$Q,s$}
  \While{$Q\neq\emptyset$}
    \State $u\gets$ \Call{Dequeue}{$Q$}
    \ForAll{$v\in\Gamma(u)$}
      \If{$\attrib{v}{color}=\white$}
        \State $\attrib{v}{color}\gets\gray$
        \State $\attrib{v}{distance}\gets\attrib{u}{distance}+1$
        \State $\attrib{v}{\pi}\gets u$
        \State \Call{Enqueue}{$Q,v$}
      \EndIf
    \EndFor
    \State $\attrib{u}{color}\gets\black$
  \EndWhile
\EndFunction
\end{algorithmic}
\end{algorithm}

BFS constructs the breadth-first tree $G_{\pi}$ and records
$\attrib{v}{distance}=\delta(s,v)$.
Running time: $\Theta(|V|+|E|)$.


\begin{algorithm}[H]
\caption{Depth-First Search}\label{alg:dfs}
\begin{algorithmic}[1]
\Function{DFS}{$G=(V,E)$}
  \State Initialize $\attrib{v}{color}\gets\white$, $\attrib{v}{\pi}\gets\nil$ for all $v\in V$
  \State $T\gets 0$
  \ForAll{$v\in V$}
    \If{$\attrib{v}{color}=\white$} \Call{DFS-Visit}{$v$} \EndIf
  \EndFor
\EndFunction

\Function{DFS-Visit}{$u$}
  \State $\attrib{u}{color}\gets\gray$
  \State $T\gets T+1$
  \State $\attrib{u}{disc}\gets T$
  \ForAll{$v\in\Gamma(u)$}
    \If{$\attrib{v}{color}=\white$}
      \State $\attrib{v}{\pi}\gets u$
      \State \Call{DFS-Visit}{$v$}
    \EndIf
  \EndFor
  \State $\attrib{u}{color}\gets\black$
  \State $T\gets T+1$
  \State $\attrib{u}{finish}\gets T$
\EndFunction
\end{algorithmic}
\end{algorithm}

DFS runs in $\Theta(|V|+|E|)$.

\subsubsection{Minimum Spanning Trees}

For a connected, undirected, weighted graph \(G=(V,E,w)\) an MST is a subset
of edges that connects all vertices with minimum total weight.

\subsubsection{Kruskal}

\begin{algorithm}[H]
\caption{Kruskal}\label{alg:kruskal}
\begin{algorithmic}[1]
\Function{Kruskal}{$G=(V,E,w)$}
  \State $T\gets\emptyset$
  \State MakeSet$(v)$ for each $v\in V$
  \ForAll{$(u,v)\in E$ in non-decreasing $w$-order}
    \If{\Call{Find}{$u$}\,$\neq$\,\Call{Find}{$v$}}
      \State $T\gets T\cup\{(u,v)\}$
      \State \Call{Union}{$u,v$}
    \EndIf
  \EndFor
  \State \Return $T$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsubsection{Prim}

\begin{algorithm}[H]
\caption{Prim (binary heap)}\label{alg:prim}
\begin{algorithmic}[1]
\Function{Prim}{$G=(V,E,w),\,r$}
  \State Initialize $\attrib{v}{key}\gets\infty$, $\attrib{v}{\pi}\gets\nil$ for all $v\in V$
  \State $\attrib{r}{key}\gets 0$
  \State $Q\gets V$  \Comment{min-priority queue by \attrib{·}{key}}
  \While{$Q\neq\emptyset$}
    \State $u\gets$ \Call{ExtractMin}{$Q$}
    \ForAll{$v\in\Gamma(u)$}
      \If{$v\in Q$ and $w(u,v)<\attrib{v}{key}$}
        \State $\attrib{v}{\pi}\gets u$
        \State $\attrib{v}{key}\gets w(u,v)$
        \State \Call{DecreaseKey}{$Q,v,\attrib{v}{key}$}
      \EndIf
    \EndFor
  \EndWhile
  \State \Return $\{(\attrib{v}{\pi},v)\mid v\in V\setminus\{r\}\}$
\EndFunction
\end{algorithmic}
\end{algorithm}

Both Kruskal and Prim run in $\Theta(|E|\log|V|)$ with binary heaps.

\subsection{Single-Source Shortest Paths}

\subsubsection{Dijkstra (non-negative weights)}

\begin{algorithm}[H]
\caption{Dijkstra}\label{alg:dijkstra}
\begin{algorithmic}[1]
\Function{Dijkstra}{$G=(V,E,w),\,s$}
  \State Initialize $\attrib{v}{distance}\gets\infty$, $\attrib{v}{\pi}\gets\nil$ for all $v\in V$
  \State $\attrib{s}{distance}\gets 0$
  \State $Q\gets V$  \Comment{min-queue keyed by \attrib{·}{distance}}
  \While{$Q\neq\emptyset$}
    \State $u\gets$ \Call{ExtractMin}{$Q$}
    \ForAll{$(u,v)\in E$}
      \If{$\attrib{v}{distance}>\attrib{u}{distance}+w(u,v)$}
        \State $\attrib{v}{distance}\gets\attrib{u}{distance}+w(u,v)$
        \State $\attrib{v}{\pi}\gets u$
        \State \Call{DecreaseKey}{$Q,v,\attrib{v}{distance}$}
      \EndIf
    \EndFor
  \EndWhile
  \State \Return $\bigl(\attrib{v}{distance}\bigr)_{v\in V}$
\EndFunction
\end{algorithmic}
\end{algorithm}

Binary heap: $\Theta(|E|\log|V|)$; Fibonacci heap: $\Theta(|E|+|V|\log|V|)$.

\subsection{Complexity Overview}

\begin{center}\setlength{\tabcolsep}{8pt}
\begin{tabular}{lcc}
\toprule
\textbf{Algorithm} & \textbf{Adjacency list} & \textbf{Conditions} \\\midrule
BFS, DFS           & $\Theta(|V|+|E|)$          & – \\
Kruskal            & $\Theta(|E|\log|V|)$       & sort + DSU \\
Prim (bin.\ heap)  & $\Theta(|E|\log|V|)$       & – \\
Dijkstra (bin.)    & $\Theta(|E|\log|V|)$       & $w\ge 0$ \\
\bottomrule
\end{tabular}
\end{center}
\FloatBarrier






\clearpage

\end{document}